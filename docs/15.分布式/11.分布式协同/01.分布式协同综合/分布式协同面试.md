---
title: 分布式协同
date: 2024-12-16 23:57:04
categories:
  - 分布式
  - 分布式协同
  - 分布式协同综合
tags:
  - 分布式
  - 协同
  - 面试
permalink: /pages/372a9cc0/
---

# 分布式协同

## 复制

### 【基础】什么是复制？复制有什么作用？

:::details 要点

**复制主要指通过网络在多台机器上保存相同数据的副本**。

复制数据，可能出于各种各样的原因：

- **提高可用性** - 当部分组件出现位障，系统依然可以继续工作，系统依然可以继续工作。
- **降低访问延迟** - 使数据在地理位置上更接近用户。
- **提高读吞吐量** - 扩展至多台机器以同时提供数据访问服务。

:::

### 【中级】复制有哪些模式？

:::details 要点

复制的模式有以下几种：

- **主从复制** - **所有的写入操作都发送到主节点**，由主节点负责将数据更改事件发送到从节点。每个从节点都可以接收读请求，但内容可能是过期值。支持主从复制的系统：
  - 数据库：Mysql、PostgreSQL、MongoDB 等
  - 消息队列：Kafka、RabbitMQ 等
- **多主复制** - **系统存在多个主节点，每个都可以接收写请求**，客户端将写请求发送到其中的一个主节点上，由该主节点负责将数据更改事件同步到其他主节点和自己的从节点。
- **无主复制** - **系统中不存在主节点，每一个节点都能接受客户端的写请求**。此外，**读取时从多个节点上并行读取，以此检测和纠正某些过期数据**。支持无主复制的系统：
  - 数据库：Cassandra

此外，复制还需要考虑以下问题：

- **同步还是异步**
- **如何处理失败的副本**
- **如何保证数据一致**

:::

### 【中级】主从复制是如何工作的？

:::details 要点

最常见的解决方案就是主从复制，其原理如下：

主从复制模式中只有一个主副本（或称为主节点） ，其余称为从副本（或称为从节点）。

1. 所有的写请求只能发送给主副本，主副本首先将新数据写入本地存储。

2. 然后，主副本将数据更改作为复制的日志或更新流发送给所有从副本。每个从副本获得更新数据之后将其应用到本地，且严格保持与主副本相同的写入顺序。

3. 读请求既可以在主副本上，也可以在从副本上执行。

再次强调，**只有主副本才可以接受写请求**：从客户端的角度来看，从副本都是只读的。如果由于某种原因，例如与主节点之间的网络中断而导致主节点无法连接，主从复制方案就会影响所有的写入操作。

![主从复制系统](https://raw.githubusercontent.com/dunwu/images/master/snap/20220302202101.png)

:::

### 【中级】同步复制、半同步复制、异步复制有什么差异？

:::details 要点

![主从复制——同步和异步](https://raw.githubusercontent.com/dunwu/images/master/snap/20220302202158.png)

一般，复制速度会非常快；但是，系统不能保证复制多久能完成。有些情况下，从节点可能落后主节点几分钟甚至更长时间，例如：从节点刚从故障中恢复；或系统已经接近最大设计上限；或节点之间的网络出现问题。

**全同步复制**的优缺点：

- **优点**：只有所有从节点都完成复制，才视为成功，因此是**强一致的**。
- **缺点**：即使只有一个从节点未完成复制，写入都不能视为成功。所有从节点完成复制过程之前，主节点会**阻塞**后续所有的写操作。

因此，**把所有从节点都配置为同步复制有些不切实际**。因为这样的话，任何一个同步节点的中断都会导致整个系统更新停滞不前。

**全异步复制**的优缺点：

- **优点**：不管从节点上数据多么滞后，主节点总是可以继续响应写请求，**系统的性能更好**。
- **缺点**：如果主节点发生故障且不可恢复，则**所有尚未复制到从节点的写请求都会丢失**。

还有一种折中的方案——**半同步复制**：**只要有一个从节点或半数以上的从节点同步成功，就视为同步，直接返回结果；剩下的节点都通过异步方式同步**。万一同步的从节点变得不可用或性能下降，则将另一个异步的从节点提升为同步模式。这样可以保证至少有两个节点（即主节点和一个同步从节点）拥有最新的数据副本。

:::

### 【中级】新的从节点如何复制主节点数据？

:::details 要点

两种不可行的方案：

- 由于主节点会源源不断接受新的写入数据，数据始终处于变化中，因此**一次性从主节点复制数据到从节点是无法保证数据一致的**。
- 另一种思路是：考虑**锁定数据库**（使其不可写）来使磁盘上的文件保持一致，但这会**违反高可用的设计目标**。

可行的方案：

1. 生成主节点某时刻的快照，避免长时间锁定数据库。
2. 将快照复制到从节点。
3. 从节点复制主节点快照过程中，所有的数据变更写入一个日志中（这个数据变更日志在不同数据库中有着不同的称呼，Mysql 称其为 binlog；Redis 称其为 AOF）。
4. 从节点复制完主节点的快照后，请求数据变更日志中的数据，并基于此补全数据，这个过程称为**追赶**，直至主从数据一致。井重复步骤 1 ～步骤 4 。

:::

### 【高级】如何通过主从复制技术来实现系统高可用呢？

:::details 要点

#### 从节点失效：追赶式恢复

从节点的本地磁盘上都保存了副本收到的数据变更日志。如果从节点从故障中恢复，可以和主节点对比数据变更日志的偏移量，从而确认数据是否滞后。如果数据存在滞后，则向主节点请求数据变更日志，并补全数据。这个过程称为**追赶**。

#### 主节点失效：节点切换

主节点失效后，需要选举出新主节点。然后，客户端需要更新路由，将所有写请求发送给新的主节点；其他从节点要接受来自新的主节点上的变更数据。这个过程称之为**切换**。

主节点切换可以手动或自动进行。自动切换的步骤通常如下：

1. **确认主节点失效**。有很多种出错可能性，很难准确检测出问题的原因。所以，大多数系统都基于超时机制来确认主节点是否失效：节点间频繁地互相发生发送心跳存活悄息，如果发现某一个节点在一段比较长时间内没有响应，即认为该节点发生失效。
2. **选举新的主节点**。基于多数派共识选主。候选节点最好与原主节点的数据差异最小，这样可以最小化数据丢失的风险。
3. **重新配置系统使新主节点生效**。客户端现在需要将写请求发送给新的主节点。原主节点若恢复，需降级处理，避免脑裂。

:::

### 【高级】复制日志如何实现？

:::details 要点

复制日志的视线方式：

- **基于语句的复制** - 将数据写操作写入日志。主要缺点是**必须完全按照相同顺序执行**，否则可能会产生不同的结果。
- **基于预写日志（WAL）传输** - 通常每个写操作都是以追加写的方式写入到日志中。主要缺点是**日志描述的数据结果非常底层**，如果数据库不同版本的存储格式存在差异，就可能无法兼容。
  - 对于日志结构存储引擎，日志是主要的存储方式。日志段在后台压缩井支持垃圾回收。
  - 对于采用覆写磁盘的 BTree 结构，每次修改会预先写入日志，如系统发生崩溃，通过索引更新的方式迅速恢复到此前一致状态。
- **基于行的逻辑日志复制** - 如果复制和存储引擎采用不同的日志格式，这样复制与存储的逻辑就可以剥离。这种复制日志称为逻辑日志，以区分物理存储引擎的数据表示。
- **基于触发器的复制** - 这种方式**很灵活**，可以定制化控制复制逻辑。主要缺点是复制**开销更高，也更容易出错**。

:::

### 【高级】多主复制是如何工作的？

:::details 要点

对主从复制模型进行自然的扩展，则可以配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似：处理写的每个主节点都必须将该数据更改转发到所有其他节点。这就是多主节点（ 也称为主－主，或主动／主动）复制。此时，每个主节点还同时扮演其他主节点的从节点。

在一个数据中心内部使用多主节点基本没有太大意义，其复杂性已经超过所能带来的好处。

但是，以下场景这种配置则是合理的：

- 多数据中心
- 离线客户端操作
- 协作编辑

#### 多数据中心

有了多主节点复制模型，则可以在每个数据中心都配置主节点。在每个数据中心内，采用常规的主从复制方案；而在数据中心之间，由各个数据中心的主节点来负责同其他数据中心的主节点进行数据的交换、更新。

![img](https://raw.githubusercontent.com/dunwu/images/master/snap/202405122221705.png)

部署单主节点的主从复制方案与多主复制方案之间的差异

- **性能**：对于主从复制，每个写请求都必须经由广域网传送至主节点所在的数据中心。这会大大增加写入延迟，井基本偏离了采用多数据中心的初衷（即就近访问）。而在多主节点模型中，每个写操作都可以在本地数据中心快速响应，然后采用异步复制方式将变化同步到其他数据中心。因此，对上层应用有效屏蔽了数据中心之间的网络延迟，使得终端用户所体验到的性能更好。
- **容忍数据中心失效**：对于主从复制，如果主节点所在的数据中心发生故障，必须切换至另一个数据中心，将其中的一个从节点被提升为主节点。在多主节点模型中，每个数据中心则可以独立于其他数据中心继续运行，发生故障的数据中心在恢复之后更新到最新状态。
- **容忍网络问题**：数据中心之间的通信通常经由广域网，它往往不如数据中心内的本地网络可靠。对于主从复制模型，由于写请求是同步操作，对数据中心之间的网络性能和稳定性等更加依赖。多主节点模型则通常采用异步复制，可以更好地容忍此类问题，例如临时网络闪断不会妨碍写请求最终成功。

:::

### 【高级】无主复制是如何工作的？

:::details 要点

无主复制模式，**系统中不存在主节点，每一个节点都能接受客户端的写请求**。此外，**读取时从多个节点上并行读取，以此检测和纠正某些过期数据**。

#### 读修复和反熵

复制模型应确保所有数据最终复制到所有的副本。当一个失效的节点重新上线之后，如何赶上中间错过的那些写请求呢？

有以下两种机制：

- **读修复** - 客户端并行读取多个副本，根据版本识别过期返回值并更新最新值到相应副本。这种方法主要适合那些被频繁读取的场景。
- **反熵** - 利用后台进程不断查找副本间的数据差异，将任何缺少的数据从一个副本复制到另一个副本。与基于主节点复制的复制日志不同，反熵过程并不保证以特定的顺序复制写入，并且会引入明显的同步滞后。

#### QuorumNWR 算法

无主复制模式中，究竟多少个副本完成才可以认为写成功？

如果有 n 个副本，写人需要 w 个节点确认，读取必须至少查询 r 个节点， 则只要 `w+r>n` ，读取的节点中一定会包含最新值。

#### 并发写冲突

无主模式中，并发向多副本写操作，以及读时修复或数据回传都会导致并发写冲突。如何解决冲突呢？有以下几种机制：

- 最后写入者获胜（丢弃并发写入） - 每个副本总是保存最新值，允许覆盖井丢弃旧值。
- Happens Before - 利用全序的逻辑时钟来确定事件发生的前后顺序。
- 向量时钟、版本向量时钟 - 本质上是将全序的逻辑时钟改造为维护所有副本版本号的合集，基于此合集可以进行偏序比较。

:::

## 分区

### 【基础】什么是分区？为什么要分区？

:::details 要点

分区通常是这样定义的，即每一条数据（或者每条记录，每行或每个文档）只属于某个特定分区。实际上，每个分区都可以视为一个完整的小型数据库，虽然数据库可能存在一些跨分区的操作。

在不同系统中，分区有着不同的称呼，例如它对应于 MongoDB, Elasticsearch 和 SolrCloud 中的 shard, HBase 的 region, Bigtable 中的 tablet, Cassandra 和 Riak 中的 vnode ，以及 Couch base 中的 vBucket。总体而言，分区是最普遍的术语。

数据量如果太大，单台机器进行存储和处理就会成为瓶颈，因此需要引入数据分区机制。

分区的目地是通过多台机器均匀分布数据和查询负载，避免出现热点。这需要选择合适的数据分区方案，在节点添加或删除时重新动态平衡分区。

:::

### 【中级】分区有哪些模式？

:::details 要点

分区通常与复制结合使用，即每个分区在多个节点都存有副本。这意味着某条记录属于特定的分区，而同样的内容会保存在不同的节点上以提高系统的容错性。

一个节点上可能存储了多个分区。每个分区都有自己的主副本，例如被分配给某节点，而从副本则分配在其他一些节点。一个节点可能既是某些分区的主副本，同时又是其他分区的从副本。

分区主要有两种模式：

- **基于关键字区间的分区** - 先对关键字进行排序，每个分区只负责一段包含最小到最大关键字范围的一段关键字。对关键字排序的优点是可以支持高效的区间查询，但是如果应用程序经常访问与排序一致的某段关键字，就会存在热点的风险。采用这种方怯，当分区太大时，通常将其分裂为两个子区间，从而动态地再平衡分区。典型代表：HBase
- **哈希分区** - 将哈希函数作用于每个关键字，每个分区负责一定范围的哈希值。这种方法打破了原关键字的顺序关系，它的区间查询效率比较低，但可以更均匀地分配负载。采用哈希分区时，通常事先创建好足够多（但固定数量）的分区， 让每个节点承担多个分区，当添加或删除节点时将某些分区从一个节点迁移到另一个节点，也可以支持动态分区。典型代表：Elasticsearch、Redis。

:::

### 【高级】二级索引如何分区？

:::details 要点

二级索引是关系数据库的必备特性，在文档数据库中应用也非常普遍。但考虑到其复杂性，许多键值存储（如 HBase 和 Voldemort）并不支持二级索引。此外， 二级索引技术也是 Solr 和 Elasticsearch 等搜索引擎数据库存在之根本。

分区不仅仅是针对数据，二级索引也需要分区。通常有两种方法：

**基于文档来分区二级索引（本地索引）** - 二级索引存储在与关键字相同的分区中，这意味着写入时我们只需要更新一个分区，但缺点是读取二级索引时需要在所有分区上并行执行。它广泛用于实践： MongoDB 、Riak、Cassandra、Elasticsearch 、SolrCloud 和 VoltDB 都支持基于文档分区二级索引。

![img](https://raw.githubusercontent.com/dunwu/images/master/snap/20220303111528.png)

**基于词条来分区二级索引（全局索引）** - 它是基于索引的值而进行的独立分区。二级索引中的条目可能包含来自关键字的多个分区里的记录。在写入时，不得不更新二级索引的多个分区；但读取时，则可以从单个分区直接快速提取数据。

![img](https://raw.githubusercontent.com/dunwu/images/master/snap/20220303112708.png)

:::

### 【基础】什么是分区再均衡？

:::details 要点

集群节点数变化，数据规模增长等情况，都会导致分区的分布不均。要保持分区的均衡，势必要将数据和请求进行迁移，这样一个迁移负载的过程称为**分区再均衡**。

:::

### 【高级】分区再均衡有哪些策略？

:::details 要点

#### 固定数量的分区

创建远超实际节点数的分区数，然后为每个节点分配多个分区。接下来， 如果集群中添加了一个新节点，该新节点可以从每个现有的节点上匀走几个分区，直到分区再次达到全局平衡。

选中的整个分区会在节点之间迁移，但分区的总数量仍维持不变，也不会改变关键字到分区的映射关系。这里唯一要调整的是分区与节点的对应关系。考虑到节点间通过网络传输数据总是需要些时间，这样调整可以逐步完成，在此期间，旧分区仍然可以接收读写请求。

![](https://raw.githubusercontent.com/dunwu/images/master/snap/202405122231579.png)

原则上，也可以将集群中的不同的硬件配置因素考虑进来，即性能更强大的节点将分配更多的分区，从而分担更多的负载。

目前，Riak、Elasticsearch、Couchbase 和 Voldemort 都支持这种动态平衡方法。

使用该策略时，分区的数量往往在数据库创建时就确定好，之后不会改变。原则上也可以拆分和合并分区（稍后介绍），但固定数量的分区使得相关操作非常简单，因此许多采用固定分区策略的数据库决定不支持分区拆分功能。所以，在初始化时，已经充分考虑将来扩容增长的需求（未来可能拥有的最大节点数），设置一个足够大的分区数。而每个分区也有些额外的管理开销，选择过高的数字可能会有副作用。

#### 动态分区

对于采用关键宇区间分区的数据库，如果边界设置有问题，最终可能会出现所有数据都挤在一个分区而其他分区基本为空，那么设定固定边界、固定数量的分区将非常不便：而手动去重新配置分区边界又非常繁琐。

因此， 一些数据库如 HBase 和 RethinkDB 等采用了动态创建分区。当分区的数据增长超过一个可配的参数阔值（HBase 上默认值是 10GB），它就拆分为两个分区，每个承担一半的数据量。相反，如果大量数据被删除，并且分区缩小到某个阈值以下，则将其与相邻分区进行合井。该过程类似于 B 树的分裂操作。

每个分区总是分配给一个节点，而每个节点可以承载多个分区，这点与固定数量的分区一样。当一个大的分区发生分裂之后，可以将其中的一半转移到其他某节点以平衡负载。对于 HBase，分区文件的传输需要借助 HDFS。

动态分区的一个优点是分区数量可以自动适配数据总量。如果只有少量的数据，少量的分区就足够了，这样系统开销很小；如果有大量的数据，每个分区的大小则被限制在一个可配的最大值。

但是，需要注意的是，对于一个空的数据库， 因为没有任何先验知识可以帮助确定分区的边界，所以会从一个分区开始。可能数据集很小，但直到达到第一个分裂点之前，所有的写入操作都必须由单个节点来处理， 而其他节点则处于空闲状态。为了缓解这个问题，HBase 和 MongoDB 允许在一个空的数据库上配置一组初始分区（这被称为预分裂）。对于关键字区间分区，预分裂要求已经知道一些关键字的分布情况。

动态分区不仅适用于关键字区间分区，也适用于基于哈希的分区策略。MongoDB 从版本 2.4 开始，同时支持二者，井且都可以动态分裂分区。

#### 按节点比例分区

采用动态分区策略，拆分和合并操作使每个分区的大小维持在设定的最小值和最大值之间，因此分区的数量与数据集的大小成正比关系。另一方面，对于固定数量的分区方式，其每个分区的大小也与数据集的大小成正比。两种情况，分区的数量都与节点数无关。

Cassandra 和 Ketama 则采用了第三种方式，使分区数与集群节点数成正比关系。换句话说，每个节点具有固定数量的分区。此时， 当节点数不变时，每个分区的大小与数据集大小保持正比的增长关系； 当节点数增加时，分区则会调整变得更小。较大的数据量通常需要大量的节点来存储，因此这种方法也使每个分区大小保持稳定。

当一个新节点加入集群时，它随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一半数据量，将另一半数据留在原节点。随机选择可能会带来不太公平的分区分裂，但是当平均分区数量较大时（Cassandra 默认情况下，每个节点有 256 个分区），新节点最终会从现有节点中拿走相当数量的负载。Cassandra 在 3.0 时推出了改进算洁，可以避免上述不公平的分裂。

随机选择分区边界的前提要求采用基于哈希分区（可以从哈希函数产生的数字范围里设置边界）。这种方法也最符合本章开头所定义一致性哈希。一些新设计的哈希函数也可以以较低的元数据开销达到类似的效果。

:::

### 【高级】如何确定读写请求发往哪个节点？

:::details 要点

当数据集分布到多个节点上，需要解决一个问题：当客户端发起请求时，如何知道应该连接哪个节点？如果发生了分区再平衡，分区与节点的对应关系随之还会变化。

这其实属于一类典型的服务发现问题，任何通过网络访问的系统都有这样的需求，尤其是当服务目标支持高可用时（在多台机器上有冗余配置）。

服务发现有以下处理策略：

1. 允许客户端链接任意的节点（例如，采用循环式的负载均衡器）。如果某节点恰好拥有所请求的分区，则直接处理该请求：否则，将请求转发到下一个合适的节点，接收答复，并将答复返回给客户端。
2. 将所有客户端的请求都发送到一个路由层，由后者负责将请求转发到对应的分区节点上。路由层本身不处理任何请求，它仅充一个分区感知的负载均衡器。
3. 客户端感知分区和节点分配关系。此时，客户端可以直接连接到目标节点，而不需要任何中介。

![img](https://raw.githubusercontent.com/dunwu/images/master/snap/20220304120137.png)

许多分布式数据系统依靠独立的协调服务（如 ZooKeeper ）跟踪集群范围内的元数据。每个节点都向 ZooKeeper 中注册自己， ZooKeeper 维护了分区到节点的最终映射关系。其他参与者（如路由层或分区感知的客户端）可以向 ZooKeeper 订阅此信息。一旦分区发生了改变，或者添加、删除节点， ZooKeeper 就会主动通知路由层，这样使路由信息保持最新状态。

例如，HBase、SolrCloud 和 Kafka 也使用 ZooKeeper 来跟踪分区分配情况。MongoDB 有类似的设计，但它依赖于自己的配置服务器和 mongos 守护进程来充当路由层。

Cassandra 和 Redis 则采用了不同的方法，它们在节点之间使用 gossip 协议来同步群集状态的变化。请求可以发送到任何节点，由该节点负责将其转发到目标分区节点。这种方式增加了数据库节点的复杂性，但是避免了对 ZooKeeper 之类的外部协调服务的依赖。

![img](https://raw.githubusercontent.com/dunwu/images/master/snap/20220304163629.png)

:::

## 共识

## 分布式事务

### 【基础】什么是事务？什么是分布式事务？

### 【基础】什么是 ACID？

### 【基础】什么是 BASE？什么是最终一致性？

### 【中级】有哪些分布式事务解决方案？各有什么利弊？

### 【中级】2PC 是如何工作的？

### 【中级】3PC 是如何工作的？

### 【中级】TCC 是如何工作的？

### 【高级】本地消息表是如何工作的？

### 【高级】消息事务是如何工作的？

### 【高级】SAGA 事务是如何工作的？

## 分布式锁

## 参考资料

- [**数据密集型应用系统设计**](https://book.douban.com/subject/30329536/) - 这可能是目前最好的分布式存储书籍，强力推荐【进阶】