---
title: 分布式协同面试
date: 2024-12-16 23:57:04
categories:
  - 分布式
  - 分布式协同
tags:
  - 分布式
  - 协同
  - 面试
permalink: /pages/808cce55/
---

# 分布式协同面试

## 复制

### 【简单】什么是复制？复制有什么作用？

**复制**是将同一份数据存储在多台机器上，以提升系统的可用性、可靠性和性能。

复制数据，可能出于各种各样的原因：

- **提高可用性**：当部分组件出现位障，系统依然可以继续工作，系统依然可以继续工作。
- **降低访问延迟**：使数据在地理位置上更接近用户。
- **提高读吞吐量**：扩展至多台机器以同时提供数据访问服务。

### 【中等】复制有哪些模式？

复制的模式有以下几种：

- **主从复制**：**所有的写入操作都发送到主节点**，由主节点负责将数据更改事件发送到从节点。每个从节点都可以接收读请求，但内容可能是过期值。支持主从复制的系统：
  - 数据库：Mysql、PostgreSQL、MongoDB 等
  - 消息队列：Kafka、RabbitMQ 等
- **多主复制**：**系统存在多个主节点，每个都可以接收写请求**，客户端将写请求发送到其中的一个主节点上，由该主节点负责将数据更改事件同步到其他主节点和自己的从节点。
- **无主复制**：**系统中不存在主节点，每一个节点都能接受客户端的写请求**。此外，**读取时从多个节点上并行读取，以此检测和纠正某些过期数据**。支持无主复制的系统：
  - 数据库：Cassandra

此外，复制还需要考虑以下问题：

- **同步还是异步**
- **如何处理失败的副本**
- **如何保证数据一致**

### 【中等】主从复制的工作原理是什么？⭐⭐

**主节点负责写操作，从节点同步主节点数据并处理读操作**。

最常见的解决方案就是主从复制，其原理如下：

主从复制模式中只有一个主副本（或称为主节点） ，其余称为从副本（或称为从节点）。

1. 所有的写请求只能发送给主副本，主副本首先将新数据写入本地存储。
2. 然后，主副本将数据更改作为复制的日志或更新流发送给所有从副本。每个从副本获得更新数据之后将其应用到本地，且严格保持与主副本相同的写入顺序。
3. 读请求既可以在主副本上，也可以在从副本上执行。

再次强调，**只有主副本才可以接受写请求**：从客户端的角度来看，从副本都是只读的。如果由于某种原因，例如与主节点之间的网络中断而导致主节点无法连接，主从复制方案就会影响所有的写入操作。

![主从复制系统](https://raw.githubusercontent.com/dunwu/images/master/archive/2022/03/1047cda884e64a49b0e59d9fb3ab6326.png)

### 【中等】同步复制、半同步复制、异步复制有什么差异？⭐⭐

![主从复制——同步和异步](https://raw.githubusercontent.com/dunwu/images/master/archive/2022/03/bca92c92811d4f038e3cc8ccee91b964.png)

一般，复制速度会非常快；但是，系统不能保证复制多久能完成。有些情况下，从节点可能落后主节点几分钟甚至更长时间，例如：从节点刚从故障中恢复；或系统已经接近最大设计上限；或节点之间的网络出现问题。

**全同步复制**的优缺点：

- **优点**：只有所有从节点都完成复制，才视为成功，因此是**强一致的**。
- **缺点**：即使只有一个从节点未完成复制，写入都不能视为成功。所有从节点完成复制过程之前，主节点会**阻塞**后续所有的写操作。

因此，**把所有从节点都配置为同步复制有些不切实际**。因为这样的话，任何一个同步节点的中断都会导致整个系统更新停滞不前。

**全异步复制**的优缺点：

- **优点**：不管从节点上数据多么滞后，主节点总是可以继续响应写请求，**系统的性能更好**。
- **缺点**：如果主节点发生故障且不可恢复，则**所有尚未复制到从节点的写请求都会丢失**。

还有一种折中的方案——**半同步复制**：**只要有一个从节点或半数以上的从节点同步成功，就视为同步，直接返回结果；剩下的节点都通过异步方式同步**。万一同步的从节点变得不可用或性能下降，则将另一个异步的从节点提升为同步模式。这样可以保证至少有两个节点（即主节点和一个同步从节点）拥有最新的数据副本。

### 【中等】新的从节点如何复制主节点数据？

两种不可行的方案：

- 由于主节点会源源不断接受新的写入数据，数据始终处于变化中，因此**一次性从主节点复制数据到从节点是无法保证数据一致的**。
- 另一种思路是：考虑**锁定数据库**（使其不可写）来使磁盘上的文件保持一致，但这会**违反高可用的设计目标**。

可行的方案：**初始化连接→全量数据同步→增量数据同步**

1. 生成主节点某时刻的快照，避免长时间锁定数据库。
2. 将快照复制到从节点。
3. 从节点复制主节点快照过程中，所有的数据变更写入一个日志中（这个数据变更日志在不同数据库中有着不同的称呼，Mysql 称其为 binlog；Redis 称其为 AOF）。
4. 从节点复制完主节点的快照后，请求数据变更日志中的数据，并基于此补全数据，这个过程称为**追赶**，直至主从数据一致。井重复步骤 1 ～步骤 4 。

### 【困难】如何通过主从复制技术来实现系统高可用呢？

**主从复制实现高可用 = 数据冗余（一主多从） + 心跳监控 + 自动选主切换 + 客户端重定向**

::: info 数据冗余

:::

- 一个**主节点**（写+读），多个**从节点**（读+备份）。
- 数据从主**实时/近实时**复制到从。

从节点的本地磁盘上都保存了副本收到的数据变更日志。如果从节点从故障中恢复，可以和主节点对比数据变更日志的偏移量，从而确认数据是否滞后。如果数据存在滞后，则向主节点请求数据变更日志，并补全数据。这个过程称为**追赶**。

::: info 故障检测

:::

- **监控服务**（如 Prometheus、ZooKeeper）持续检查主节点健康。
- 常用方法：心跳续活、超时判断

::: info 自动切换

:::

- 主节点故障时，**选主算法**（如 Raft、Paxos）自动从从节点中选举新主。
- **配置更新**：客户端/中间件（如 Proxy）自动将写请求路由到新主。

主节点失效后，需要选举出新主节点。然后，客户端需要更新路由，将所有写请求发送给新的主节点；其他从节点要接受来自新的主节点上的变更数据。这个过程称之为**切换**。

主节点切换可以手动或自动进行。自动切换的步骤通常如下：

1. **确认主节点失效**。有很多种出错可能性，很难准确检测出问题的原因。所以，大多数系统都基于超时机制来确认主节点是否失效：节点间频繁地互相发生发送心跳存活悄息，如果发现某一个节点在一段比较长时间内没有响应，即认为该节点发生失效。
2. **选举新的主节点**。基于多数派共识选主。候选节点最好与原主节点的数据差异最小，这样可以最小化数据丢失的风险。
3. **重新配置系统使新主节点生效**。客户端现在需要将写请求发送给新的主节点。原主节点若恢复，需降级处理，避免脑裂。

::: info 数据一致性

:::

- 切换后，确保新主拥有最新数据（避免脑裂）。
- 常用方法：**半同步复制**（确保至少一个从有最新数据）。

### 【困难】复制日志的工作原理是什么？

复制日志的实现方式：

- **基于语句的复制**：将数据写操作写入日志。主要缺点是**必须完全按照相同顺序执行**，否则可能会产生不同的结果。
- **基于预写日志（WAL）传输**：通常每个写操作都是以追加写的方式写入到日志中。主要缺点是**日志描述的数据结果非常底层**，如果数据库不同版本的存储格式存在差异，就可能无法兼容。
  - 对于日志结构存储引擎，日志是主要的存储方式。日志段在后台压缩井支持垃圾回收。
  - 对于采用覆写磁盘的 BTree 结构，每次修改会预先写入日志，如系统发生崩溃，通过索引更新的方式迅速恢复到此前一致状态。
- **基于行的逻辑日志复制**：如果复制和存储引擎采用不同的日志格式，这样复制与存储的逻辑就可以剥离。这种复制日志称为逻辑日志，以区分物理存储引擎的数据表示。
- **基于触发器的复制**：这种方式**很灵活**，可以定制化控制复制逻辑。主要缺点是复制**开销更高，也更容易出错**。

对比与选择：

- **追求性能与可靠** → **基于预写日志的复制**（适用于同构、同版本）。
- **追求灵活与兼容** → **基于行的逻辑日志复制**（现代数据库主流选择）。
- **特殊定制需求** → **基于触发器的复制**（需谨慎评估性能）。
- **遗留或简单场景** → **基于语句的复制**（不推荐用于强一致性要求）。

### 【困难】多主复制的工作原理是什么？

对主从复制模型进行自然的扩展，则可以配置多个主节点，每个主节点都可以接受写操作，后面复制的流程类似：处理写的每个主节点都必须将该数据更改转发到所有其他节点。这就是多主节点（ 也称为主－主，或主动／主动）复制。此时，每个主节点还同时扮演其他主节点的从节点。

在一个数据中心内部使用多主节点基本没有太大意义，其复杂性已经超过所能带来的好处。

但是，以下场景这种配置则是合理的：

- 多数据中心
- 离线客户端操作
- 协作编辑

有了多主节点复制模型，则可以在每个数据中心都配置主节点。在每个数据中心内，采用常规的主从复制方案；而在数据中心之间，由各个数据中心的主节点来负责同其他数据中心的主节点进行数据的交换、更新。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/05/76af97bebe6342488335154556743002.png)

部署单主节点的主从复制方案与多主复制方案之间的差异

- **性能**：对于主从复制，每个写请求都必须经由广域网传送至主节点所在的数据中心。这会大大增加写入延迟，井基本偏离了采用多数据中心的初衷（即就近访问）。而在多主节点模型中，每个写操作都可以在本地数据中心快速响应，然后采用异步复制方式将变化同步到其他数据中心。因此，对上层应用有效屏蔽了数据中心之间的网络延迟，使得终端用户所体验到的性能更好。
- **容忍数据中心失效**：对于主从复制，如果主节点所在的数据中心发生故障，必须切换至另一个数据中心，将其中的一个从节点被提升为主节点。在多主节点模型中，每个数据中心则可以独立于其他数据中心继续运行，发生故障的数据中心在恢复之后更新到最新状态。
- **容忍网络问题**：数据中心之间的通信通常经由广域网，它往往不如数据中心内的本地网络可靠。对于主从复制模型，由于写请求是同步操作，对数据中心之间的网络性能和稳定性等更加依赖。多主节点模型则通常采用异步复制，可以更好地容忍此类问题，例如临时网络闪断不会妨碍写请求最终成功。

### 【困难】无主复制的工作原理是什么？

无主复制模式，**系统中不存在主节点，每一个节点都能接受客户端的写请求**。此外，**读取时从多个节点上并行读取，以此检测和纠正某些过期数据**。

::: info 无主复制流程

:::

- **写入时**：
  - 客户端**并行**将数据写入 **N 个副本节点**（W 个成功即返回）。
  - **无固定主节点**，所有节点平等。
- **读取时**：
  - 客户端**并行**从 **N 个副本节点**读取数据（读取 R 个响应）。
  - 通过**版本号**（向量时钟、时间戳）识别最新数据。
- **修复与同步**：
  - **读修复**：读取时若发现旧副本，立即用新数据修复它。
  - **反熵**：后台进程持续同步副本，弥合差异。

::: info QuorumNWR 算法

:::

无主复制模式中，究竟多少个副本完成才可以认为写成功？

如果有 N 个副本，写人需要 W 个节点确认，读取必须至少查询 R 个节点， 则只要 `W + R > N`，读取的节点中一定会包含最新值。即：确保**读取集 (R)** 和**写入集 (W)** **必有重叠**，从而一定能读到最新写入。

::: info 并发写冲突

:::

无主模式中，并发向多副本写操作，以及读时修复或数据回传都会导致并发写冲突。如何解决冲突呢？有以下几种机制：

- **最后写入获胜 (LWW)**：简单但可能丢失并发写入。
- **版本向量**：跟踪因果历史，识别并发冲突，交应用层解决。

## 分区

### 【简单】什么是分区？为什么要分区？

**分区（Partitioning）**：将大数据集**水平切分**成多个独立子集，分散到不同节点存储与管理。

分区的核心思想是：**分而治之**。

分区的目的：

- **突破单机极限**：**数据量**、**吞吐量**
- **提升扩展性**：数据与负载**线性扩展**：加节点 → 加容量与性能。
- **实现局部性优化**：将数据就近部署到用户所在区域（地理分区），降低访问延迟。

### 【中等】分区有哪些模式？⭐⭐

分区通常与复制结合使用，即每个分区在多个节点都存有副本。这意味着某条记录属于特定的分区，而同样的内容会保存在不同的节点上以提高系统的容错性。

一个节点上可能存储了多个分区。每个分区都有自己的主副本，例如被分配给某节点，而从副本则分配在其他一些节点。一个节点可能既是某些分区的主副本，同时又是其他分区的从副本。

分区主要有两种模式：

- **范围分区**：先对关键字进行排序，每个分区只负责一段包含最小到最大关键字范围的一段关键字。对关键字排序的优点是可以支持高效的区间查询，但是如果应用程序经常访问与排序一致的某段关键字，就会存在热点的风险。采用这种方怯，当分区太大时，通常将其分裂为两个子区间，从而动态地再平衡分区。典型代表：HBase
- **哈希分区**：将哈希函数作用于每个关键字，每个分区负责一定范围的哈希值。这种方法打破了原关键字的顺序关系，它的区间查询效率比较低，但可以更均匀地分配负载。采用哈希分区时，通常事先创建好足够多（但固定数量）的分区， 让每个节点承担多个分区，当添加或删除节点时将某些分区从一个节点迁移到另一个节点，也可以支持动态分区。典型代表：Elasticsearch、Redis。

### 【困难】二级索引如何分区？

二级索引是关系数据库的必备特性，在文档数据库中应用也非常普遍。但考虑到其复杂性，许多键值存储（如 HBase 和 Voldemort）并不支持二级索引。此外， 二级索引技术也是 Solr 和 Elasticsearch 等搜索引擎数据库存在之根本。

分区不仅仅是针对数据，二级索引也需要分区。通常有两种方法：

**基于文档来分区二级索引（本地索引）**：二级索引存储在与关键字相同的分区中，这意味着写入时我们只需要更新一个分区，但缺点是读取二级索引时需要在所有分区上并行执行。它广泛用于实践： MongoDB 、Riak、Cassandra、Elasticsearch 、SolrCloud 和 VoltDB 都支持基于文档分区二级索引。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2022/03/c18212d6979c4c98be8203fb32e77dd5.png)

**基于词条来分区二级索引（全局索引）**：它是基于索引的值而进行的独立分区。二级索引中的条目可能包含来自关键字的多个分区里的记录。在写入时，不得不更新二级索引的多个分区；但读取时，则可以从单个分区直接快速提取数据。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2022/03/75756f67bc3d47e287fe78fafcc47037.png)

### 【简单】什么是分区再均衡？

**分区再均衡**：当集群节点数量发生变化时，**自动重新分区，使得集群分布均匀**的过程。

触发时机：

- **集群伸缩**：增加节点（扩容）或减少节点（缩容）。
- **数据倾斜**：某个节点负载过高（热点分区）。
- **节点故障**：故障节点被移除。

### 【困难】分区再均衡有哪些策略？⭐⭐⭐

::: info 固定数量的分区

:::

> 要点：
>
> - 预先创建**远多于节点数**的固定分区（如 1000 个）。
> - 增删节点时，只需**在节点间转移部分分区**，无需修改分区键范围。

创建远超实际节点数的分区数，然后为每个节点分配多个分区。接下来， 如果集群中添加了一个新节点，该新节点可以从每个现有的节点上匀走几个分区，直到分区再次达到全局平衡。

选中的整个分区会在节点之间迁移，但分区的总数量仍维持不变，也不会改变关键字到分区的映射关系。这里唯一要调整的是分区与节点的对应关系。考虑到节点间通过网络传输数据总是需要些时间，这样调整可以逐步完成，在此期间，旧分区仍然可以接收读写请求。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/05/a2a6bdaf17cc4972847c4a05ffe1a2dc.png)

原则上，也可以将集群中的不同的硬件配置因素考虑进来，即性能更强大的节点将分配更多的分区，从而分担更多的负载。

目前，Riak、Elasticsearch、Couchbase 和 Voldemort 都支持这种动态平衡方法。

使用该策略时，分区的数量往往在数据库创建时就确定好，之后不会改变。原则上也可以拆分和合并分区（稍后介绍），但固定数量的分区使得相关操作非常简单，因此许多采用固定分区策略的数据库决定不支持分区拆分功能。所以，在初始化时，已经充分考虑将来扩容增长的需求（未来可能拥有的最大节点数），设置一个足够大的分区数。而每个分区也有些额外的管理开销，选择过高的数字可能会有副作用。

::: info 动态分区

:::

> 分区**按数据量自动分裂与合并**（如达到阈值就分裂）。

对于采用关键宇区间分区的数据库，如果边界设置有问题，最终可能会出现所有数据都挤在一个分区而其他分区基本为空，那么设定固定边界、固定数量的分区将非常不便：而手动去重新配置分区边界又非常繁琐。

因此， 一些数据库如 HBase 和 RethinkDB 等采用了动态创建分区。当分区的数据增长超过一个可配的参数阔值（HBase 上默认值是 10GB），它就拆分为两个分区，每个承担一半的数据量。相反，如果大量数据被删除，并且分区缩小到某个阈值以下，则将其与相邻分区进行合井。该过程类似于 B 树的分裂操作。

每个分区总是分配给一个节点，而每个节点可以承载多个分区，这点与固定数量的分区一样。当一个大的分区发生分裂之后，可以将其中的一半转移到其他某节点以平衡负载。对于 HBase，分区文件的传输需要借助 HDFS。

动态分区的一个优点是分区数量可以自动适配数据总量。如果只有少量的数据，少量的分区就足够了，这样系统开销很小；如果有大量的数据，每个分区的大小则被限制在一个可配的最大值。

但是，需要注意的是，对于一个空的数据库， 因为没有任何先验知识可以帮助确定分区的边界，所以会从一个分区开始。可能数据集很小，但直到达到第一个分裂点之前，所有的写入操作都必须由单个节点来处理， 而其他节点则处于空闲状态。为了缓解这个问题，HBase 和 MongoDB 允许在一个空的数据库上配置一组初始分区（这被称为预分裂）。对于关键字区间分区，预分裂要求已经知道一些关键字的分布情况。

动态分区不仅适用于关键字区间分区，也适用于基于哈希的分区策略。MongoDB 从版本 2.4 开始，同时支持二者，井且都可以动态分裂分区。

::: info 按节点比例分区

:::

> 每个节点持有固定数量的分区（如 Redis Cluster 的哈希槽）。

采用动态分区策略，拆分和合并操作使每个分区的大小维持在设定的最小值和最大值之间，因此分区的数量与数据集的大小成正比关系。另一方面，对于固定数量的分区方式，其每个分区的大小也与数据集的大小成正比。两种情况，分区的数量都与节点数无关。

Cassandra 和 Ketama 则采用了第三种方式，使分区数与集群节点数成正比关系。换句话说，每个节点具有固定数量的分区。此时， 当节点数不变时，每个分区的大小与数据集大小保持正比的增长关系； 当节点数增加时，分区则会调整变得更小。较大的数据量通常需要大量的节点来存储，因此这种方法也使每个分区大小保持稳定。

当一个新节点加入集群时，它随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一半数据量，将另一半数据留在原节点。随机选择可能会带来不太公平的分区分裂，但是当平均分区数量较大时（Cassandra 默认情况下，每个节点有 256 个分区），新节点最终会从现有节点中拿走相当数量的负载。Cassandra 在 3.0 时推出了改进算洁，可以避免上述不公平的分裂。

随机选择分区边界的前提要求采用基于哈希分区（可以从哈希函数产生的数字范围里设置边界）。这种方法也最符合本章开头所定义一致性哈希。一些新设计的哈希函数也可以以较低的元数据开销达到类似的效果。

### 【困难】如何确定读写请求发往哪个节点？⭐

当数据集分布到多个节点上，需要解决一个问题：当客户端发起请求时，如何知道应该连接哪个节点？如果发生了分区再平衡，分区与节点的对应关系随之还会变化。

这其实属于一类典型的服务发现问题，任何通过网络访问的系统都有这样的需求，尤其是当服务目标支持高可用时（在多台机器上有冗余配置）。

服务发现有以下处理策略：

- **客户端路由**：
  - 客户端**内置/依赖 SDK**，知晓分区与节点映射（元数据）。
  - 直接连接目标节点，无中间跳转。
  - **优点**：低延迟，架构简单。
  - **缺点**：客户端需感知拓扑变化。
  - **代表**：Cassandra、Redis Cluster。
- **代理路由**：
  - 请求先发往**独立的代理/中间件**（如 ProxySQL、MyCat）。
  - 代理根据路由规则转发到正确节点。
  - **优点**：客户端无感知，集中管理。
  - **缺点**：增加一跳延迟，代理可能成瓶颈。
  - **代表**：数据库中间件、服务网格 Sidecar。
- **服务端重定向**：
  - 客户端先发请求到任意节点，若数据不在该节点，则收到**重定向响应**（含正确节点地址）。
  - 客户端再向正确节点发送请求。
  - **优点**：客户端无初始元数据。
  - **缺点**：增加一次往返。
  - **代表**：MongoDB 分片集群（`mongos`路由）、DynamoDB（SDK 缓存路由表）。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2022/03/38f37684621a4dc48852b1926de3d80c.png)

许多分布式数据系统依靠独立的协调服务（如 ZooKeeper ）跟踪集群范围内的元数据。每个节点都向 ZooKeeper 中注册自己， ZooKeeper 维护了分区到节点的最终映射关系。其他参与者（如路由层或分区感知的客户端）可以向 ZooKeeper 订阅此信息。一旦分区发生了改变，或者添加、删除节点， ZooKeeper 就会主动通知路由层，这样使路由信息保持最新状态。

例如，HBase、SolrCloud 和 Kafka 也使用 ZooKeeper 来跟踪分区分配情况。MongoDB 有类似的设计，但它依赖于自己的配置服务器和 mongos 守护进程来充当路由层。

Cassandra 和 Redis 则采用了不同的方法，它们在节点之间使用 gossip 协议来同步群集状态的变化。请求可以发送到任何节点，由该节点负责将其转发到目标分区节点。这种方式增加了数据库节点的复杂性，但是避免了对 ZooKeeper 之类的外部协调服务的依赖。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2022/03/22baa4044a4c466eb44ef7ef5a0f7f2f.png)

## 分布式事务

::: tip 扩展

- [理解分布式事务](https://juejin.cn/post/6844903734753886216?searchId=2024121710293693FB283CD941B5B19BE2)
- [分布式事务](https://dunwu.github.io/waterdrop/pages/36844fb1/)

:::

### 【简单】什么是事务？什么是分布式事务？

**事务将多个读、写操作捆绑在一起成为一个逻辑操作单元**。**事务中的所有读写是一个执行的整体，整个事务要么成功（提交）、要么失败（中止或回滚）**。

在单一数据节点中，事务仅限于对单一数据库资源的访问控制，称之为**本地事务**。几乎所有的成熟的关系型数据库都提供了对本地事务的原生支持。

**分布式事务指的是事务操作跨越多个节点，并且要求满足事务的 ACID 特性。**

### 【简单】什么是 ACID？什么是 BASE？二者有何区别？

::: info ACID

:::

ACID 是数据库事务正确执行的四个基本要素的单词缩写：

- **原子性（Atomicity）**
  - 原子是指不可分解为更小粒度的东西。事务的原子性意味着：**事务中的所有操作要么全部成功，要么全部失败**。
  - 回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。
  - ACID 中的原子性并不关乎多个操作的并发性，它并没有描述多个线程试图访问相同的数据会发生什么情况，后者其实是由 ACID 的隔离性所定义。
- **一致性（Consistency）**
  - 数据库在事务执行前后都保持一致性状态。
  - 在一致性状态下，所有事务对一个数据的读取结果都是相同的。
  - 一致性本质上要求应用层来维护状态一致（或者恒等），应用程序有责任正确地定义事务来保持一致性。这不是数据库可以保证的事情。
- **隔离性（Isolation）**
  - **同时运行的事务互不干扰**。换句话说，一个事务所做的修改在最终提交以前，对其它事务是不可见的。
- **持久性（Durability）**
  - 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。
  - 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。

::: info BASE

:::

BASE 是 **`基本可用（Basically Available）`**、**`软状态（Soft State）`** 和 **`最终一致性（Eventually Consistent）`** 三个短语的缩写。

BASE 理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。

- **基本可用（Basically Available）**分布式系统在出现故障的时候，**保证核心可用，允许损失部分可用性**。例如，电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面。
- **软状态（Soft State）**指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即**允许系统不同节点的数据副本之间进行同步的过程存在延时**。
- **最终一致性（Eventually Consistent）**强调的是**系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态**。

::: info BASE vs. ACID

:::

ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过**牺牲强一致性来达到可用性**，通常运用在大型分布式系统中。BASE 唯一可以确定的是“它不是 ACID”，此外它几乎没有承诺任何东西。

### 【简单】什么是强一致性？什么是最终一致性？

一致性（Consistency）指的是**多个数据副本是否能保持一致**的特性。

数据一致性又可以分为以下几点：

- **强一致性**：数据更新操作结果和操作响应总是一致的，即操作响应通知更新失败，那么数据一定没有被更新，而不是处于不确定状态。
- **最终一致性**：即物理存储的数据可能是不一致的，终端用户访问到的数据可能也是不一致的，但系统经过一段时间的自我修复和修正，数据最终会达到一致。

在分布式领域，要实现强一致性，代价非常高昂。因此，有人基于 CAP 理论以及 BASE 理论，有人就提出了**柔性事务**的概念。柔性事务是指：在不影响系统整体可用性的情况下 (Basically Available 基本可用），允许系统存在数据不一致的中间状态 (Soft State 软状态），在经过数据同步的延时之后，达到**最终一致性**。**并不是完全放弃了 ACID，而是通过放宽一致性要求，借助本地事务来实现最终分布式事务一致性的同时也保证系统的吞吐**。

### 【中等】分布式事务有哪些解决方案？各有什么利弊？⭐⭐⭐

分布式事务的常见方案如下：

- **两阶段提交（2PC）**：将事务的提交过程分为两个阶段来进行处理：准备阶段和提交阶段。参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。
- **三阶段提交（3PC）**：与二阶段提交不同的是，引入超时机制。同时在协调者和参与者中都引入超时机制。将二阶段的准备阶段拆分为 2 个阶段，插入了一个 preCommit 阶段，使得原先在二阶段提交中，参与者在准备之后，由于协调者发生崩溃或错误，而导致参与者处于无法知晓是否提交或者中止的“不确定状态”所产生的可能相当长的延时的问题得以解决。
- **补偿事务（TCC）**
  - **Try**：操作作为一阶段，负责资源的检查和预留。
  - **Confirm**：操作作为二阶段提交操作，执行真正的业务。
  - **Cancel**：是预留资源的取消。
- **本地消息表**：在事务主动发起方额外新建事务消息表，事务发起方处理业务和记录事务消息在本地事务中完成，轮询事务消息表的数据发送事务消息，事务被动方基于消息中间件消费事务消息表中的事务。
- **事务消息**：基于 MQ 的分布式事务方案其实是对本地消息表的封装。
- **SAGA**：Saga 事务核心思想是将长事务拆分为多个本地短事务，由 Saga 事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。

分布式事务方案对比：

- **要强一致，不怕慢** → **2PC、3PC**
- **强一致，性能中等，但侵入性高**：**TCC**
- **要高性能，可接受最终一致** → **本地消息表、MQ 事务消息、SAGA**
- **长事务，跨多服务** → **SAGA**
- **快速集成，有现成框架** → **Seata**

|            | 2PC | 3PC | TCC | 本地消息表 | MQ 事务消息 | SAGA |
| ---------- | --- | --- | --- | ---------- | ----------- | ---- |
| 数据一致性 | 强  | 强  | 若  | 弱         | 弱          | 弱   |
| 容错性     | 低  | 低  | 高  | 高         | 高          | 高   |
| 复杂性     | 中  | 高  | 高  | 低         | 低          | 高   |
| 性能       | 低  | 低  | 中  | 中         | 高          | 中   |
| 维护成本   | 低  | 中  | 高  | 中         | 中          | 高   |

### 【中等】2PC 的工作原理是什么？⭐⭐⭐

二阶段提交协议（Two-phase Commit，即 2PC）**将事务的提交过程分为两个阶段来进行处理：准备阶段和提交阶段**。事务的发起者称协调者，事务的执行者称参与者。二阶段提交的思路可以概括为：**参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈，决定提交或回滚**。

**阶段 1：准备阶段**

1. 协调者向所有参与者发送事务内容，询问是否可以提交事务，并等待所有参与者答复。
2. 各参与者执行事务操作，将 undo 和 redo 信息记入事务日志中（但不提交事务）。
3. 如参与者执行成功，给协调者反馈 yes，即可以提交；如执行失败，给协调者反馈 no，即不可提交。

**阶段 2：提交阶段**

如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚 (rollback) 消息；否则，发送提交 (commit) 消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。（注意：必须在最后阶段释放锁资源） 接下来分两种情况分别讨论提交阶段的过程。

**情况 1，当所有参与者均反馈 yes，提交事务**。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/a61850ac9ae940f1a710e7da3ac568bd.png)

> 1. 协调者向所有参与者发出正式提交事务的请求（即 commit 请求）。
> 2. 参与者执行 commit 请求，并释放整个事务期间占用的资源。
> 3. 各参与者向协调者反馈 ack（应答）完成的消息。
> 4. 协调者收到所有参与者反馈的 ack 消息后，即完成事务提交。

**情况 2，任何一个参与者反馈 no，中断事务**。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/7f5027cb2dfe444cb4c57508acedde44.png)

> 1. 协调者向所有参与者发出回滚请求（即 rollback 请求）。
> 2. 参与者使用阶段 1 中的 undo 信息执行回滚操作，并释放整个事务期间占用的资源。
> 3. 各参与者向协调者反馈 ack 完成的消息。
> 4. 协调者收到所有参与者反馈的 ack 消息后，即完成事务中断。

方案总结：

2PC 方案实现起来简单，实际项目中使用比较少，主要因为以下问题：

- **性能问题**：所有参与者在事务提交阶段处于同步阻塞状态，占用系统资源，容易导致性能瓶颈。
- **可靠性问题**：如果协调者存在单点故障问题，如果协调者出现故障，参与者将一直处于锁定状态。
- **数据一致性问题**：在阶段 2 中，如果发生局部网络问题，一部分事务参与者收到了提交消息，另一部分事务参与者没收到提交消息，那么就导致了节点之间数据的不一致。

### 【中等】3PC 的工作原理是什么？⭐⭐

三阶段提交协议（Three-phase Commit，3PC），是二阶段提交协议的改进版本，与二阶段提交不同的是，引入超时机制。同时在协调者和参与者中都引入超时机制。

**阶段 1：canCommit**

协调者向参与者发送 commit 请求，参与者如果可以提交就返回 yes 响应（参与者不执行事务操作），否则返回 no 响应：

1. 协调者向所有参与者发出包含事务内容的 canCommit 请求，询问是否可以提交事务，并等待所有参与者答复。
2. 参与者收到 canCommit 请求后，如果认为可以执行事务操作，则反馈 yes 并进入预备状态，否则反馈 no。

**阶段 2：preCommit**

协调者根据阶段 1 canCommit 参与者的反应情况来决定是否可以基于事务的 preCommit 操作。根据响应情况，有以下两种可能。

**情况 1：阶段 1 所有参与者均反馈 yes，参与者预执行事务**。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/238831c899bc46a7a9a370e39d400390.png)

> 1. 协调者向所有参与者发出 preCommit 请求，进入准备阶段。
> 2. 参与者收到 preCommit 请求后，执行事务操作，将 undo 和 redo 信息记入事务日志中（但不提交事务）。
> 3. 各参与者向协调者反馈 ack 响应或 no 响应，并等待最终指令。

**情况 2：阶段 1 任何一个参与者反馈 no，或者等待超时后协调者尚无法收到所有参与者的反馈，即中断事务**。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/c91a839204454bc0b7ede485434c145d.png)

> 1. 协调者向所有参与者发出 abort 请求。
> 2. 无论收到协调者发出的 abort 请求，或者在等待协调者请求过程中出现超时，参与者均会中断事务。

**阶段 3：doCommit**

该阶段进行真正的事务提交，也可以分为以下两种情况：

**情况 1：阶段 2 所有参与者均反馈 ack 响应，执行真正的事务提交**。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/a0e6a29dc8e042ddb2d9940af9991162.png)

> 1. 如果协调者处于工作状态，则向所有参与者发出 doCommit 请求。
> 2. 参与者收到 doCommit 请求后，会正式执行事务提交，并释放整个事务期间占用的资源。
> 3. 各参与者向协调者反馈 ack 完成的消息。
> 4. 协调者收到所有参与者反馈的 ack 消息后，即完成事务提交。

**情况 2：任何一个参与者反馈 no，或者等待超时后协调者尚无法收到所有参与者的反馈，即中断事务**。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/2c635073eceb473b8325921ac02c8f61.png)

> 1. 如果协调者处于工作状态，向所有参与者发出 abort 请求。
> 2. 参与者使用阶段 1 中的 undo 信息执行回滚操作，并释放整个事务期间占用的资源。
> 3. 各参与者向协调者反馈 ack 完成的消息。
> 4. 协调者收到所有参与者反馈的 ack 消息后，即完成事务中断。

注意：进入阶段 3 后，无论协调者出现问题，或者协调者与参与者网络出现问题，都会导致参与者无法接收到协调者发出的 doCommit 请求或 abort 请求。此时，参与者都会在等待超时之后，继续执行事务提交。

**方案总结**：

- 优点：**相比二阶段提交，三阶段降低了阻塞范围**，在**等待超时后协调者或参与者会中断事务**。避免了协调者单点问题，阶段 3 中协调者出现问题时，参与者会继续提交事务。

- 缺点：**数据不一致问题依然存在**，当在参与者收到 preCommit 请求后等待 doCommit 指令时，此时如果协调者请求中断事务，而协调者无法与参与者正常通信，会导致参与者继续提交事务，造成数据不一致。

### 【中等】TCC 的工作原理是什么？⭐⭐

TCC 是服务化的二阶段编程模型，其 Try、Confirm、Cancel 3 个方法均由业务编码实现；

- **Try**：操作作为一阶段，负责资源的检查和预留。
- **Confirm**：操作作为二阶段提交操作，执行真正的业务。
- **Cancel**：是预留资源的取消。

TCC 事务的 Try、Confirm、Cancel 可以理解为 SQL 事务中的 Lock、Commit、Rollback。

**Try 阶段**

从执行阶段来看，与传统事务机制中业务逻辑相同。但从业务角度来看，却不一样。TCC 机制中的 Try 仅是一个初步操作，它和后续的确认一起才能真正构成一个完整的业务逻辑，这个阶段主要完成：

- 完成所有业务检查（一致性）
- 预留必须业务资源（准隔离性）
- Try 尝试执行业务 TCC 事务机制以初步操作（Try）为中心的，确认操作（Confirm）和取消操作（Cancel）都是围绕初步操作（Try）而展开。因此，Try 阶段中的操作，其保障性是最好的，即使失败，仍然有取消操作（Cancel）可以将其执行结果撤销。

假设商品库存为 100，购买数量为 2，这里检查和更新库存的同时，冻结用户购买数量的库存，同时创建订单，订单状态为待确认。

**Confirm / Cancel 阶段**

根据 Try 阶段服务是否全部正常执行，继续执行确认操作（Confirm）或取消操作（Cancel）。 Confirm 和 Cancel 操作满足幂等性，如果 Confirm 或 Cancel 操作执行失败，将会不断重试直到执行完成。

**Confirm：当 Try 阶段服务全部正常执行， 执行确认业务逻辑操作**

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/78a02f8b51fe4af1864e5a8300b5941b.png)

这里使用的资源一定是 Try 阶段预留的业务资源。在 TCC 事务机制中认为，如果在 Try 阶段能正常的预留资源，那 Confirm 一定能完整正确的提交。Confirm 阶段也可以看成是对 Try 阶段的一个补充，Try+Confirm 一起组成了一个完整的业务逻辑。

**Cancel：当 Try 阶段存在服务执行失败， 进入 Cancel 阶段**

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/78a02f8b51fe4af1864e5a8300b5941b.png)

Cancel 取消执行，释放 Try 阶段预留的业务资源，上面的例子中，Cancel 操作会把冻结的库存释放，并更新订单状态为取消。

**方案总结**

TCC 事务机制相比于上面介绍的 XA 事务机制，有以下优点：

- **性能提升**：具体业务来实现控制资源锁的粒度变小，不会锁定整个资源。
- **数据最终一致性**：基于 Confirm 和 Cancel 的幂等性，保证事务最终完成确认或者取消，保证数据的一致性。
- **可靠性**：解决了 XA 协议的协调者单点故障问题，由主业务方发起并控制整个业务活动，业务活动管理器也变成多点，引入集群。

缺点： TCC 的 Try、Confirm 和 Cancel 操作功能要按具体业务来实现，**业务耦合度较高**，提高了开发成本。

### 【困难】SAGA 事务的工作原理是什么？⭐

Saga 事务的核心思想是：将长事务拆分为多个本地短事务，由 Saga 事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序依次调用补偿操作。

**Saga 事务基本协议如下**：

- **将长事务拆分为多个有序子事务**：每个 Saga 事务由一系列幂等的有序子事务 (sub-transaction) Ti 组成。
- **每个子事务 Ti 都有对应的幂等补偿动作 Ci**，补偿动作用于撤销 Ti 造成的结果。

可以看到，和 TCC 相比，Saga 没有“预留”动作，它的 Ti 就是直接提交到库。

下面以下单流程为例，整个操作包括：创建订单、扣减库存、支付、增加积分 Saga 的执行顺序有两种：

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/81b1a5b1c17b47308e5a619cb2d4ec1f.png)

- 事务正常执行完成 T1, T2, T3, ..., Tn，例如：扣减库存 (T1)，创建订单 (T2)，支付 (T3)，依次有序完成整个事务。
- 事务回滚 T1, T2, ..., Tj, Cj,..., C2, C1，其中 0 < j < n，例如：扣减库存 (T1)，创建订单 (T2)，支付 (T3，支付失败），支付回滚 (C3)，订单回滚 (C2)，恢复库存 (C1)。

恢复策略

Saga 定义了两种恢复策略：

- 向前恢复 (forward recovery)

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/c07847db73f549ab8586743118048338.png)

对应于上面第一种执行顺序，**适用于必须要成功的场景**，**失败需要进行重试**，执行顺序是类似于这样的：T1, T2, ..., Tj（失败）, Tj（重试）,..., Tn，其中 j 是发生错误的子事务 (sub-transaction)。该情况下不需要 Ci。

- 向后恢复 (backward recovery)

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/f601b72c91e345dbbc3ee0d58bd7dbde.png)

对应于上面提到的第二种执行顺序，其中 j 是发生错误的子事务 (sub-transaction)，这种做法的效果是撤销掉之前所有成功的子事务，使得整个 Saga 的执行结果撤销。

Saga 事务常见的有两种不同的实现方式：命令协调和事件编排。

**命令协调**

- **命令协调 (Order Orchestrator)：中央协调器负责集中处理事件的决策和业务逻辑排序。**

中央协调器（Orchestrator，简称 OSO）以命令/回复的方式与每项服务进行通信，全权负责告诉每个参与者该做什么以及什么时候该做什么。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/83948590ed9b400ab8b72f09a360535a.png)

以电商订单的例子为例：

> 1. 事务发起方的主业务逻辑请求 OSO 服务开启订单事务。
> 2. OSO 向库存服务请求扣减库存，库存服务回复处理结果。
> 3. OSO 向订单服务请求创建订单，订单服务回复创建结果。
> 4. OSO 向支付服务请求支付，支付服务回复处理结果。
> 5. 主业务逻辑接收并处理 OSO 事务处理结果回复。

中央协调器必须事先知道执行整个订单事务所需的流程（例如通过读取配置）。如果有任何失败，它还负责通过向每个参与者发送命令来撤销之前的操作来协调分布式的回滚。基于中央协调器协调一切时，回滚要容易得多，因为协调器默认是执行正向流程，回滚时只要执行反向流程即可。

**事件编排**

- **事件编排 (Event Choreography0：没有中央协调器（没有单点风险）时，每个服务产生并观察其他服务的事件，并决定是否应采取行动**。

在事件编排方法中，第一个服务执行一个事务，然后发布一个事件。该事件被一个或多个服务进行监听，这些服务再执行本地事务并发布（或不发布）新的事件。

当最后一个服务执行本地事务并且不发布任何事件时，意味着分布式事务结束，或者它发布的事件没有被任何 Saga 参与者听到都意味着事务结束。

以电商订单的例子为例：

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/45398110df5142e39356a65e1b878e74.png)

> 1. 事务发起方的主业务逻辑发布开始订单事件
> 2. 库存服务监听开始订单事件，扣减库存，并发布库存已扣减事件
> 3. 订单服务监听库存已扣减事件，创建订单，并发布订单已创建事件
> 4. 支付服务监听订单已创建事件，进行支付，并发布订单已支付事件
> 5. 主业务逻辑监听订单已支付事件并处理。

事件编排是实现 Saga 模式的自然方式，它很简单，容易理解，不需要太多的代码来构建。如果事务涉及 2 至 4 个步骤，则可能是非常合适的。

**方案总结**

**命令协调设计的优点和缺点：**

优点如下：

- 服务之间关系简单，避免服务之间的循环依赖关系，因为 Saga 协调器会调用 Saga 参与者，但参与者不会调用协调器
- 程序开发简单，只需要执行命令/回复（其实回复消息也是一种事件消息），降低参与者的复杂性。
- 易维护扩展，在添加新步骤时，事务复杂性保持线性，回滚更容易管理，更容易实施和测试

缺点如下：

- 中央协调器容易处理逻辑容易过于复杂，导致难以维护。
- 存在协调器单点故障风险。

**事件/编排设计的优点和缺点**

优点如下：

- 避免中央协调器单点故障风险。
- 当涉及的步骤较少服务开发简单，容易实现。

缺点如下：

- 服务之间存在循环依赖的风险。
- 当涉及的步骤较多，服务间关系混乱，难以追踪调测。

值得补充的是，由于 Saga 模型中没有 Prepare 阶段，因此事务间不能保证隔离性，当多个 Saga 事务操作同一资源时，就会产生更新丢失、脏数据读取等问题，这时需要在业务层控制并发，例如：在应用层面加锁，或者应用层面预先冻结资源。

### 【困难】本地消息表的工作原理是什么？⭐⭐

本地消息表的核心思路是将分布式事务拆分成本地事务进行处理。

方案通过在事务主动发起方额外新建事务消息表，事务发起方处理业务和记录事务消息在本地事务中完成，轮询事务消息表的数据发送事务消息，事务被动方基于消息中间件消费事务消息表中的事务。

这样设计可以避免”**业务处理成功 + 事务消息发送失败**"，或"**业务处理失败 + 事务消息发送成功**"的棘手情况出现，保证 2 个系统事务的数据一致性。

事务的主动方需要额外新建事务消息表，用于记录分布式事务的消息的发生、处理状态。

整个业务处理流程如下：

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/11f55331104f4302a23f996bd7467efd.png)

> 1. **步骤 1、事务主动方处理本地事务。** 事务主动发在本地事务中处理业务更新操作和写消息表操作。 上面例子中库存服务阶段再本地事务中完成扣减库存和写消息表（图中 1、2)。
> 2. **步骤 2、事务主动方通过 MQ 通知事务被动方处理事务**。 消息中间件可以基于 Kafka、RocketMQ 消息队列，事务主动方法主动写消息到消息队列，事务消费方消费并处理消息队列中的消息。 上面例子中，库存服务把事务待处理消息写到消息中间件，订单服务消费消息中间件的消息，完成新增订单（图中 3 - 5）。
> 3. **步骤 3、事务被动方通过 MQ 返回处理结果。** 上面例子中，订单服务把事务已处理消息写到消息中间件，库存服务消费中间件的消息，并将事务消息的状态更新为已完成（图中 6 - 8)

为了数据的一致性，当处理错误需要重试，事务发送方和事务接收方相关业务处理需要支持幂等。具体保存一致性的容错处理如下：

> - 当步骤 1 处理出错，事务回滚，相当于什么都没发生。
> - 当步骤 2、步骤 3 处理出错，由于未处理的事务消息还是保存在事务发送方，事务发送方可以定时轮询超时 d 的消息数据，再次发送消息到 MQ 进行处理。事务被动方消费事务消息重试处理。
> - 如果是业务上的失败，事务被动方可以发消息给事务主动方进行回滚。
> - 如果多个事务被动方已经消费消息，事务主动方需要回滚事务时需要通知事务被动方回滚。

**方案总结**

方案的优点如下：

- 从应用设计开发的角度实现了消息数据的可靠性，**消息数据的可靠性不依赖于消息中间件**，弱化了对 MQ 中间件特性的依赖。
- **方案简单**，容易实现。

缺点如下：

- 与具体的业务场景绑定，**耦合性高，不可复用**。
- 需要额外维护消息数据的传输，占用业务系统资源。
- 业务系统在使用关系型数据库的情况下，消息服务性能会受到关系型数据库并发性能的局限。

### 【困难】事务消息的工作原理是什么？⭐⭐

MQ 事务方案本质是利用 MQ 功能实现的本地消息表。事务消息需要消息队列提供相应的功能才能实现，Kafka 和 RocketMQ 都提供了事务相关功能。

- **Kafka** 的解决方案是：直接抛出异常，让用户自行处理。用户可以在业务代码中反复重试提交，直到提交成功，或者删除之前修改的数据记录进行事务补偿。
- **RocketMQ** 的解决方案是：通过事务反查机制来解决事务消息提交失败的问题。如果 Producer 在提交或者回滚事务消息时发生网络异常，RocketMQ 的 Broker 没有收到提交或者回滚的请求，Broker 会定期去 Producer 上反查这个事务对应的本地事务的状态，然后根据反查结果决定提交或者回滚这个事务。为了支撑这个事务反查机制，业务代码需要实现一个反查本地事务状态的接口，告知 RocketMQ 本地事务是成功还是失败。

事务消息是 Apache RocketMQ 提供的一种困难消息类型，支持在分布式场景下保障消息生产和本地事务的最终一致性。

**事务消息处理流程**

事务消息交互流程如下图所示。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2025/09/ee5fa22853b045119e12f9a96d41aec7.png)

1. 生产者将消息发送至 Apache RocketMQ 服务端。
2. Apache RocketMQ 服务端将消息持久化成功之后，向生产者返回 Ack 确认消息已经发送成功，此时消息被标记为"暂不能投递"，这种状态下的消息即为半事务消息。
3. 生产者开始执行本地事务逻辑。
4. 生产者根据本地事务执行结果向服务端提交二次确认结果（Commit 或是 Rollback），服务端收到确认结果后处理逻辑如下：
   - 二次确认结果为 Commit：服务端将半事务消息标记为可投递，并投递给消费者。
   - 二次确认结果为 Rollback：服务端将回滚事务，不会将半事务消息投递给消费者。
5. 在断网或者是生产者应用重启的特殊情况下，若服务端未收到发送者提交的二次确认结果，或服务端收到的二次确认结果为 Unknown 未知状态，经过固定时间后，服务端将对消息生产者即生产者集群中任一生产者实例发起消息回查。 **说明** 服务端回查的间隔时间和最大回查次数，请参见 [参数限制](https://rocketmq.apache.org/zh/docs/introduction/03limits)。
6. 生产者收到消息回查后，需要检查对应消息的本地事务执行的最终结果。
7. 生产者根据检查到的本地事务的最终状态再次提交二次确认，服务端仍按照步骤 4 对半事务消息进行处理。

**事务消息生命周期**

![事务消息](https://rocketmq.apache.org/zh/assets/images/lifecyclefortrans-fe4a49f1c9fdae5d590a64546722036f.png)

- **初始化**：半事务消息被生产者构建并完成初始化，待发送到服务端的状态。
- **事务待提交**：半事务消息被发送到服务端，和普通消息不同，并不会直接被服务端持久化，而是会被单独存储到事务存储系统中，等待第二阶段本地事务返回执行结果后再提交。此时消息对下游消费者不可见。
- **消息回滚**：第二阶段如果事务执行结果明确为回滚，服务端会将半事务消息回滚，该事务消息流程终止。
- **提交待消费**：第二阶段如果事务执行结果明确为提交，服务端会将半事务消息重新存储到普通存储系统中，此时消息对下游消费者可见，等待被消费者获取并消费。
- **消费中**：消息被消费者获取，并按照消费者本地的业务逻辑进行处理的过程。 此时服务端会等待消费者完成消费并提交消费结果，如果一定时间后没有收到消费者的响应，Apache RocketMQ 会对消息进行重试处理。具体信息，请参见 [消费重试](https://rocketmq.apache.org/zh/docs/featureBehavior/10consumerretrypolicy)。
- **消费提交**：消费者完成消费处理，并向服务端提交消费结果，服务端标记当前消息已经被处理（包括消费成功和失败）。 Apache RocketMQ 默认支持保留所有消息，此时消息数据并不会立即被删除，只是逻辑标记已消费。消息在保存时间到期或存储空间不足被删除前，消费者仍然可以回溯消息重新消费。
- **消息删除**：Apache RocketMQ 按照消息保存机制滚动清理最早的消息数据，将消息从物理文件中删除。更多信息，请参见 [消息存储和清理机制](https://rocketmq.apache.org/zh/docs/featureBehavior/11messagestorepolicy)。

**MQ 事务方案总结**

相比本地消息表方案，MQ 事务方案优点是：

- **业务解耦**：消息数据独立存储 ，降低业务系统与消息系统之间的耦合。
- **吞吐量优于本地消息表**方案。

缺点是：

- **一次消息发送需要两次网络请求** (half 消息 + commit/rollback 消息）
- **业务处理服务需要实现消息状态回查接口**

## 分布式锁

::: tip 扩展

- [分布式锁实现汇总](https://juejin.im/post/5a20cd8bf265da43163cdd9a)
- [分布式锁实现原理与最佳实践 - 阿里云开发者](https://mp.weixin.qq.com/s/JzCHpIOiFVmBoAko58ZuGw)
- [聊聊分布式锁 - 字节跳动技术团队](https://mp.weixin.qq.com/s/-N4x6EkxwAYDGdJhwvmZLw)
- [Redis、ZooKeeper、Etcd，谁有最好用的分布式锁？ - 腾讯云开发者](https://mp.weixin.qq.com/s/yZC6VJGxt1ANZkn0SljZBg)

:::

### 【简单】什么是分布式锁？为什么需要分布式锁？

在计算机科学中，**锁是在并发场景下用于强行限制资源访问的一种同步机制**，即用于在并发控制中通过互斥手段来保证数据同步安全。

在 Java 进程中，可以使用 Lock、synchronized 等来支持并发锁。如果是同一台机器的不同进程，想要同时操作一个共享资源（例如修改同一个文件），可以使用操作系统提供的「文件锁」或「信号量」来做互斥。这些发生在同一台机器上的互斥操作，可以称为**本地锁**。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/49f1ff0431e142f6b19797762136ed43.png)

本地锁无法协同不同机器间的互斥操作。为了解决这个问题，需要引入分布式锁。

**分布式锁**，顾名思义，应用于分布式场景下，它和单进程中的锁并没有本质上的不同，只是控制对象由一个进程中的多个线程变成了多个进程中的多个线程。此外，临界区的资源也由进程内共享资源变成了分布式系统内部共享资源。

![](https://raw.githubusercontent.com/dunwu/images/master/archive/2024/12/c113dc1f2b904355b3df4864d8f4cb37.png)

### 【困难】实现分布式锁有哪些要点？⭐⭐

分布式锁的解决方案大致有以下几种：

- 基于数据库实现
- 基于缓存（Redis，Memcached 等）实现
- 基于 Zookeeper 实现

分布式锁的实现要点大同小异，仅在实现细节上有所不同。

**分布式锁的实现要点**如下：

- **互斥**：**分布式锁必须是独一无二的**，表现形式为：向数据存储插入一个唯一的 key，一旦有一个线程插入这个 key，其他线程就不能再插入了。
  - 保证 key 唯一性的最简单的方式是使用 UUID。
  - 此外，可以参考 Snowflake ID（雪花算法），将机器地址（IP 地址、机器 ID、MAC 地址）、Jvm 进程 ID（应用 ID、服务 ID）、时间戳等关键信息拼接起来作为唯一标识。
- **避免死锁**：在分布式锁的场景中，部分失败和异步网络这两个问题是同时存在的。如果一个进程获得了锁，但是这个进程与锁服务之间的网络出现了问题，导致无法通信，那么这个情况下，如果锁服务让它一直持有锁，就会导致死锁的发生。
  - 常见的解决思路是引入**超时机制**，即成功申请锁后，超过一定时间，锁失效（删除 key）。超时机制解锁了死锁问题，但又引入了一个新问题：如果应用加锁时，对于操作共享资源的时长估计不足，可能会出现：操作尚未执行完，但是锁没了的尴尬情况。为了解决这个问题，需要引入**锁续期**机制：当持有锁的线程尚未执行完操作前，不断周期性检测锁的超时时间，一旦发现快要过期，就自动为锁续期。
  - ZooKeeper 分布式锁避免死锁采用了另外一种思路—— **Watch 机制**。
- **可重入**：**可重入**指的是：**同一个线程在没有释放锁之前，能否再次获得该锁**。其实现方案是：只需在加锁的时候，**记录好当前获取锁的节点 + 线程组合的唯一标识**，然后在后续的加锁请求时，如果当前请求的节点 + 线程的唯一标识和当前持有锁的相同，那么就直接返回加锁成功；如果不相同，则按正常加锁流程处理。
- **公平性**：当多个线程请求同一锁时，它们必须按照请求的顺序来获取锁，即先来先得的原则。锁的公平性的实现也非常简单，对于被阻塞的加锁请求，我们只要先记录好它们的顺序，在锁被释放后，按顺序颁发就可以了。
- **重试**：有时候，加锁失败可能只是由于网络波动、请求超时等原因，稍候就可以成功获取锁。为了应对这种情况，加锁操作需要支持重试机制。常见的做法是，设置一个加锁超时时间，在该时间范围内，不断自旋重试加锁操作，超时后再判定加锁失败。
- **容错**：分布式锁若存储在单一节点，一旦该节点宕机或失联，就会导致锁失效。将分布式锁存储在多数据库实例中，加锁时并发写入 `N` 个节点，只要 `N / 2 + 1` 个节点写入成功即视为加锁成功。

### 【中等】数据库分布式锁的工作原理是什么？⭐⭐

::: info 数据库分布式锁原理

:::

（1）创建锁表

```sql
CREATE TABLE `distributed_lock` (
	`id` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '主键',
	`resource` VARCHAR(64) NOT NULL DEFAULT '' COMMENT '资源',
	`count` INT(10) UNSIGNED NOT NULL DEFAULT '0' COMMENT '锁次数，统计可重入锁',
	`desc` TEXT DEFAULT NULL COMMENT '备注',
	`create_time` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
	`update_time` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
	PRIMARY KEY (`id`),
	UNIQUE KEY `uniq_resource`(`resource`)
)
	ENGINE = InnoDB DEFAULT CHARSET = `utf8mb4`;
```

（2）获取锁

想要锁住某个方法时，执行以下 SQL：

```sql
insert into distributed_lock(resource,desc) values (‘resourceA’,‘desc’)
```

因为我们对 `resource` 做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。

成功插入则获取锁。

（3）释放锁

当方法执行完毕之后，想要释放锁的话，需要执行以下 Sql:

```sql
delete from methodLock where resource ='resourceA'
```

::: info 数据库分布式锁小结

:::

数据库分布式锁的**问题**：

- **死锁**：一旦释放锁操作失败，或持有锁的机器宕机、断连，就会导致锁记录一直存在，其他线程无法再获得锁。解决办法：为锁增加失效时间字段，启动一个定时任务，隔一段时间清除一次过期的数据。
- **非阻塞**：因为 `insert` 操作一旦失败就会报错，因此未获得锁的线程并不会进入排队队列，要想获得锁就要再次触发加锁操作。解决办法：循环重试，直到插入成功，这么做会产生一定额外开销。
- **非重入**：同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。解决办法：在数据库表中加个字段，记录当前获得锁的节点信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。
- **单点问题**：如果数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。解决办法：单点问题可以用多数据库实例，同时写入 `N` 个节点，`N / 2 + 1` 个成功就加锁成功。

数据库分布式锁的**利弊**：

- **优点**：直接借助数据库，简单易懂。
- **缺点**：会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。此外，数据库性能易成为瓶颈。

### 【困难】ZooKeeper 分布式锁的工作原理是什么？⭐⭐

ZooKeeper 分布式锁的实现基于 ZooKeeper 的两个重要特性：

- **顺序临时节点**：ZooKeeper 的存储类似于 DNS 那样的具有层级的命名空间。ZooKeeper 节点类型可以分为持久节点（`PERSISTENT`）、临时节点（`EPHEMERAL`），每个节点还能被标记为有序性（`SEQUENTIAL`），一旦节点被标记为有序性，那么整个节点就具有顺序自增的特点。
- **Watch 机制**：ZooKeeper 允许用户在指定节点上注册一些 `Watcher`，并且在特定事件触发的时候，ZooKeeper 服务端会将事件通知给用户。

下面是 ZooKeeper 分布式锁的工作流程：

1. 创建一个目录节点，比如叫做 `/locks`；
2. 线程 A 想获取锁，就在 `/locks` 目录下创建临时顺序 zk 节点；
3. 获取 `/locks`目录下所有的子节点，检查是否存在比自己顺序更小的节点：若不存在，则说明当前线程创建的节点顺序最小，获取锁成功；
4. 此时，线程 B 试图获取锁，发现自己的节点顺序不是最小，设置监听锁号在自己前一位的节点；
5. 线程 A 处理完，删除自己的节点。线程 B 监听到变更事件，判断自己是不是最小的节点，如果是则获得锁。

ZooKeeper 分布式锁的**优点**是较为**可靠**：

- **避免死锁**：ZooKeeper 通过**顺序临时节点 + 监听机制**，可以保证：如果持有临时节点的线程主动解锁或断连，Zk 会自动删除临时节点，这意味着锁的释放。所以，不存在锁永久不释放从而导致死锁的问题。
- **单点问题**：ZooKeeper 采用主从架构，并确保主从同步是强一致的，因此不会出现单点问题。

ZooKeeper 分布式锁的**缺点**是：加锁、解锁操作，本质上是对 ZooKeeper 的写操作，全部由 ZooKeeper 主节点负责，然后再同步到从节点。如果加锁、解锁的吞吐量很大，容易出现单点写入瓶颈。

### 【困难】Redis 分布式锁的工作原理是什么？⭐⭐⭐

::: info 极简版本

:::

我们先来看一下，如何实现一个极简版本的 Redis 分布式锁。

（1）加锁

Redis 中的 `setnx` 命令，表示当且仅当 key 不存在时，才会写入 key。由于其互斥性，所以可以基于此来实现分布式锁。

执行 `setnx key val`，若返回 1，表示写入成功，即加锁成功；若返回 0，表示该 key 已存在，写入失败，即加锁失败。

（2）解锁

Redis 分布式锁如何解锁呢？

很简单，删除 key 就意味着释放锁，即执行 `del key` 命令。

::: info 避免死锁

:::

极简版本的解决方案有一个很大的问题：**存在死锁的可能**。持有锁的节点如果执行业务过程中出现异常或机器宕机，都可能导致无法释放锁。这种情况下，其他节点永远也无法再获取锁。

对于异常，在 Java 中，可以通过 `try...catch...finally` 来保证：最终一定会释放锁，其他编程语言也有相似的语法特性。

对于机器宕机这种情况，如何处理呢？通常的对策是：为锁加上**超时机制，过期自动删除**。

在 Redis 中，`expire` 命令可以为 key 设置一个超时时间，一旦过期，Redis 会自动删除 key。如此看来，`setnx` + `expire` 组合使用，就能解决死锁问题了。可惜，没那么简单。Redis 只能保证单一命令的原子性，不保证组合命令的原子性。

那么，Redis 中有没有一条命令可以实现 setnx + expire 的组合语义呢？还真有，可以通过下面的命令来实现：

```bash
# 下面两条命令是等价的
SET key val NX PX 30000
SET key val NX EX 30
```

参数说明：

- `NX`：该参数表示当且仅当 key 不存在，才能写入成功
- `PX`：超时时间，单位毫秒
- `EX`：超时时间，单位秒

::: info 超时续期

:::

为了避免死锁，我们为锁添加了超时时间。但这里有一个问题，如果应用加锁时，对于操作共享资源的时长估计不足，可能会出现：操作尚未执行完，但是锁没了的尴尬情况。为了解决这个问题，很自然会想到，时间不够，就续期呗。

具体来说，如何续期呢？一种方案是：加锁后，启动一个定时任务，周期性检测锁是否快要过期，如果快要过期并且操作尚未结束，就对锁进行自动续期。自行实现这个方案似乎有点繁琐，好在开源 Redis 客户端 [Redisson](https://github.com/redisson/redisson) 中已经为锁的**超时续期**提供了一个成熟的机制——WatchDog（看门狗）。我们可以直接拿来主义即可。

::: info 安全解锁

:::

前文提到了，解锁的操作，实际上就是 `del key`。这里存在一个问题：因为没有任何判断，任何节点都可以随意删除 key，换句话说，锁可能会被其他节点释放。如何避免这个问题呢？解决方法就是：为锁添加**唯一性标识**来进行互斥。唯一性标识可以是 UUID，可以是雪花算法 ID 等。

在 Redis 分布式锁中，唯一性标识的具体实现就是在 `set key val` 时，将唯一性标识 id 作为 `val` 写入。**解锁前，先判断 key 的 value，必须和 set 时写入的 id 值保持一致，以此确认锁归属于自己**。解锁的伪代码如下：

```java
if (redis.get("key") == id)
	redis.del("key");
```

这里依然存在一个问题，由于需要在 Redis 中，先 `get`，后 `del` 操作，所以无法保证操作的原子性。为了保证原子性，可以将这段伪代码用 lua 脚本来实现，这么做的理由是 Redis 中支持原子性的执行 lua 脚本。下面是安全解锁的 lua 脚本代码：

```lua
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

::: info 自旋重试

:::

有时候，加锁失败可能只是由于网络波动、请求超时等原因，稍候就可以成功获取锁。为了应对这种情况，加锁操作需要支持重试机制。常见的做法是，设置一个加锁超时时间，在该时间范围内，不断自旋重试加锁操作，超时后再判定加锁失败。

下面是一个自旋重试获取锁的伪代码示例：

```java
try {
    long begin = System.currentTimeMillis();
    while (true) {
        String result = jedis.set(lockKey, uniqId, "NX", "PX", expireTime);
        if ("OK".equals(result)) {
            // 加锁成功，执行业务操作
            return true;
        }

        long time = System.currentTimeMillis() - begin;
        if (time >= timeout) {
            return false;
        }
        try {
            Thread.sleep(50);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
} catch (Exception e) {
    // 异常处理
} finally {
    // 释放锁
}
```

::: info Redis 分布式锁小结

:::

在前文中，为了实现一个靠谱的 Redis 分布式锁，我们讨论了避免死锁、超时续期、安全解锁几个问题以及应对策略。但是，依然存在一些其他问题：

- **不可重入**：同一个线程无法多次获取同一把锁。
- **单点问题**：Redis 主从同步存在延迟，有可能导致锁冲突。举例来说：线程一在主节点加锁，如果主节点尚未同步给从节点就发生宕机；此时，Redis 集群会选举一个从节点作为新的主节点。此时，新的主节点没有锁的数据，若有其他线程试图加锁，就可以成功获取锁，即出现同时有多个线程持有锁的情况。解决这个问题，可以使用 RedLock 算法。

[Redisson](https://github.com/redisson/redisson) 是一个流行的 Redis Java 客户端，它基于 Netty 开发，并提供了丰富的扩展功能，如：[分布式计数器](https://redisson.org/docs/data-and-services/counters/)、[分布式集合](https://redisson.org/docs/data-and-services/collections/)、[分布式锁](https://redisson.org/docs/data-and-services/locks-and-synchronizers/) 等。

Redisson 支持的分布式锁有多种：Lock, FairLock, MultiLock, RedLock, ReadWriteLock, Semaphore, PermitExpirableSemaphore, CountDownLatch，可以根据场景需要去选择，非常方面。一般而言，使用 Redis 分布式锁，推荐直接使用 Redisson 提供的 API，功能全面且较为可靠。

### 【中等】RedLock 分布式锁的工作原理是什么？⭐⭐⭐

RedLock 分布式锁，是 Redis 的作者 Antirez 提出的一种解决方案。

> 扩展：[RedLock 官方文档](https://redis.io/docs/latest/develop/use/patterns/distributed-locks/)

::: info RedLock 分布式锁原理

:::

RedLock 分布式锁在普通 Redis 分布式锁的简单上，进行了扩展，其要点在于：

- （1）加锁操作不是写入单一节点，而是同时写入多个主节点，官方推荐集群中至少有 5 个主节点。
- （2）只要半数以上的主节点写入成功，即视为加锁成功。
- （3）大多数节点加锁的总耗时，要小于锁设置的过期时间。
- （4）解锁时，要向所有节点发起请求。

下面来逐一解释以上各要点的用意：

（1）RedLock 加锁时，为什么要同时写入多个主节点？

这是为了避免单点问题，即使有部分实例出现异常，依然可以正常提供加锁、解锁能力。

（2）为什么要半数以上的主节点写入成功，才视为加锁成功？

在分布式系统中，为了达成共识，常常采用“多数派”策略来进行决策：大多数节点认可的行为，就视为整体通过。

（3）为什么加锁成功后，还要计算加锁的累计耗时？

因为操作的是多个节点，所以耗时肯定会比操作单个实例耗时更久。而且，网络情况是复杂的，可能存在延迟、丢包、超时等情况。网络请求越多，异常发生的概率就越大。所以，即使大多数节点加锁成功，但如果加锁的累计耗时已经**超过**了锁的过期时间，那此时有些实例上的锁可能已经失效了，这个锁就没有意义了。

（4）解锁时，为什么要向所有节点发起请求？

因为网络环境的复杂性，可能会存在这种情况：向某主节点写入锁信息，实际写入成功，但是响应超时或丢包。

所以，释放锁时，不管之前有没有加锁成功，需要释放**所有节点**的锁，以保证清理节点上**残留**的锁。

::: info RedLock 分布式锁小结

:::

（1）**RedLock 不能完全保证安全性**

分布式系统会遇到三座大山：**NPC**

- N：Network Delay，**网络延迟**；
- P：Process Pause，进程暂停（**GC**）；
- C：Clock Drift，**时钟漂移**。

RedLock 在遇到以上情况时，不能保证安全性。

（2）RedLock 加锁、解锁需要处理多个节点，代价太高

> 总结来说，**已知的分布式锁，无论采用什么解决方案，在极端情况下，都无法保证百分百的安全。**

### 【中等】Redssion 分布式锁的工作原理是什么？⭐⭐⭐

Redssion 分布式锁解决了原生 Redis 锁（`SET NX EX`）的 “锁续期、死锁、单点故障” 等痛点，核心逻辑可总结为 “原子加锁→看门狗续期→安全解锁→高可用扩展”，兼顾易用性和生产级可靠性。

**步骤 1：加锁（原子操作 + 可重入设计）**

客户端调用`lock()`/`lock(time, unit)`时，Redisson 执行以下逻辑：

（1） **生成唯一标识**：客户端 ID（UUID）+ 线程 ID，作为锁的 value（如`a1b2c3:123`）；

（2）原子加锁命令（Redis 2.6+）：

```shell
# NX=不存在才设置，PX=过期时间（毫秒），RETRY_COUNT=重试次数
SET lock_key {客户端 ID}:{线程 ID}:1 NX PX 30000
```

- 可重入逻辑：若锁已存在且 value 匹配当前客户端，执行`HINCRBY lock_key {客户端 ID}:{线程 ID} 1`（Hash 结构），增加重入次数，无需重新加锁；
- 过期时间：默认 30 秒（可配置），避免死锁；若指定了`lock(time, unit)`，则用用户指定的过期时间，**关闭看门狗**。

**步骤 2：加锁失败→阻塞 / 重试（公平 / 非公平策略）**

若加锁失败（锁已被其他客户端持有）：

- **非公平锁**：客户端通过`SUBSCRIBE`订阅锁的释放消息，阻塞等待，直到收到解锁通知后重试加锁；
- **公平锁**：客户端先将自己加入等待队列（Redis List），仅当队列头部是自己时才尝试加锁，保证 “先到先得”。

**步骤 3：看门狗续期（核心痛点解决）**

若加锁时未指定过期时间（调用`lock()`无参方法），Redisson 自动启动**看门狗线程**：

（1）看门狗是一个后台定时线程，每隔`lockWatchdogTimeout/3`（默认 10 秒）执行一次；

（2）续期逻辑：通过 Lua 脚本原子性延长锁的过期时间至 30 秒：

```lua
if redis.call('hexists', KEYS[1], ARGV[1]) == 1 then
  redis.call('pexpire', KEYS[1], ARGV[2])
  return 1
else
  return 0
end
```

（3）停止条件：客户端执行`unlock()`解锁，或客户端宕机（看门狗线程终止，锁到期自动释放）。

**步骤 4：解锁（安全校验 + 原子删除）**

客户端调用`unlock()`时，Redisson 执行以下逻辑（Lua 脚本保证原子性）：

1. **校验锁归属**：判断锁的 value 是否匹配当前客户端 ID + 线程 ID；
2. **处理可重入**：若重入次数 > 1，执行`HINCRBY lock_key {客户端 ID}:{线程 ID} -1`，减少重入次数；
3. **删除锁**：若重入次数 = 1，执行`DEL lock_key`删除锁，并发布解锁通知（`PUBLISH`），唤醒等待的客户端；
4. **异常处理**：若锁已过期 / 归属不匹配，抛出`IllegalMonitorStateException`，避免误删锁。

### 【困难】分布式锁如何进行技术选型？⭐⭐⭐

下面是主流分布式锁技术方案的对比，可以在技术选型时作为参考：

|          | 数据库                                                                                        | Redis                                                                                                   | ZooKeeper                                                                                                            |
| -------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |
| 要点     | 1. 维护一张锁表，为锁的唯一标识字段添加唯一性约束。<br/>2. 只要 insert 成功，即视为加锁成功。 | `set lockKey randomValue NX PX/EX time` 当且仅当 key 不存在时才可以写入，并且设定超时时间，以避免死锁。 | 加锁本质上是在 zk 中指定目录创建**顺序临时接节点**，序号最小即加锁成功。节点删除时，有监听通知机制告知申请锁的线程。 |
| 难度     | 实现简单、易于理解                                                                            | 较为简单，但要使其更可靠，需要有一些完善策略                                                            | 应用简单，但 zk 内部机制并不简单                                                                                     |
| 性能     | 性能最差，易成为瓶颈                                                                          | 性能最高                                                                                                | 性能弱于 Redis                                                                                                       |
| 可靠性   | 有锁表的风险                                                                                  | 较为可靠（需要一些完善策略）                                                                            | 可靠性最高                                                                                                           |
| 适用场景 | 一般不采用                                                                                    | 适用于高并发的场景                                                                                      | 适用于要求可靠，但并发量不高的场景                                                                                   |
| 开源实现 | 无                                                                                            | [Redisson](https://github.com/redisson/redisson)                                                        | [Apache Curator](https://curator.apache.org/docs/about/)                                                             |

## 分布式 ID

::: tip 扩展

- [如果再有人问你分布式 ID，这篇文章丢给他](https://juejin.im/post/5bb0217ef265da0ac2567b42)
- [理解分布式 id 生成算法 SnowFlake](https://segmentfault.com/a/1190000011282426)
- [Leaf——美团点评分布式 ID 生成系统](https://tech.meituan.com/2017/04/21/mt-leaf.html)
- [UUID 规范](https://www.ietf.org/rfc/rfc4122.txt)
- [百度分布式 ID](https://github.com/baidu/uid-generator/blob/master/README.zh_cn.md)
- [ShardingSphere 分布式主键](https://shardingsphere.apache.org/document/current/cn/features/sharding/other-features/key-generator/)

:::

### 【中等】有哪些生成分布式 ID 的方式？⭐⭐

生成分布式 ID 主要有以下方式：

- **UUID**：UUID 是通用唯一识别码（Universally Unique Identifier）的缩写，是一种 128 位的标识符，用 16 进制表示，需要 32 个字符。**UUID 会根据运行应用的计算机网卡 MAC 地址、时间戳、命令空间等元素，通过一定的随机算法产生**。
  - UUID 存在 5 个版本。
  - UUID 不保证全局唯一性，我们需要小心 ID 冲突（尽管这种可能性很小）。
  - **优点**：实现简单、生成速度较快（本地生成，不依赖其他服务）。
  - **缺点**：无序、长度过长、不安全（基于 MAC 地址生成 UUID 的算法，可能会造成 MAC 地址泄露）。
- **数据库自增主键**：大多数数据库都支持自增主键。基于此特性，可以利用事务管理控制生成唯一 ID。
  - **优点**：实现简单、有序、长度较小
  - **缺点**：性能差、存在单点问题、不安全（可以通过 ID 递增规律推算出数据量）
- **数据库号段**：一次批量生成一个 segment（号段），号段的大小由 step（步长）控制。用完之后再去数据库获取新的号段。
- **原子计数器**：一些 NoSQL 数据库提供了原子性的计数器原子计数器 - 利用一些 NoSQL 数据库提供的原子性计数器，来实现分布式 ID。
  - **Redis `incr` / `incrby`**：Redis 的 String 类型提供 `INCR` 和 `INCRBY` 命令将 key 中储存的数字**原子递增**。
    - **优点**：高性能、有序
    - **缺点**：和数据库自增序列方案的缺点类似
  - **ZooKeeper 顺序节点**：利用 ZooKeeper 数据模型中的顺序节点作为分布式 ID。
    - **优点**：简单、可靠性高
    - **缺点**：性能不高
- **Snowflak（雪花算法）**：Snowflake ID 生成过程包含多个组件：时间戳、机器 ID 和序列号。第一位未使用，以确保 ID 正确。此生成器不需要通过网络与 ID 生成器通信，因此速度快且可扩展。Snowflake 的实现各不相同。例如，可以将数据中心 ID 添加到“MachineID”组件中，以保证全局唯一性。

## 分布式会话

### 【简单】Cookie 和 Session 有什么区别？⭐⭐⭐

由于 Http 是一种无状态的协议，服务器单从网络连接上无从知道客户身份。

所以服务器与浏览器为了进行会话跟踪（知道是谁在访问我），就必须主动的去维护一个状态，这个状态用于告知服务端前后两个请求是否来自同一浏览器。而这个状态需要通过 cookie 或者 session 去实现。

**Cookie** 实际上是存储在用户浏览器上的文本信息，并保留了各种跟踪的信息。生成 Cookie 后，用户后续每次请求都会携带 Cookie。

Cookie 通常有大小限制（4KB）。用户可以选择在浏览器中禁用 Cookie。

一个简单的 cookie 设置如下：

```http
Set-Cookie: <cookie-name>=<cookie-value>
```

```http
HTTP/2.0 200 OK
Content-Type: text/html
Set-Cookie: yummy_cookie=choco
Set-Cookie: tasty_cookie=strawberry

[page content]
```

Session 是在服务器端创建和存储的。服务器上通常会生成一个唯一的会话 ID（sessionId），sessionId 附加到特定的用户会话。sessionId 以 Cookie 的形式返回到客户端。Session 可以容纳大量数据。由于 Session 数据不直接由客户端访问，因此 Session 提供了更高的安全性。

Cookie 和 Session 的主要区别可以参考以下表格：

|              | Cookie                                                        | Session                                                                                            |
| ------------ | ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| **作用范围** | 保存在客户端（浏览器）                                        | 保存在服务器端                                                                                     |
| **隐私策略** | 存储在客户端，比较容易遭到非法获取                            | 存储在服务端，安全性相对 Cookie 要好一些                                                           |
| **存储方式** | 只能保存 ASCII                                                | 可以保存任意数据类型。<br/>一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserId 等。 |
| **存储大小** | 不能超过 4K                                                   | 存储大小远高于 Cookie                                                                              |
| **生命周期** | 可设置为永久保存<br/>比如我们经常使用的默认登录（记住我）功能 | 一般失效时间较短<br/>客户端关闭或者 Session 超时都会失效。                                         |

### 【中等】如果禁用了 Cookie 怎么办？⭐⭐

既然服务端是根据 Cookie 中的信息判断用户是否登录，那么如果浏览器中禁止了 Cookie，如何保障整个机制的正常运转。

- 第一种方案，每次请求中都携带一个 SessionID 的参数，也可以 Post 的方式提交，也可以在请求的地址后面拼接 `xxx?SessionID=123456...`。

- 第二种方案，Token 机制。Token 机制多用于 App 客户端和服务器交互的模式，也可以用于 Web 端做用户状态管理。

Token 的意思是“令牌”，是服务端生成的一串字符串，作为客户端进行请求的一个标识。Token 机制和 Cookie 和 Session 的使用机制比较类似。

当用户第一次登录后，服务器根据提交的用户信息生成一个 Token，响应时将 Token 返回给客户端，以后客户端只需带上这个 Token 前来请求数据即可，无需再次登录验证。

### 【中等】分布式 Session 有哪些实现方案？

在分布式场景下，一个用户的 Session 如果只存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器上，该服务器没有用户的 Session，就可能导致用户需要重新进行登录等操作。

分布式 Session 的几种实现策略：

1. 粘性 session
2. 应用服务器间的 session 复制共享
3. 基于缓存的 session 共享 ✔️

> 推荐：基于缓存的 session 共享

::: info 粘性 Session

:::

粘性 Session（Sticky Sessions）**需要配置负载均衡器，使得一个用户的所有请求都路由到一个服务器节点上**，这样就可以把用户的 Session 存放在该服务器节点中。

缺点：**当服务器节点宕机时，将丢失该服务器节点上的所有 Session**。

![](https://raw.githubusercontent.com/dunwu/images/master/cs/design/architecture/MultiNode-StickySessions.jpg)

::: info Session 复制

:::

Session 复制共享（Session Replication）**在服务器节点之间进行 Session 同步操作**，这样的话用户可以访问任何一个服务器节点。

缺点：**占用过多内存**；**同步过程占用网络带宽以及服务器处理器时间**。

![](https://raw.githubusercontent.com/dunwu/images/master/cs/design/architecture/MultiNode-SessionReplication.jpg)

::: info Session 共享

:::

**使用一个单独的存储服务器存储 Session 数据**，可以存在 MySQL 数据库上，也可以存在 Redis 或者 Memcached 这种内存型数据库。

缺点：需要去实现存取 Session 的代码。

![](https://raw.githubusercontent.com/dunwu/images/master/cs/design/architecture/MultiNode-SpringSession.jpg)

## 参考资料

- [**数据密集型应用系统设计**](https://book.douban.com/subject/30329536/) - 这可能是目前最好的分布式存储书籍，强力推荐【进阶】