import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as p,o as l,c as i,a as n,d as e,b as s,e as t}from"./app-418b068d.js";const c={},d=t('<h1 id="hive-面试" tabindex="-1"><a class="header-anchor" href="#hive-面试" aria-hidden="true">#</a> Hive 面试</h1><h2 id="hive-简介" tabindex="-1"><a class="header-anchor" href="#hive-简介" aria-hidden="true">#</a> Hive 简介</h2><h3 id="【基础】什么是-hive" tabindex="-1"><a class="header-anchor" href="#【基础】什么是-hive" aria-hidden="true">#</a> 【基础】什么是 Hive？</h3><details class="hint-container details"><summary>要点</summary><p><strong>Apache Hive 是一种分布式、容错数据仓库，支持大规模分析</strong>。Hive Metastore （HMS） 提供了一个元数据的中央存储库，可以轻松分析以做出明智的数据驱动决策，因此它是许多数据湖架构的关键组件。Hive 构建在 Apache Hadoop 之上，并通过 hdfs 支持在 S3、adls、gs 等上进行存储。Hive 允许用户使用 SQL 读取、写入和管理 PB 级数据。</p><p>Hive 可以将结构化的数据文件映射成表，并提供类 SQL 查询功能。用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。</p><p><strong>特点</strong>：</p><ol><li>简单、容易上手（提供了类似 sql 的查询语言 hql），使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；</li><li>灵活性高，可以自定义用户函数 (UDF) 和存储格式；</li><li>为超大的数据集设计的计算和存储能力，集群扩展容易；</li><li>统一的元数据管理，可与 presto／impala／sparksql 等共享数据；</li><li>执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。</li></ol></details><h3 id="【基础】什么是-hms" tabindex="-1"><a class="header-anchor" href="#【基础】什么是-hms" aria-hidden="true">#</a> 【基础】什么是 HMS？</h3>',5),r={class:"hint-container details"},u=n("summary",null,"要点",-1),k=n("p",null,"Hive Metastore （HMS） 是关系数据库中 Hive 表和分区元数据的中央存储库，它使用元存储服务 API 为客户端（包括 Hive、Impala 和 Spark）提供对此信息的访问。它已成为利用各种开源软件（如 Apache Spark 和 Presto）的数据湖的构建块。事实上，整个工具生态系统，无论是开源的还是其他的，都是围绕 Hive Metastore 构建的，下图说明了其中一些。",-1),v={href:"https://cwiki.apache.org/confluence/display/hive/design",target:"_blank",rel:"noopener noreferrer"},m=n("img",{src:"https://hive.apache.org/images/HMS.jpg",alt:"Apache Software Foundation",tabindex:"0",loading:"lazy"},null,-1),h=n("figcaption",null,"Apache Software Foundation",-1),b=t(`<h2 id="hive-存储" tabindex="-1"><a class="header-anchor" href="#hive-存储" aria-hidden="true">#</a> Hive 存储</h2><h3 id="【基础】hive-支持哪些数据类型" tabindex="-1"><a class="header-anchor" href="#【基础】hive-支持哪些数据类型" aria-hidden="true">#</a> 【基础】Hive 支持哪些数据类型？</h3><details class="hint-container details"><summary>要点</summary><p>Hive 表中的列支持以下基本数据类型：</p><table><thead><tr><th>大类</th><th>类型</th></tr></thead><tbody><tr><td><strong>Integers（整型）</strong></td><td>TINYINT—1 字节的有符号整数 <br>SMALLINT—2 字节的有符号整数<br> INT—4 字节的有符号整数<br> BIGINT—8 字节的有符号整数</td></tr><tr><td><strong>Boolean（布尔型）</strong></td><td>BOOLEAN—TRUE/FALSE</td></tr><tr><td><strong>Floating point numbers（浮点型）</strong></td><td>FLOAT— 单精度浮点型 <br>DOUBLE—双精度浮点型</td></tr><tr><td><strong>Fixed point numbers（定点数）</strong></td><td>DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2)</td></tr><tr><td><strong>String types（字符串）</strong></td><td>STRING—指定字符集的字符序列<br> VARCHAR—具有最大长度限制的字符序列 <br>CHAR—固定长度的字符序列</td></tr><tr><td><strong>Date and time types（日期时间类型）</strong></td><td>TIMESTAMP — 时间戳 <br>TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度<br> DATE—日期类型</td></tr><tr><td><strong>Binary types（二进制类型）</strong></td><td>BINARY—字节序列</td></tr></tbody></table><blockquote><p>TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下：</p><ul><li><strong>TIMESTAMP WITH LOCAL TIME ZONE</strong>：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。</li><li><strong>TIMESTAMP</strong> ：提交什么时间就保存什么时间，查询时也不做任何转换。</li></ul></blockquote><p>此外，Hive 还支持以下复杂类型：</p><table><thead><tr><th>类型</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td><strong>STRUCT</strong></td><td>类似于对象，是字段的集合，字段的类型可以不同，可以使用 <code>名称。字段名</code> 方式进行访问</td><td>STRUCT (&#39;xiaoming&#39;, 12 , &#39;2018-12-12&#39;)</td></tr><tr><td><strong>MAP</strong></td><td>键值对的集合，可以使用 <code>名称 [key]</code> 的方式访问对应的值</td><td>map(&#39;a&#39;, 1, &#39;b&#39;, 2)</td></tr><tr><td><strong>ARRAY</strong></td><td>数组是一组具有相同类型和名称的变量的集合，可以使用 <code>名称 [index]</code> 访问对应的值</td><td>ARRAY(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;)</td></tr></tbody></table></details><h3 id="【基础】hive-支持哪些存储格式" tabindex="-1"><a class="header-anchor" href="#【基础】hive-支持哪些存储格式" aria-hidden="true">#</a> 【基础】Hive 支持哪些存储格式？</h3><details class="hint-container details"><summary>要点</summary><p>Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式：</p><table><thead><tr><th>格式</th><th>说明</th></tr></thead><tbody><tr><td><strong>TextFile</strong></td><td>存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。</td></tr><tr><td><strong>SequenceFile</strong></td><td>SequenceFile 是 Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。</td></tr><tr><td><strong>RCFile</strong></td><td>RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。</td></tr><tr><td><strong>ORC Files</strong></td><td>ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。</td></tr><tr><td><strong>Avro Files</strong></td><td>Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。</td></tr><tr><td><strong>Parquet</strong></td><td>Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。</td></tr></tbody></table><blockquote><p>以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。</p></blockquote><p>通常在创建表的时候使用 <code>STORED AS</code> 参数指定：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> page_view<span class="token punctuation">(</span>viewTime <span class="token keyword">INT</span><span class="token punctuation">,</span> userid <span class="token keyword">BIGINT</span><span class="token punctuation">)</span>
 <span class="token keyword">ROW</span> FORMAT DELIMITED
   <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">&#39;\\001&#39;</span>
   COLLECTION ITEMS <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">&#39;\\002&#39;</span>
   MAP <span class="token keyword">KEYS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">&#39;\\003&#39;</span>
 STORED <span class="token keyword">AS</span> SEQUENCEFILE<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>各个存储文件类型指定方式如下：</p><ul><li>STORED AS TEXTFILE</li><li>STORED AS SEQUENCEFILE</li><li>STORED AS ORC</li><li>STORED AS PARQUET</li><li>STORED AS AVRO</li><li>STORED AS RCFILE</li></ul></details><h3 id="【基础】hive-中的内部表和外部表有什么区别" tabindex="-1"><a class="header-anchor" href="#【基础】hive-中的内部表和外部表有什么区别" aria-hidden="true">#</a> 【基础】Hive 中的内部表和外部表有什么区别？</h3><details class="hint-container details"><summary>要点</summary><p>内部表又叫做管理表 (Managed/Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下：</p><table><thead><tr><th></th><th>内部表</th><th>外部表</th></tr></thead><tbody><tr><td>数据存储位置</td><td>内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 <code>/user/hive/warehouse/数据库名。db/表名/</code> 目录下</td><td>外部表数据的存储位置创建表时由 <code>Location</code> 参数指定；</td></tr><tr><td>导入数据</td><td>在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理</td><td>外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置</td></tr><tr><td>删除表</td><td>删除元数据（metadata）和文件</td><td>只删除元数据（metadata）</td></tr></tbody></table></details><h3 id="【基础】什么是分区表" tabindex="-1"><a class="header-anchor" href="#【基础】什么是分区表" aria-hidden="true">#</a> 【基础】什么是分区表？</h3><details class="hint-container details"><summary>要点</summary><p>Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。</p><p><strong>分区为 HDFS 上表目录的子目录</strong>，数据按照分区存储在子目录中。如果查询的 <code>where</code> 子句中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。</p><p>分区表并非 Hive 独有的概念，实际上这个概念非常常见。通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。比如，在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。</p><p>在 Hive 中可以使用 <code>PARTITIONED BY</code> 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code> <span class="token keyword">CREATE</span> EXTERNAL <span class="token keyword">TABLE</span> emp_partition<span class="token punctuation">(</span>
    empno <span class="token keyword">INT</span><span class="token punctuation">,</span>
    ename STRING<span class="token punctuation">,</span>
    job STRING<span class="token punctuation">,</span>
    mgr <span class="token keyword">INT</span><span class="token punctuation">,</span>
    hiredate <span class="token keyword">TIMESTAMP</span><span class="token punctuation">,</span>
    sal <span class="token keyword">DECIMAL</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    comm <span class="token keyword">DECIMAL</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
    PARTITIONED <span class="token keyword">BY</span> <span class="token punctuation">(</span>deptno <span class="token keyword">INT</span><span class="token punctuation">)</span>   <span class="token comment">-- 按照部门编号进行分区</span>
    <span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">&quot;\\t&quot;</span>
    LOCATION <span class="token string">&#39;/hive/emp_partition&#39;</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>加载数据到分区表时候必须要指定数据所处的分区：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token comment"># 加载部门编号为 20 的数据到表中</span>
<span class="token keyword">LOAD</span> <span class="token keyword">DATA</span> <span class="token keyword">LOCAL</span> INPATH <span class="token string">&quot;/usr/file/emp20.txt&quot;</span> OVERWRITE <span class="token keyword">INTO</span> <span class="token keyword">TABLE</span> emp_partition <span class="token keyword">PARTITION</span> <span class="token punctuation">(</span>deptno<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>
<span class="token comment"># 加载部门编号为 30 的数据到表中</span>
<span class="token keyword">LOAD</span> <span class="token keyword">DATA</span> <span class="token keyword">LOCAL</span> INPATH <span class="token string">&quot;/usr/file/emp30.txt&quot;</span> OVERWRITE <span class="token keyword">INTO</span> <span class="token keyword">TABLE</span> emp_partition <span class="token keyword">PARTITION</span> <span class="token punctuation">(</span>deptno<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 <code>deptno=20</code> 和 <code>deptno=30</code>, 这就是分区目录，分区目录下才是我们加载的数据文件。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># hadoop fs -ls  hdfs://hadoop001:8020/hive/emp_partition/</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>这时候当你的查询语句的 <code>where</code> 包含 <code>deptno=20</code>，则就去对应的分区目录下进行查找，而不用扫描全表。</p></details><h3 id="【基础】什么是分桶表" tabindex="-1"><a class="header-anchor" href="#【基础】什么是分桶表" aria-hidden="true">#</a> 【基础】什么是分桶表？</h3>`,10),T={class:"hint-container details"},g=n("summary",null,"要点",-1),E=n("p",null,"分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。",-1),y=n("p",null,"分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。",-1),f=n("p",null,"单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。",-1),_=n("p",null,"当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图：",-1),I={href:"https://camo.githubusercontent.com/fcfe76ad0ab86ee4cc640c6ac0d180775eddc18caaa381c1235d746fbb956169/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f486173684d61702d486173685461626c652e706e67",target:"_blank",rel:"noopener noreferrer"},w=n("img",{src:"https://camo.githubusercontent.com/fcfe76ad0ab86ee4cc640c6ac0d180775eddc18caaa381c1235d746fbb956169/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f486173684d61702d486173685461626c652e706e67",alt:"img",tabindex:"0",loading:"lazy"},null,-1),A=n("figcaption",null,"img",-1),R={href:"http://www.itcuties.com/java/hashmap-hashtable/",target:"_blank",rel:"noopener noreferrer"},S=t(`<p>在 Hive 中，我们可以通过 <code>CLUSTERED BY</code> 指定分桶列，并通过 <code>SORTED BY</code> 指定桶中数据的排序参考列。下面为分桶表建表语句示例：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code>  <span class="token keyword">CREATE</span> EXTERNAL <span class="token keyword">TABLE</span> emp_bucket<span class="token punctuation">(</span>
    empno <span class="token keyword">INT</span><span class="token punctuation">,</span>
    ename STRING<span class="token punctuation">,</span>
    job STRING<span class="token punctuation">,</span>
    mgr <span class="token keyword">INT</span><span class="token punctuation">,</span>
    hiredate <span class="token keyword">TIMESTAMP</span><span class="token punctuation">,</span>
    sal <span class="token keyword">DECIMAL</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    comm <span class="token keyword">DECIMAL</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    deptno <span class="token keyword">INT</span><span class="token punctuation">)</span>
    <span class="token keyword">CLUSTERED</span> <span class="token keyword">BY</span><span class="token punctuation">(</span>empno<span class="token punctuation">)</span> SORTED <span class="token keyword">BY</span><span class="token punctuation">(</span>empno <span class="token keyword">ASC</span><span class="token punctuation">)</span> <span class="token keyword">INTO</span> <span class="token number">4</span> BUCKETS  <span class="token comment">--按照员工编号散列到四个 bucket 中</span>
    <span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">&quot;\\t&quot;</span>
    LOCATION <span class="token string">&#39;/hive/emp_bucket&#39;</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这里直接使用 <code>Load</code> 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。</p><p>这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下：</p><p>（1）设置强制分桶</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">set</span> hive<span class="token punctuation">.</span>enforce<span class="token punctuation">.</span>bucketing <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment">--Hive 2.x 不需要这一步</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>在 Hive 0.x and 1.x 版本，必须使用设置 <code>hive.enforce.bucketing = true</code>，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。</p><p>（2）CTAS 导入数据</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> <span class="token keyword">TABLE</span> emp_bucket <span class="token keyword">SELECT</span> <span class="token operator">*</span>  <span class="token keyword">FROM</span> emp<span class="token punctuation">;</span>  <span class="token comment">--这里的 emp 表就是一张普通的雇员表</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致：</p>`,10),O={href:"https://camo.githubusercontent.com/668d80c787133f868e172856b92983364719ea86f645b019d0b7cb19f8c0b838/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f686976652d6861646f6f702d6d6170726564756365722e706e67",target:"_blank",rel:"noopener noreferrer"},H=n("img",{src:"https://camo.githubusercontent.com/668d80c787133f868e172856b92983364719ea86f645b019d0b7cb19f8c0b838/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f686976652d6861646f6f702d6d6170726564756365722e706e67",alt:"img",tabindex:"0",loading:"lazy"},null,-1),L=n("figcaption",null,"img",-1),N=n("p",null,"查看分桶文件",-1),D=n("p",null,"bucket（桶） 本质上就是表目录下的具体文件：",-1),M={href:"https://camo.githubusercontent.com/9500f855c80ae87e1294a71f4dd3c4d31602fb5a87009682474e31c293432a43/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f686976652d6861646f6f702d6275636b65742e706e67",target:"_blank",rel:"noopener noreferrer"},q=n("img",{src:"https://camo.githubusercontent.com/9500f855c80ae87e1294a71f4dd3c4d31602fb5a87009682474e31c293432a43/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f686976652d6861646f6f702d6275636b65742e706e67",alt:"img",tabindex:"0",loading:"lazy"},null,-1),x=n("figcaption",null,"img",-1),C=t(`<h3 id="【基础】分区和分桶可以一起使用吗" tabindex="-1"><a class="header-anchor" href="#【基础】分区和分桶可以一起使用吗" aria-hidden="true">#</a> 【基础】分区和分桶可以一起使用吗？</h3><details class="hint-container details"><summary>要点</summary><p>分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> page_view_bucketed<span class="token punctuation">(</span>
	viewTime <span class="token keyword">INT</span><span class="token punctuation">,</span> 
    userid <span class="token keyword">BIGINT</span><span class="token punctuation">,</span>
    page_url STRING<span class="token punctuation">,</span> 
    referrer_url STRING<span class="token punctuation">,</span>
    ip STRING <span class="token punctuation">)</span>
 PARTITIONED <span class="token keyword">BY</span><span class="token punctuation">(</span>dt STRING<span class="token punctuation">)</span>
 <span class="token keyword">CLUSTERED</span> <span class="token keyword">BY</span><span class="token punctuation">(</span>userid<span class="token punctuation">)</span> SORTED <span class="token keyword">BY</span><span class="token punctuation">(</span>viewTime<span class="token punctuation">)</span> <span class="token keyword">INTO</span> <span class="token number">32</span> BUCKETS
 <span class="token keyword">ROW</span> FORMAT DELIMITED
   <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">&#39;\\001&#39;</span>
   COLLECTION ITEMS <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">&#39;\\002&#39;</span>
   MAP <span class="token keyword">KEYS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">&#39;\\003&#39;</span>
 STORED <span class="token keyword">AS</span> SEQUENCEFILE<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>此时导入数据时需要指定分区：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">INSERT</span> OVERWRITE page_view_bucketed
<span class="token keyword">PARTITION</span> <span class="token punctuation">(</span>dt<span class="token operator">=</span><span class="token string">&#39;2009-02-25&#39;</span><span class="token punctuation">)</span>
<span class="token keyword">SELECT</span> <span class="token operator">*</span> <span class="token keyword">FROM</span> page_view <span class="token keyword">WHERE</span> dt<span class="token operator">=</span><span class="token string">&#39;2009-02-25&#39;</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></details><h2 id="hive-索引" tabindex="-1"><a class="header-anchor" href="#hive-索引" aria-hidden="true">#</a> Hive 索引</h2><h3 id="【中级】hive-的索引是如何工作的" tabindex="-1"><a class="header-anchor" href="#【中级】hive-的索引是如何工作的" aria-hidden="true">#</a> 【中级】Hive 的索引是如何工作的？</h3><details class="hint-container details"><summary>要点</summary><p>Hive 在 0.7.0 引入了索引的功能，索引的设计目标是提高表某些列的查询速度。如果没有索引，带有谓词的查询（如&#39;WHERE table1.column = 10&#39;）会加载整个表或分区并处理所有行。但是如果 column 存在索引，则只需要加载和处理文件的一部分。</p><p>在指定列上建立索引，会产生一张索引表（表结构如下），里面的字段包括：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。在查询涉及到索引字段时，首先到索引表查找索引列值对应的 HDFS 文件路径及偏移量，这样就避免了全表扫描。</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token operator">+</span><span class="token comment">--------------+----------------+----------+--+</span>
<span class="token operator">|</span>   col_name   <span class="token operator">|</span>   data_type    <span class="token operator">|</span> <span class="token keyword">comment</span>     <span class="token operator">|</span>
<span class="token operator">+</span><span class="token comment">--------------+----------------+----------+--+</span>
<span class="token operator">|</span> empno        <span class="token operator">|</span> <span class="token keyword">int</span>            <span class="token operator">|</span>  建立索引的列  <span class="token operator">|</span>   
<span class="token operator">|</span> _bucketname  <span class="token operator">|</span> string         <span class="token operator">|</span>  HDFS 文件路径  <span class="token operator">|</span>
<span class="token operator">|</span> _offsets     <span class="token operator">|</span> array<span class="token operator">&lt;</span><span class="token keyword">bigint</span><span class="token operator">&gt;</span>  <span class="token operator">|</span>  偏移量       <span class="token operator">|</span>
<span class="token operator">+</span><span class="token comment">--------------+----------------+----------+--+</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>创建索引：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">CREATE</span> <span class="token keyword">INDEX</span> index_name     <span class="token comment">--索引名称</span>
  <span class="token keyword">ON</span> <span class="token keyword">TABLE</span> base_table_name <span class="token punctuation">(</span>col_name<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>  <span class="token comment">--建立索引的列</span>
  <span class="token keyword">AS</span> index_type    <span class="token comment">--索引类型</span>
  <span class="token punctuation">[</span><span class="token keyword">WITH</span> DEFERRED REBUILD<span class="token punctuation">]</span>    <span class="token comment">--重建索引</span>
  <span class="token punctuation">[</span>IDXPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment">--索引额外属性</span>
  <span class="token punctuation">[</span><span class="token operator">IN</span> <span class="token keyword">TABLE</span> index_table_name<span class="token punctuation">]</span>    <span class="token comment">--索引表的名字</span>
  <span class="token punctuation">[</span>
     <span class="token punctuation">[</span> <span class="token keyword">ROW</span> FORMAT <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> STORED <span class="token keyword">AS</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  
     <span class="token operator">|</span> STORED <span class="token keyword">BY</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
  <span class="token punctuation">]</span>   <span class="token comment">--索引表行分隔符 、 存储格式</span>
  <span class="token punctuation">[</span>LOCATION hdfs_path<span class="token punctuation">]</span>  <span class="token comment">--索引表存储位置</span>
  <span class="token punctuation">[</span>TBLPROPERTIES <span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span>   <span class="token comment">--索引表表属性</span>
  <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> <span class="token string">&quot;index comment&quot;</span><span class="token punctuation">]</span><span class="token punctuation">;</span>  <span class="token comment">--索引注释</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>查看索引：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token comment">--显示表上所有列的索引</span>
<span class="token keyword">SHOW</span> FORMATTED <span class="token keyword">INDEX</span> <span class="token keyword">ON</span> table_name<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>删除索引：</p><p>删除索引会删除对应的索引表。</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">DROP</span> <span class="token keyword">INDEX</span> <span class="token punctuation">[</span><span class="token keyword">IF</span> <span class="token keyword">EXISTS</span><span class="token punctuation">]</span> index_name <span class="token keyword">ON</span> table_name<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>如果存在索引的表被删除了，其对应的索引和索引表都会被删除。如果被索引表的某个分区被删除了，那么分区对应的分区索引也会被删除。</p><p>重建索引：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">ALTER</span> <span class="token keyword">INDEX</span> index_name <span class="token keyword">ON</span> table_name <span class="token punctuation">[</span><span class="token keyword">PARTITION</span> partition_spec<span class="token punctuation">]</span> REBUILD<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>重建索引。如果指定了 <code>PARTITION</code>，则仅重建该分区的索引。</p></details><h3 id="【中级】hive-索引有什么缺陷" tabindex="-1"><a class="header-anchor" href="#【中级】hive-索引有什么缺陷" aria-hidden="true">#</a> 【中级】Hive 索引有什么缺陷？</h3>`,6),B={class:"hint-container details"},F=n("summary",null,"要点",-1),P=n("p",null,"索引表最主要的一个缺陷在于：索引表无法自动 rebuild，这也就意味着如果表中有数据新增或删除，则必须手动 rebuild，重新执行 MapReduce 作业，生成索引表数据。",-1),Y={href:"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Indexing",target:"_blank",rel:"noopener noreferrer"},Q=n("ul",null,[n("li",null,"具有自动重写的物化视图 (Materialized View) 可以产生与索引相似的效果（Hive 2.3.0 增加了对物化视图的支持，在 3.0 之后正式引入）。"),n("li",null,"使用列式存储文件格式（Parquet，ORC）进行存储时，这些格式支持选择性扫描，可以跳过不需要的文件或块。")],-1),U={href:"http://lxw1234.com/archives/2016/04/632.htm",target:"_blank",rel:"noopener noreferrer"},W=n("h2",{id:"hive-架构",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#hive-架构","aria-hidden":"true"},"#"),s(" Hive 架构")],-1),G=n("h3",{id:"【高级】hive-sql-如何执行的",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#【高级】hive-sql-如何执行的","aria-hidden":"true"},"#"),s(" 【高级】Hive SQL 如何执行的？")],-1),V={class:"hint-container details"},X=n("summary",null,"要点",-1),z=n("p",null,"Hive 在执行一条 HQL 的时候，会经过以下步骤：",-1),K=n("ol",null,[n("li",null,"语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree；"),n("li",null,"语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock；"),n("li",null,"生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree；"),n("li",null,"优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量；"),n("li",null,"生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务；"),n("li",null,"优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。")],-1),j={href:"https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html",target:"_blank",rel:"noopener noreferrer"},J=n("h2",{id:"参考资料",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#参考资料","aria-hidden":"true"},"#"),s(" 参考资料")],-1),Z={href:"https://github.com/heibaiying/BigData-Notes/blob/master/notes/Hive",target:"_blank",rel:"noopener noreferrer"};function $(nn,sn){const a=p("ExternalLinkIcon");return l(),i("div",null,[d,n("details",r,[u,k,n("figure",null,[n("a",v,[m,e(a)]),h])]),b,n("details",T,[g,E,y,f,_,n("figure",null,[n("a",I,[w,e(a)]),A]),n("blockquote",null,[n("p",null,[s("图片引用自："),n("a",R,[s("HashMap vs. Hashtable"),e(a)])])]),S,n("figure",null,[n("a",O,[H,e(a)]),L]),N,D,n("figure",null,[n("a",M,[q,e(a)]),x])]),C,n("details",B,[F,P,n("p",null,[s("同时按照 "),n("a",Y,[s("官方文档"),e(a)]),s(" 的说明，Hive 会从 3.0 开始移除索引功能，主要基于以下两个原因：")]),Q,n("blockquote",null,[n("p",null,[s("ORC 内置的索引功能可以参阅这篇文章："),n("a",U,[s("Hive 性能优化之 ORC 索引–Row Group Index vs Bloom Filter Index"),e(a)])])])]),W,G,n("details",V,[X,z,K,n("blockquote",null,[n("p",null,[s("关于 Hive SQL 的详细执行流程可以参考美团技术团队的文章："),n("a",j,[s("Hive SQL 的编译过程"),e(a)])])])]),J,n("ul",null,[n("li",null,[s("[Hive 简介及核心概念]("),n("a",Z,[s("https://github.com/heibaiying/BigData-Notes/blob/master/notes/Hive"),e(a)]),s(" 简介及核心概念。md)")])])])}const tn=o(c,[["render",$],["__file","index.html.vue"]]);export{tn as default};
