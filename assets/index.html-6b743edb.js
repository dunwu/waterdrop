import{_ as i}from"./plugin-vue_export-helper-c27b6911.js";import{r as t,o as l,c as p,a,b as s,d as n,e as o}from"./app-73fea9c6.js";const c={},d=o('<h1 id="sqoop" tabindex="-1"><a class="header-anchor" href="#sqoop" aria-hidden="true">#</a> Sqoop</h1><h2 id="sqoop-简介" tabindex="-1"><a class="header-anchor" href="#sqoop-简介" aria-hidden="true">#</a> Sqoop 简介</h2><p><strong>Sqoop 是一种工具，旨在在 Hadoop 和关系数据库之间进行批量数据迁移的工具。</strong></p><p>Sqoop 是一个常用的数据迁移工具，主要用于在不同存储系统之间实现数据的导入与导出：</p><ul><li>导入数据：从 MySQL，Oracle 等关系型数据库中导入数据到 HDFS、Hive、HBase 等分布式文件存储系统中；</li><li>导出数据：从 分布式文件系统中导出数据到关系数据库中。</li></ul><figure><img src="https://cdn-media-1.freecodecamp.org/images/rEfjXBnXyMjmyvtcIub-cxby3LS31vpFCFyt" alt="Image" tabindex="0" loading="lazy"><figcaption>Image</figcaption></figure><p>目前 Sqoop 主要分为 Sqoop1 和 Sqoop2 两个版本，其中，版本号为 1.4.x 属于 Sqoop1，而版本号为 1.99.x 的属于 Sqoop2。这两个版本开发时的定位方向不同，体系结构具有很大的差异，因此它们之间互不兼容。</p><p>Sqoop1 功能结构简单，部署方便，提供命令行操作方式，主要适用于系统服务管理人员进行简单的数据迁移操作；Sqoop2 功能完善、操作简便，同时支持多种访问模式（命令行操作、Web 访问、Rest API），引入角色安全机制增加安全性等多种优点，但是结构复杂，配置部署更加繁琐。</p><p>Sqoop 社区提供了多种连接器，可以在很多数据存储之间进行数据迁移。</p><ul><li><strong>内置连接器</strong><ul><li>经过优化的专用 RDBMS 连接器：MySQL、PostgreSQL、Oracle、DB2、SQL Server、Netzza 等</li><li>通用的 JDBC 连接器：支持 JDBC 协议的数据库</li></ul></li><li><strong>第三方连接器</strong><ul><li>数据仓库：Teradata</li><li>NoSQL 数据库：Couchbase</li></ul></li></ul><h2 id="sqoop-原理" tabindex="-1"><a class="header-anchor" href="#sqoop-原理" aria-hidden="true">#</a> Sqoop 原理</h2><p><strong>Sqoop 的工作原理是：将执行命令转化成 MapReduce 作业来实现数据的迁移</strong>。</p><h3 id="导入原理" tabindex="-1"><a class="header-anchor" href="#导入原理" aria-hidden="true">#</a> 导入原理</h3><p>在导入数据之前，Sqoop 使用 JDBC 检查导入的数据表，检索出表中的所有列以及列的 SQL 数据类型，并将这些 SQL 类型映射为 Java 数据类型。在转换后的 MapReduce 应用中使用这些对应的 Java 类型来保存字段的值，Sqoop 的代码生成器使用这些信息来创建对应表的类，用于保存从表中抽取的记录。</p><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/bigdata/Sqoop/sqoop-import.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h3 id="导出原理" tabindex="-1"><a class="header-anchor" href="#导出原理" aria-hidden="true">#</a> 导出原理</h3><p>在导出数据之前，Sqoop 会根据数据库连接字符串来选择一个导出方法，对于大部分系统来说，Sqoop 会选择 JDBC。Sqoop 会根据目标表的定义生成一个 Java 类，这个生成的类能够从文本中解析出记录数据，并能够向表中插入类型合适的值，然后启动一个 MapReduce 作业，从 HDFS 中读取源数据文件，使用生成的类解析出记录，并且执行选定的导出方法。</p><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/cs/bigdata/Sqoop/sqoop-export.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h2 id="sqoop-应用" tabindex="-1"><a class="header-anchor" href="#sqoop-应用" aria-hidden="true">#</a> Sqoop 应用</h2>',19),r=a("p",null,"参考手册：",-1),u={href:"https://sqoop.apache.org/docs/1.99.7/admin/Installation.html",target:"_blank",rel:"noopener noreferrer"},m={href:"https://sqoop.apache.org/docs/1.99.7/user/CommandLineClient.html",target:"_blank",rel:"noopener noreferrer"},h={href:"https://sqoop.apache.org/docs/1.99.7/user/CommandLineClient.html",target:"_blank",rel:"noopener noreferrer"},v=o(`<h3 id="sqoop-与-mysql" tabindex="-1"><a class="header-anchor" href="#sqoop-与-mysql" aria-hidden="true">#</a> Sqoop 与 MySQL</h3><h4 id="查询-mysql-所有数据库" tabindex="-1"><a class="header-anchor" href="#查询-mysql-所有数据库" aria-hidden="true">#</a> 查询 MySQL 所有数据库</h4><p>通常用于 Sqoop 与 MySQL 连通测试：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop list-databases <span class="token punctuation">\\</span>
<span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/ <span class="token punctuation">\\</span>
<span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
<span class="token parameter variable">--password</span> root
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,4),b={href:"https://camo.githubusercontent.com/a908521864b3f952eeea127801a3614e0cb29169e6667c19fa930d5bacd0ed88/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f702d6c6973742d6461746162617365732e706e67",target:"_blank",rel:"noopener noreferrer"},f=a("img",{src:"https://camo.githubusercontent.com/a908521864b3f952eeea127801a3614e0cb29169e6667c19fa930d5bacd0ed88/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f702d6c6973742d6461746162617365732e706e67",alt:"img",tabindex:"0",loading:"lazy"},null,-1),k=a("figcaption",null,"img",-1),g=o(`<h4 id="查询指定数据库中所有数据表" tabindex="-1"><a class="header-anchor" href="#查询指定数据库中所有数据表" aria-hidden="true">#</a> 查询指定数据库中所有数据表</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop list-tables <span class="token punctuation">\\</span>
<span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/mysql <span class="token punctuation">\\</span>
<span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
<span class="token parameter variable">--password</span> root
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="sqoop-与-hdfs" tabindex="-1"><a class="header-anchor" href="#sqoop-与-hdfs" aria-hidden="true">#</a> Sqoop 与 HDFS</h3><h4 id="mysql-数据导入到-hdfs" tabindex="-1"><a class="header-anchor" href="#mysql-数据导入到-hdfs" aria-hidden="true">#</a> MySQL 数据导入到 HDFS</h4><h5 id="导入命令" tabindex="-1"><a class="header-anchor" href="#导入命令" aria-hidden="true">#</a> 导入命令</h5><p>示例：导出 MySQL 数据库中的 <code>help_keyword</code> 表到 HDFS 的 <code>/sqoop</code> 目录下，如果导入目录存在则先删除再导入，使用 3 个 <code>map tasks</code> 并行导入。</p><blockquote><p>注：help_keyword 是 MySQL 内置的一张字典表，之后的示例均使用这张表。</p></blockquote><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop <span class="token function">import</span> <span class="token punctuation">\\</span>
<span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/mysql <span class="token punctuation">\\</span>
<span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
<span class="token parameter variable">--password</span> root <span class="token punctuation">\\</span>
<span class="token parameter variable">--table</span> help_keyword <span class="token punctuation">\\</span>           <span class="token comment"># 待导入的表</span>
--delete-target-dir <span class="token punctuation">\\</span>            <span class="token comment"># 目标目录存在则先删除</span>
--target-dir /sqoop <span class="token punctuation">\\</span>            <span class="token comment"># 导入的目标目录</span>
--fields-terminated-by <span class="token string">&#39;\\t&#39;</span>  <span class="token punctuation">\\</span>   <span class="token comment"># 指定导出数据的分隔符</span>
<span class="token parameter variable">-m</span> <span class="token number">3</span>                             <span class="token comment"># 指定并行执行的 map tasks 数量</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>日志输出如下，可以看到输入数据被平均 <code>split</code> 为三份，分别由三个 <code>map task</code> 进行处理。数据默认以表的主键列作为拆分依据，如果你的表没有主键，有以下两种方案：</p><ul><li>添加 <code>-- autoreset-to-one-mapper</code> 参数，代表只启动一个 <code>map task</code>，即不并行执行；</li><li>若仍希望并行执行，则可以使用 <code>--split-by &lt;column-name&gt;</code> 指明拆分数据的参考列。</li></ul>`,10),_={href:"https://camo.githubusercontent.com/c2e23d7fe456107c3351a2da5d2ff651e851e8d36c134339f07297df0a96319a/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f702d6d61702d7461736b2e706e67",target:"_blank",rel:"noopener noreferrer"},q=a("img",{src:"https://camo.githubusercontent.com/c2e23d7fe456107c3351a2da5d2ff651e851e8d36c134339f07297df0a96319a/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f702d6d61702d7461736b2e706e67",alt:"img",tabindex:"0",loading:"lazy"},null,-1),y=a("figcaption",null,"img",-1),S=o(`<h5 id="导入验证" tabindex="-1"><a class="header-anchor" href="#导入验证" aria-hidden="true">#</a> 导入验证</h5><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 查看导入后的目录</span>
hadoop fs <span class="token parameter variable">-ls</span>  <span class="token parameter variable">-R</span> /sqoop
<span class="token comment"># 查看导入内容</span>
hadoop fs <span class="token parameter variable">-text</span>  /sqoop/part-m-00000
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>查看 HDFS 导入目录，可以看到表中数据被分为 3 部分进行存储，这是由指定的并行度决定的。</p>`,3),x={href:"https://camo.githubusercontent.com/9e788568906a827b27cdcd07c1ee14d2410030a74af0233a06fe00b15a2c54d9/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f705f686466735f6c732e706e67",target:"_blank",rel:"noopener noreferrer"},w=a("img",{src:"https://camo.githubusercontent.com/9e788568906a827b27cdcd07c1ee14d2410030a74af0233a06fe00b15a2c54d9/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f705f686466735f6c732e706e67",alt:"img",tabindex:"0",loading:"lazy"},null,-1),H=a("figcaption",null,"img",-1),L=o(`<h4 id="hdfs-数据导出到-mysql" tabindex="-1"><a class="header-anchor" href="#hdfs-数据导出到-mysql" aria-hidden="true">#</a> HDFS 数据导出到 MySQL</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop <span class="token builtin class-name">export</span>  <span class="token punctuation">\\</span>
    <span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/mysql <span class="token punctuation">\\</span>
    <span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
    <span class="token parameter variable">--password</span> root <span class="token punctuation">\\</span>
    <span class="token parameter variable">--table</span> help_keyword_from_hdfs <span class="token punctuation">\\</span>        <span class="token comment"># 导出数据存储在 MySQL 的 help_keyword_from_hdf 的表中</span>
    --export-dir /sqoop  <span class="token punctuation">\\</span>
    --input-fields-terminated-by <span class="token string">&#39;\\t&#39;</span><span class="token punctuation">\\</span>
    <span class="token parameter variable">--m</span> <span class="token number">3</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>表必须预先创建，建表语句如下：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> help_keyword_from_hdfs <span class="token operator">LIKE</span> help_keyword<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="sqoop-与-hive" tabindex="-1"><a class="header-anchor" href="#sqoop-与-hive" aria-hidden="true">#</a> Sqoop 与 Hive</h3><h4 id="mysql-数据导入到-hive" tabindex="-1"><a class="header-anchor" href="#mysql-数据导入到-hive" aria-hidden="true">#</a> MySQL 数据导入到 Hive</h4><p>Sqoop 导入数据到 Hive 是通过先将数据导入到 HDFS 上的临时目录，然后再将数据从 HDFS 上 <code>Load</code> 到 Hive 中，最后将临时目录删除。可以使用 <code>target-dir</code> 来指定临时目录。</p><h5 id="导入命令-1" tabindex="-1"><a class="header-anchor" href="#导入命令-1" aria-hidden="true">#</a> 导入命令</h5><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop <span class="token function">import</span> <span class="token punctuation">\\</span>
  <span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/mysql <span class="token punctuation">\\</span>
  <span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
  <span class="token parameter variable">--password</span> root <span class="token punctuation">\\</span>
  <span class="token parameter variable">--table</span> help_keyword <span class="token punctuation">\\</span>        <span class="token comment"># 待导入的表</span>
  --delete-target-dir <span class="token punctuation">\\</span>         <span class="token comment"># 如果临时目录存在删除</span>
  --target-dir /sqoop_hive  <span class="token punctuation">\\</span>   <span class="token comment"># 临时目录位置</span>
  --hive-database sqoop_test <span class="token punctuation">\\</span>  <span class="token comment"># 导入到 Hive 的 sqoop_test 数据库，数据库需要预先创建。不指定则默认为 default 库</span>
  --hive-import <span class="token punctuation">\\</span>               <span class="token comment"># 导入到 Hive</span>
  --hive-overwrite <span class="token punctuation">\\</span>            <span class="token comment"># 如果 Hive 表中有数据则覆盖，这会清除表中原有的数据，然后再写入</span>
  <span class="token parameter variable">-m</span> <span class="token number">3</span>                          <span class="token comment"># 并行度</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>导入到 Hive 中的 <code>sqoop_test</code> 数据库需要预先创建，不指定则默认使用 Hive 中的 <code>default</code> 库。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code> <span class="token comment"># 查看 hive 中的所有数据库</span>
 hive<span class="token operator">&gt;</span>  SHOW DATABASES<span class="token punctuation">;</span>
 <span class="token comment"># 创建 sqoop_test 数据库</span>
 hive<span class="token operator">&gt;</span>  CREATE DATABASE sqoop_test<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="导入验证-1" tabindex="-1"><a class="header-anchor" href="#导入验证-1" aria-hidden="true">#</a> 导入验证</h5><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 查看 sqoop_test 数据库的所有表</span>
 hive<span class="token operator">&gt;</span>  SHOW  TABLES  IN  sqoop_test<span class="token punctuation">;</span>
<span class="token comment"># 查看表中数据</span>
 hive<span class="token operator">&gt;</span> SELECT * FROM sqoop_test.help_keyword<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,13),E={href:"https://camo.githubusercontent.com/00cf19bbcf1278b0bd28640a5f59862ebdb55e0173e2228fce850cfbcc05e485/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f705f686976655f7461626c65732e706e67",target:"_blank",rel:"noopener noreferrer"},D=a("img",{src:"https://camo.githubusercontent.com/00cf19bbcf1278b0bd28640a5f59862ebdb55e0173e2228fce850cfbcc05e485/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f705f686976655f7461626c65732e706e67",alt:"img",tabindex:"0",loading:"lazy"},null,-1),B=a("figcaption",null,"img",-1),Q=a("h5",{id:"可能出现的问题",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#可能出现的问题","aria-hidden":"true"},"#"),s(" 可能出现的问题")],-1),M={href:"https://camo.githubusercontent.com/27ca396ed586d5d9be1ba106f26dd672401785ac585e1c24adc63069343a6fe6/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f705f686976655f6572726f722e706e67",target:"_blank",rel:"noopener noreferrer"},C=a("img",{src:"https://camo.githubusercontent.com/27ca396ed586d5d9be1ba106f26dd672401785ac585e1c24adc63069343a6fe6/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f705f686976655f6572726f722e706e67",alt:"img",tabindex:"0",loading:"lazy"},null,-1),I=a("figcaption",null,"img",-1),j=o(`<p>如果执行报错 <code>java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</code>，则需将 Hive 安装目录下 <code>lib</code> 下的 <code>hive-exec-**.jar</code> 放到 sqoop 的 <code>lib</code> 。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token punctuation">[</span>root@hadoop001 lib<span class="token punctuation">]</span><span class="token comment"># ll hive-exec-*</span>
-rw-r--r--. <span class="token number">1</span> <span class="token number">1106</span> <span class="token number">4001</span> <span class="token number">19632031</span> <span class="token number">11</span> 月 <span class="token number">13</span> <span class="token number">21</span>:45 hive-exec-1.1.0-cdh5.15.2.jar
<span class="token punctuation">[</span>root@hadoop001 lib<span class="token punctuation">]</span><span class="token comment"># cp hive-exec-1.1.0-cdh5.15.2.jar  \${SQOOP_HOME}/lib</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="hive-导出数据到-mysql" tabindex="-1"><a class="header-anchor" href="#hive-导出数据到-mysql" aria-hidden="true">#</a> Hive 导出数据到 MySQL</h4><p>由于 Hive 的数据是存储在 HDFS 上的，所以 Hive 导入数据到 MySQL，实际上就是 HDFS 导入数据到 MySQL。</p><h5 id="查看-hive-表在-hdfs-的存储位置" tabindex="-1"><a class="header-anchor" href="#查看-hive-表在-hdfs-的存储位置" aria-hidden="true">#</a> 查看 Hive 表在 HDFS 的存储位置</h5><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 进入对应的数据库</span>
hive<span class="token operator">&gt;</span> use sqoop_test<span class="token punctuation">;</span>
<span class="token comment"># 查看表信息</span>
hive<span class="token operator">&gt;</span> desc formatted help_keyword<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>Location</code> 属性为其存储位置：</p><figure><img src="https://camo.githubusercontent.com/ed9d22fb7fcdc3f71d067010820d1c7de41a243bf3cf8be572020923ecdf0802/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f702d686976652d6c6f636174696f6e2e706e67" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>这里可以查看一下这个目录，文件结构如下：</p><figure><img src="https://camo.githubusercontent.com/27f1234dafcee45782a5f711e85a3161592a1ca787d587b4a7e14089b4bf7e72/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f702d686976652d686466732e706e67" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h5 id="执行导出命令" tabindex="-1"><a class="header-anchor" href="#执行导出命令" aria-hidden="true">#</a> 执行导出命令</h5><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop <span class="token builtin class-name">export</span>  <span class="token punctuation">\\</span>
    <span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/mysql <span class="token punctuation">\\</span>
    <span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
    <span class="token parameter variable">--password</span> root <span class="token punctuation">\\</span>
    <span class="token parameter variable">--table</span> help_keyword_from_hive <span class="token punctuation">\\</span>
    --export-dir /user/hive/warehouse/sqoop_test.db/help_keyword  <span class="token punctuation">\\</span>
    -input-fields-terminated-by <span class="token string">&#39;\\001&#39;</span> <span class="token punctuation">\\</span>             <span class="token comment"># 需要注意的是 hive 中默认的分隔符为 \\001</span>
    <span class="token parameter variable">--m</span> <span class="token number">3</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>MySQL 中的表需要预先创建：</p><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> help_keyword_from_hive <span class="token operator">LIKE</span> help_keyword<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="sqoop-与-hbase" tabindex="-1"><a class="header-anchor" href="#sqoop-与-hbase" aria-hidden="true">#</a> Sqoop 与 HBase</h3><blockquote><p>本小节只讲解从 RDBMS 导入数据到 HBase，因为暂时没有命令能够从 HBase 直接导出数据到 RDBMS。</p></blockquote><h4 id="mysql-导入数据到-hbase" tabindex="-1"><a class="header-anchor" href="#mysql-导入数据到-hbase" aria-hidden="true">#</a> MySQL 导入数据到 HBase</h4><h5 id="导入数据" tabindex="-1"><a class="header-anchor" href="#导入数据" aria-hidden="true">#</a> 导入数据</h5><p>将 <code>help_keyword</code> 表中数据导入到 HBase 上的 <code>help_keyword_hbase</code> 表中，使用原表的主键 <code>help_keyword_id</code> 作为 <code>RowKey</code>，原表的所有列都会在 <code>keywordInfo</code> 列族下，目前只支持全部导入到一个列族下，不支持分别指定列族。</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop <span class="token function">import</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/mysql <span class="token punctuation">\\</span>
    <span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
    <span class="token parameter variable">--password</span> root <span class="token punctuation">\\</span>
    <span class="token parameter variable">--table</span> help_keyword <span class="token punctuation">\\</span>              <span class="token comment"># 待导入的表</span>
    --hbase-table help_keyword_hbase <span class="token punctuation">\\</span>  <span class="token comment"># hbase 表名称，表需要预先创建</span>
    --column-family keywordInfo <span class="token punctuation">\\</span>       <span class="token comment"># 所有列导入到 keywordInfo 列族下</span>
    --hbase-row-key help_keyword_id     <span class="token comment"># 使用原表的 help_keyword_id 作为 RowKey</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>导入的 HBase 表需要预先创建：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 查看所有表</span>
hbase<span class="token operator">&gt;</span> list
<span class="token comment"># 创建表</span>
hbase<span class="token operator">&gt;</span> create <span class="token string">&#39;help_keyword_hbase&#39;</span>, <span class="token string">&#39;keywordInfo&#39;</span>
<span class="token comment"># 查看表信息</span>
hbase<span class="token operator">&gt;</span> desc <span class="token string">&#39;help_keyword_hbase&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="导入验证-2" tabindex="-1"><a class="header-anchor" href="#导入验证-2" aria-hidden="true">#</a> 导入验证</h5><p>使用 <code>scan</code> 查看表数据：</p>`,24),F={href:"https://camo.githubusercontent.com/7fdd034cab4722d9e194d16a91ce0c23db4b9589d0c4d7c241b56034686b490a/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f705f68626173652e706e67",target:"_blank",rel:"noopener noreferrer"},A=a("img",{src:"https://camo.githubusercontent.com/7fdd034cab4722d9e194d16a91ce0c23db4b9589d0c4d7c241b56034686b490a/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f73716f6f705f68626173652e706e67",alt:"img",tabindex:"0",loading:"lazy"},null,-1),R=a("figcaption",null,"img",-1),z=o(`<h3 id="全库导出" tabindex="-1"><a class="header-anchor" href="#全库导出" aria-hidden="true">#</a> 全库导出</h3><p>Sqoop 支持通过 <code>import-all-tables</code> 命令进行全库导出到 HDFS/Hive，但需要注意有以下两个限制：</p><ul><li>所有表必须有主键；或者使用 <code>--autoreset-to-one-mapper</code>，代表只启动一个 <code>map task</code>;</li><li>你不能使用非默认的分割列，也不能通过 WHERE 子句添加任何限制。</li></ul><blockquote><p>第二点解释得比较拗口，这里列出官方原本的说明：</p><ul><li>You must not intend to use non-default splitting column, nor impose any conditions via a <code>WHERE</code> clause.</li></ul></blockquote><p>全库导出到 HDFS：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop import-all-tables <span class="token punctuation">\\</span>
    <span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/数据库名 <span class="token punctuation">\\</span>
    <span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
    <span class="token parameter variable">--password</span> root <span class="token punctuation">\\</span>
    --warehouse-dir  /sqoop_all <span class="token punctuation">\\</span>     <span class="token comment"># 每个表会单独导出到一个目录，需要用此参数指明所有目录的父目录</span>
    --fields-terminated-by <span class="token string">&#39;\\t&#39;</span>  <span class="token punctuation">\\</span>
    <span class="token parameter variable">-m</span> <span class="token number">3</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>全库导出到 Hive：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop import-all-tables <span class="token parameter variable">-Dorg.apache.sqoop.splitter.allow_text_splitter</span><span class="token operator">=</span>true <span class="token punctuation">\\</span>
  <span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/数据库名 <span class="token punctuation">\\</span>
  <span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
  <span class="token parameter variable">--password</span> root <span class="token punctuation">\\</span>
  --hive-database sqoop_test <span class="token punctuation">\\</span>         <span class="token comment"># 导出到 Hive 对应的库</span>
  --hive-import <span class="token punctuation">\\</span>
  --hive-overwrite <span class="token punctuation">\\</span>
  <span class="token parameter variable">-m</span> <span class="token number">3</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="sqoop-数据过滤" tabindex="-1"><a class="header-anchor" href="#sqoop-数据过滤" aria-hidden="true">#</a> Sqoop 数据过滤</h3><h4 id="query-参数" tabindex="-1"><a class="header-anchor" href="#query-参数" aria-hidden="true">#</a> query 参数</h4><p>Sqoop 支持使用 <code>query</code> 参数定义查询 SQL，从而可以导出任何想要的结果集。使用示例如下：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop <span class="token function">import</span> <span class="token punctuation">\\</span>
  <span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/mysql <span class="token punctuation">\\</span>
  <span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
  <span class="token parameter variable">--password</span> root <span class="token punctuation">\\</span>
  <span class="token parameter variable">--query</span> <span class="token string">&#39;select * from help_keyword where  $CONDITIONS and  help_keyword_id &lt; 50&#39;</span> <span class="token punctuation">\\</span>
  --delete-target-dir <span class="token punctuation">\\</span>
  --target-dir /sqoop_hive  <span class="token punctuation">\\</span>
  --hive-database sqoop_test <span class="token punctuation">\\</span>           <span class="token comment"># 指定导入目标数据库 不指定则默认使用 Hive 中的 default 库</span>
  --hive-table filter_help_keyword <span class="token punctuation">\\</span>     <span class="token comment"># 指定导入目标表</span>
  --split-by help_keyword_id <span class="token punctuation">\\</span>           <span class="token comment"># 指定用于 split 的列</span>
  --hive-import <span class="token punctuation">\\</span>                        <span class="token comment"># 导入到 Hive</span>
  --hive-overwrite <span class="token punctuation">\\</span>                     、
  <span class="token parameter variable">-m</span> <span class="token number">3</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在使用 <code>query</code> 进行数据过滤时，需要注意以下三点：</p><ul><li>必须用 <code>--hive-table</code> 指明目标表；</li><li>如果并行度 <code>-m</code> 不为 1 或者没有指定 <code>--autoreset-to-one-mapper</code>，则需要用 <code>--split-by</code> 指明参考列；</li><li>SQL 的 <code>where</code> 字句必须包含 <code>$CONDITIONS</code>，这是固定写法，作用是动态替换。</li></ul><h4 id="增量导入" tabindex="-1"><a class="header-anchor" href="#增量导入" aria-hidden="true">#</a> 增量导入</h4><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sqoop <span class="token function">import</span> <span class="token punctuation">\\</span>
    <span class="token parameter variable">--connect</span> jdbc:mysql://hadoop001:3306/mysql <span class="token punctuation">\\</span>
    <span class="token parameter variable">--username</span> root <span class="token punctuation">\\</span>
    <span class="token parameter variable">--password</span> root <span class="token punctuation">\\</span>
    <span class="token parameter variable">--table</span> help_keyword <span class="token punctuation">\\</span>
    --target-dir /sqoop_hive  <span class="token punctuation">\\</span>
    --hive-database sqoop_test <span class="token punctuation">\\</span>
    <span class="token parameter variable">--incremental</span>  append  <span class="token punctuation">\\</span>             <span class="token comment"># 指明模式</span>
    --check-column  help_keyword_id <span class="token punctuation">\\</span>    <span class="token comment"># 指明用于增量导入的参考列</span>
    --last-value <span class="token number">300</span>  <span class="token punctuation">\\</span>                  <span class="token comment"># 指定参考列上次导入的最大值</span>
    --hive-import <span class="token punctuation">\\</span>
    <span class="token parameter variable">-m</span> <span class="token number">3</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>incremental</code> 参数有以下两个可选的选项：</p><ul><li><strong>append</strong>：要求参考列的值必须是递增的，所有大于 <code>last-value</code> 的值都会被导入；</li><li><strong>lastmodified</strong>：要求参考列的值必须是 <code>timestamp</code> 类型，且插入数据时候要在参考列插入当前时间戳，更新数据时也要更新参考列的时间戳，所有时间晚于 <code>last-value</code> 的数据都会被导入。</li></ul><p>通过上面的解释我们可以看出来，其实 Sqoop 的增量导入并没有太多神器的地方，就是依靠维护的参考列来判断哪些是增量数据。当然我们也可以使用上面介绍的 <code>query</code> 参数来进行手动的增量导出，这样反而更加灵活。</p><h4 id="类型支持" tabindex="-1"><a class="header-anchor" href="#类型支持" aria-hidden="true">#</a> 类型支持</h4><p>Sqoop 默认支持数据库的大多数字段类型，但是某些特殊类型是不支持的。遇到不支持的类型，程序会抛出异常 <code>Hive does not support the SQL type for column xxx</code> 异常，此时可以通过下面两个参数进行强制类型转换：</p><ul><li><code>--map-column-java&lt;mapping&gt;</code> - 重写 SQL 到 Java 类型的映射；</li><li><code>--map-column-hive &lt;mapping&gt;</code> - 重写 Hive 到 Java 类型的映射。</li></ul><p>示例如下，将原先 <code>id</code> 字段强制转为 String 类型，<code>value</code> 字段强制转为 Integer 类型：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>$ sqoop <span class="token function">import</span> <span class="token punctuation">..</span>. --map-column-java <span class="token assign-left variable">id</span><span class="token operator">=</span>String,value<span class="token operator">=</span>Integer
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料" aria-hidden="true">#</a> 参考资料</h2>`,25),N={href:"https://sqoop.apache.org/",target:"_blank",rel:"noopener noreferrer"},O={href:"https://sqoop.apache.org/",target:"_blank",rel:"noopener noreferrer"},T={href:"https://github.com/heibaiying/BigData-Notes/blob/master/notes/Sqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.md",target:"_blank",rel:"noopener noreferrer"};function J(W,K){const e=t("ExternalLinkIcon");return l(),p("div",null,[d,a("blockquote",null,[r,a("p",null,[a("a",u,[s("Sqoop 官方文档之安装说明"),n(e)])]),a("p",null,[a("a",m,[s("Sqoop 官方文档之 Shell 命令"),n(e)])]),a("p",null,[a("a",h,[s("Sqoop 官方文档之连接器"),n(e)])])]),v,a("figure",null,[a("a",b,[f,n(e)]),k]),g,a("figure",null,[a("a",_,[q,n(e)]),y]),S,a("figure",null,[a("a",x,[w,n(e)]),H]),L,a("figure",null,[a("a",E,[D,n(e)]),B]),Q,a("figure",null,[a("a",M,[C,n(e)]),I]),j,a("figure",null,[a("a",F,[A,n(e)]),R]),z,a("ul",null,[a("li",null,[a("a",N,[s("Sqoop Github"),n(e)])]),a("li",null,[a("a",O,[s("Sqoop 官网"),n(e)])]),a("li",null,[a("a",T,[s("Sqoop 基本使用"),n(e)])])])])}const P=i(c,[["render",J],["__file","index.html.vue"]]);export{P as default};
