import{_ as n}from"./plugin-vue_export-helper-c27b6911.js";import{r,o as t,c as s,a as o,b as l,d as e,e as g}from"./app-6ed3f423.js";const d={},c=o("h1",{id:"《mysql-实战-45-讲》笔记二",tabindex:"-1"},[o("a",{class:"header-anchor",href:"#《mysql-实战-45-讲》笔记二","aria-hidden":"true"},"#"),l(" 《MySQL 实战 45 讲》笔记二")],-1),a={href:"https://time.geekbang.org/column/intro/139",target:"_blank",rel:"noopener noreferrer"},u=g(`<h2 id="_16-order-by-是怎么工作的" tabindex="-1"><a class="header-anchor" href="#_16-order-by-是怎么工作的" aria-hidden="true">#</a> 16 <code>order by</code> 是怎么工作的？</h2><p>用 explain 命令查看执行计划时，Extra 这个字段中的“Using filesort”表示的就是需要排序。</p><h3 id="全字段排序" tabindex="-1"><a class="header-anchor" href="#全字段排序" aria-hidden="true">#</a> 全字段排序</h3><div class="language-sql line-numbers-mode" data-ext="sql"><pre class="language-sql"><code><span class="token keyword">select</span> city<span class="token punctuation">,</span>name<span class="token punctuation">,</span>age <span class="token keyword">from</span> t <span class="token keyword">where</span> city<span class="token operator">=</span><span class="token string">&#39;杭州&#39;</span> <span class="token keyword">order</span> <span class="token keyword">by</span> name <span class="token keyword">limit</span> <span class="token number">1000</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>这个语句执行流程如下所示 ：</p><p><strong>执行流程</strong>：</p><ul><li>初始化 <code>sort_buffer</code>，确定放入需要排序的字段（如 <code>name</code>、<code>city</code>、<code>age</code>）。</li><li>从索引中找到满足条件的记录，取出对应的字段值存入 <code>sort_buffer</code>。</li><li>对 <code>sort_buffer</code> 中的数据按照排序字段进行排序。</li><li>返回排序后的结果。</li></ul><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20220728090300.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>内存与磁盘排序</strong>：</p><ul><li>如果排序数据量小于 <code>sort_buffer_size</code>，排序在内存中完成。</li><li>如果数据量过大，MySQL 会使用临时文件进行外部排序（归并排序）。<strong>MySQL 将需要排序的数据分成 N 份，每一份单独排序后存在这些临时文件中。然后把这 N 个有序文件再合并成一个有序的大文件。</strong></li></ul><p><strong>优化器追踪</strong>：通过 <code>OPTIMIZER_TRACE</code> 可以查看排序过程中是否使用了临时文件（<code>number_of_tmp_files</code>）。</p><h3 id="rowid-排序" tabindex="-1"><a class="header-anchor" href="#rowid-排序" aria-hidden="true">#</a> rowid 排序</h3><ul><li><strong>执行流程</strong>： <ul><li>当单行数据过大时，MySQL 会采用 <code>rowid</code> 排序，只将排序字段（如 <code>name</code>）和主键 <code>id</code> 放入 <code>sort_buffer</code>。</li><li>排序完成后，根据 <code>id</code> 回表查询其他字段（如 <code>city</code>、<code>age</code>）。</li></ul></li><li><strong>性能影响</strong>：<code>rowid</code> 排序减少了 <code>sort_buffer</code> 的内存占用，但增加了回表操作，导致更多的磁盘 I/O。</li></ul><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20220728090919.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="全字段排序-vs-rowid-排序" tabindex="-1"><a class="header-anchor" href="#全字段排序-vs-rowid-排序" aria-hidden="true">#</a> 全字段排序 VS rowid 排序</h3><ul><li><strong>内存优先</strong>： <ul><li>如果内存足够大，MySQL 优先使用全字段排序，以减少磁盘访问。</li><li>只有在内存不足时，才会使用 <code>rowid</code> 排序。</li></ul></li><li><strong>设计思想</strong>：<strong>如果内存够，就要多利用内存，尽量减少磁盘访问。</strong></li></ul><p>并不是所有的 order by 语句，都需要排序操作的。MySQL 之所以需要生成临时表，并且在临时表上做排序操作，<strong>其原因是原来的数据都是无序的</strong>。如果查询的字段和排序字段可以通过联合索引覆盖，MySQL 可以直接利用索引的有序性，避免排序操作。</p><h2 id="_17-如何正确地显示随机消息" tabindex="-1"><a class="header-anchor" href="#_17-如何正确地显示随机消息" aria-hidden="true">#</a> 17 如何正确地显示随机消息？</h2><p><strong><code>ORDER BY RAND()</code> 的执行流程</strong></p><ul><li>使用 <code>ORDER BY RAND()</code> 时，MySQL 会创建一个临时表，并为每一行生成一个随机数，然后对临时表进行排序。</li><li>排序过程可能使用内存临时表或磁盘临时表，具体取决于数据量和 <code>tmp_table_size</code> 的设置。</li></ul><p><strong><code>ORDER BY RAND()</code> 的性能问题</strong>：<code>ORDER BY RAND()</code> 需要扫描全表并生成随机数，排序过程消耗大量资源，尤其是在数据量大时，性能较差。</p><p><strong>内存临时表与磁盘临时表</strong></p><p><strong>内存临时表</strong>：当临时表大小小于 <code>tmp_table_size</code> 时，MySQL 使用内存临时表，排序过程使用 <code>rowid</code> 排序算法。</p><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202503200808843.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>磁盘临时表</strong>：当临时表大小超过 <code>tmp_table_size</code> 时，MySQL 会使用磁盘临时表，排序过程使用归并排序算法。</p><p><strong>优先队列排序</strong>：当只需要返回少量数据（如 <code>LIMIT 3</code>）时，MySQL 5.6 引入了优先队列排序算法，避免对整个数据集进行排序，减少计算量。</p><p><strong>随机排序的优化方法</strong></p><ul><li><p><strong>随机算法 1</strong>：通过 <code>max(id)</code> 和 <code>min(id)</code> 生成随机数，然后使用 <code>LIMIT</code> 获取随机行。问题是：ID 不连续时，某些行的概率不均匀。</p></li><li><p><strong>随机算法 2</strong>：先获取表的总行数 <code>C</code>，然后生成随机数 <code>Y</code>，使用 <code>LIMIT Y, 1</code> 获取随机行。<strong>优点</strong>：解决了概率不均匀的问题，但需要扫描 <code>C + Y + 1</code> 行。</p></li><li><p><strong>随机算法 3</strong>：扩展随机算法 2，生成多个随机数 <code>Y1, Y2, Y3</code>，分别使用 <code>LIMIT Y, 1</code> 获取多行随机数据。<strong>优点</strong>：适用于需要返回多行随机数据的场景。</p><p><strong>总结</strong></p></li><li><p><strong>避免使用 <code>ORDER BY RAND()</code></strong>：<code>ORDER BY RAND()</code> 的性能较差，尤其是在数据量大时，应尽量避免使用。</p></li><li><p><strong>应用层处理随机逻辑</strong>：将随机逻辑放在应用层处理，数据库只负责数据读取，减少数据库的计算压力。</p></li><li><p><strong>优化扫描行数</strong>：通过合理的随机算法，减少扫描行数，提升查询性能。</p></li></ul><h2 id="_18-为什么这些-sql-语句逻辑相同-性能却差异巨大" tabindex="-1"><a class="header-anchor" href="#_18-为什么这些-sql-语句逻辑相同-性能却差异巨大" aria-hidden="true">#</a> 18 为什么这些 SQL 语句逻辑相同，性能却差异巨大？</h2><p><strong>对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。</strong></p><p><strong>案例一：条件字段函数操作</strong></p><ul><li><strong>问题</strong>：在 <code>WHERE</code> 条件中对索引字段使用函数（如 <code>month(t_modified)</code>），会导致 MySQL 无法使用索引的快速定位功能，转而进行全索引扫描。</li><li><strong>原因</strong>：对索引字段进行函数操作会破坏索引值的有序性，优化器会放弃树搜索功能，转而进行全索引扫描。</li><li><strong>解决方案</strong>：避免在索引字段上使用函数操作，改为基于字段本身的范围查询。例如，将 <code>month(t_modified)=7</code> 改为 <code>t_modified</code> 的范围查询。</li></ul><p><strong>案例二：隐式类型转换</strong></p><ul><li><strong>问题</strong>：当查询条件中的字段类型与索引字段类型不一致时（如 <code>varchar</code> 和 <code>int</code>），MySQL 会进行隐式类型转换，导致无法使用索引。</li><li><strong>原因</strong>：隐式类型转换相当于对索引字段进行了函数操作（如 <code>CAST</code>），优化器会放弃树搜索功能，转而进行全表扫描。</li><li><strong>解决方案</strong>：确保查询条件中的字段类型与索引字段类型一致，避免隐式类型转换。</li></ul><p><strong>案例三：隐式字符编码转换</strong></p><ul><li><strong>问题</strong>：当两个表的字符集不同时（如 <code>utf8</code> 和 <code>utf8mb4</code>），在进行表连接查询时，MySQL 会对被驱动表的索引字段进行字符集转换，导致无法使用索引。</li><li><strong>原因</strong>：字符集转换相当于对索引字段进行了函数操作（如 <code>CONVERT</code>），优化器会放弃树搜索功能，转而进行全表扫描。</li><li><strong>解决方案</strong>： <ul><li><strong>统一字符集</strong>：将两个表的字符集统一为 <code>utf8mb4</code>，避免字符集转换。</li><li><strong>手动转换</strong>：在 SQL 语句中手动进行字符集转换，确保转换操作发生在驱动表上，而不是被驱动表的索引字段上。</li></ul></li></ul><h2 id="_19-为什么我只查一行的语句-也执行这么慢" tabindex="-1"><a class="header-anchor" href="#_19-为什么我只查一行的语句-也执行这么慢" aria-hidden="true">#</a> 19 为什么我只查一行的语句，也执行这么慢？</h2><p><strong>查询长时间不返回的可能原因</strong></p><ul><li><strong>等 MDL 锁</strong>：当查询需要获取表的 MDL 读锁，而其他线程持有 MDL 写锁时，查询会被阻塞。 <ul><li><strong>解决方案</strong>：通过 <code>sys.schema_table_lock_waits</code> 表找到持有 MDL 写锁的线程，并 <code>KILL</code> 掉该线程。</li></ul></li><li><strong>等 flush</strong>：当有线程正在对表执行 <code>flush tables</code> 操作时，其他查询会被阻塞。 <ul><li><strong>解决方案</strong>：找到阻塞 <code>flush</code> 操作的线程并 <code>KILL</code> 掉。</li></ul></li><li><strong>等行锁</strong>：当查询需要获取某行的读锁，而其他事务持有该行的写锁时，查询会被阻塞。 <ul><li><strong>解决方案</strong>：通过 <code>sys.innodb_lock_waits</code> 表找到持有写锁的线程，并 <code>KILL</code> 掉该连接。</li></ul></li></ul><p><strong>查询慢的可能原因</strong></p><ul><li><strong>全表扫描</strong>：当查询条件中的字段没有索引时，MySQL 会进行全表扫描，导致查询缓慢。 <ul><li><strong>解决方案</strong>：为查询条件中的字段添加索引。</li></ul></li><li><strong>一致性读与当前读</strong>： <ul><li><strong>一致性读</strong>：当查询使用一致性读时，如果该行有大量 undo log（如被频繁更新），MySQL 需要依次执行这些 undo log 才能返回结果，导致查询缓慢。</li><li><strong>当前读</strong>：使用 <code>lock in share mode</code> 或 <code>for update</code> 进行当前读时，MySQL 会直接读取最新的数据，因此速度较快。</li><li><strong>解决方案</strong>：理解一致性读和当前读的区别，根据业务需求选择合适的查询方式。</li></ul></li></ul><h2 id="_20-幻读是什么-幻读有什么问题" tabindex="-1"><a class="header-anchor" href="#_20-幻读是什么-幻读有什么问题" aria-hidden="true">#</a> 20 幻读是什么，幻读有什么问题？</h2><p><strong>幻读的定义</strong></p><ul><li>幻读指的是一个事务在前后两次查询同一个范围时，后一次查询看到了前一次查询没有看到的行。</li><li>幻读仅在“当前读”（如<code>select ... for update</code>）时出现，普通的快照读不会出现幻读。</li></ul><p><strong>幻读的问题</strong></p><ul><li><strong>语义问题</strong>：事务 A 声明要锁住所有满足条件的行，但由于幻读的存在，其他事务可以插入或修改这些行，破坏了事务 A 的加锁声明。</li><li><strong>数据一致性问题</strong>：幻读可能导致数据和日志在逻辑上不一致，尤其是在使用 binlog 进行数据同步或恢复时，可能会导致数据不一致。</li></ul><p><strong>幻读的解决方案</strong></p><p>产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。</p><ul><li><strong>间隙锁（Gap Lock）</strong>：为了解决幻读问题，InnoDB 引入了间隙锁。间隙锁锁住的是索引记录之间的间隙，防止新记录的插入。</li><li><strong>Next-Key Lock</strong>：间隙锁和行锁合称 Next-Key Lock，它锁住的是一个前开后闭的区间，确保在锁定范围内无法插入新记录。</li></ul><p><strong>间隙锁的影响</strong>：</p><ul><li>间隙锁虽然解决了幻读问题，但也带来了并发度下降和死锁的风险。特别是在高并发场景下，间隙锁可能会导致更多的锁冲突和死锁。</li></ul><p><strong>隔离级别的选择</strong></p><ul><li>在<strong>可重复读</strong>隔离级别下，间隙锁生效，可以有效防止幻读。</li><li>在<strong>读提交</strong>隔离级别下，间隙锁不生效，幻读问题可能会出现，但可以通过将 binlog 格式设置为<code>row</code>来解决数据一致性问题。</li></ul><p><strong>实际应用中的考虑</strong></p><ul><li>业务开发人员在设计表结构和 SQL 语句时，不仅要考虑行锁，还要考虑间隙锁的影响，避免因间隙锁导致的死锁问题。</li><li>隔离级别的选择应根据业务需求来决定，如果业务不需要可重复读的保证，读提交隔离级别可能是一个更合适的选择。</li></ul><h2 id="_21-为什么我只改一行的语句-锁这么多" tabindex="-1"><a class="header-anchor" href="#_21-为什么我只改一行的语句-锁这么多" aria-hidden="true">#</a> 21 为什么我只改一行的语句，锁这么多？</h2><p><strong>加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。</strong></p><ol><li><strong>原则 1</strong>：加锁的基本单位是 Next-Key Lock，即前开后闭区间。</li><li><strong>原则 2</strong>：查找过程中访问到的对象才会加锁。</li><li><strong>优化 1</strong>：索引上的等值查询，给唯一索引加锁时，Next-Key Lock 退化为行锁。</li><li><strong>优化 2</strong>：索引上的等值查询，向右遍历时且最后一个值不满足等值条件时，Next-Key Lock 退化为间隙锁。</li><li><strong>一个 bug</strong>：唯一索引上的范围查询会访问到不满足条件的第一个值为止。</li></ol><p><strong>锁的范围与隔离级别</strong>：</p><ul><li>在<strong>可重复读</strong>隔离级别下，Next-Key Lock 和间隙锁生效，防止幻读。</li><li>在<strong>读提交</strong>隔离级别下，间隙锁不生效，锁的范围更小，锁的时间更短。</li></ul><h2 id="_22-mysql-有哪些-饮鸩止渴-提高性能的方法" tabindex="-1"><a class="header-anchor" href="#_22-mysql-有哪些-饮鸩止渴-提高性能的方法" aria-hidden="true">#</a> 22 MySQL 有哪些“饮鸩止渴”提高性能的方法？</h2><h4 id="短连接风暴" tabindex="-1"><a class="header-anchor" href="#短连接风暴" aria-hidden="true">#</a> 短连接风暴</h4><ul><li><strong>问题</strong>：短连接模式下，业务高峰期连接数暴涨，可能导致数据库连接数超过<code>max_connections</code>限制，进而拒绝新连接。</li><li><strong>解决方案</strong>： <ul><li><strong>方法一</strong>：主动断开空闲连接。优先断开事务外空闲的连接，再考虑断开事务内空闲的连接。可以通过<code>kill connection</code>命令手动断开连接。</li><li><strong>方法二</strong>：减少连接过程的消耗。通过<code>--skip-grant-tables</code>参数重启数据库，跳过权限验证，但这种方法风险极高，尤其是在外网可访问的情况下。</li></ul></li><li><strong>风险</strong>：断开连接可能导致应用端未正确处理连接丢失，进而引发更多问题。</li></ul><h3 id="慢查询性能问题" tabindex="-1"><a class="header-anchor" href="#慢查询性能问题" aria-hidden="true">#</a> 慢查询性能问题</h3><ul><li><strong>慢查询的三种可能原因</strong>： <ol><li><strong>索引没有设计好</strong>：通过紧急创建索引来解决，建议在备库先执行<code>alter table</code>语句，再进行主备切换。</li><li><strong>SQL 语句没写好</strong>：通过改写 SQL 语句来优化，MySQL 5.7 提供了<code>query_rewrite</code>功能，可以自动重写 SQL 语句。</li><li><strong>MySQL 选错了索引</strong>：通过<code>force index</code>强制使用正确的索引。</li></ol></li><li><strong>预防措施</strong>：在上线前，通过慢查询日志和回归测试，提前发现并解决潜在的慢查询问题。</li></ul><h3 id="qps-突增问题" tabindex="-1"><a class="header-anchor" href="#qps-突增问题" aria-hidden="true">#</a> QPS 突增问题</h3><ul><li><strong>问题</strong>：由于业务高峰或应用 bug，某个 SQL 语句的 QPS 突然暴涨，导致数据库压力过大。</li><li><strong>解决方案</strong>： <ol><li><strong>下掉新功能</strong>：如果新功能有 bug，可以直接从数据库端去掉白名单或删除相关用户。</li><li><strong>重写 SQL 语句</strong>：将高 QPS 的 SQL 语句重写为<code>select 1</code>，但这种方法风险较高，可能会误伤其他功能或导致业务逻辑失败。</li></ol></li><li><strong>风险</strong>：重写 SQL 语句可能导致业务逻辑错误，应作为最后的手段。</li></ul><h2 id="_23-mysql-是怎么保证数据不丢的" tabindex="-1"><a class="header-anchor" href="#_23-mysql-是怎么保证数据不丢的" aria-hidden="true">#</a> 23 Mysql 是怎么保证数据不丢的</h2><p><strong>binlog 的写入机制</strong></p><ul><li>事务执行过程中，日志先写入 binlog cache，事务提交时再将 binlog cache 写入 binlog 文件。</li><li>binlog cache 是每个线程独有的，而 binlog 文件是共享的。</li><li>写入操作分为 write（写入文件系统的 page cache）和 fsync（持久化到磁盘）。</li><li>参数 <code>sync_binlog</code> 控制 fsync 的时机： <ul><li><code>sync_binlog=0</code>：每次提交事务只 write，不 fsync。</li><li><code>sync_binlog=1</code>：每次提交事务都 fsync。</li><li><code>sync_binlog=N</code>：每 N 个事务提交后 fsync。</li></ul></li></ul><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20220802060429.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>redo log 的写入机制</strong></p><ul><li>事务执行过程中，redo log 先写入 redo log buffer。</li><li>redo log 的三种状态： <ul><li>在 redo log buffer 中（内存）。</li><li>写入文件系统的 page cache（write）。</li><li>持久化到磁盘（fsync）。</li></ul></li><li>参数 <code>innodb_flush_log_at_trx_commit</code> 控制 redo log 的写入策略： <ul><li><code>0</code>：事务提交时只写入 redo log buffer。</li><li><code>1</code>：事务提交时将 redo log 持久化到磁盘。</li><li><code>2</code>：事务提交时只写入 page cache。</li></ul></li></ul><p><strong>redo log 写入磁盘的触发时机</strong></p><ul><li>后台线程每秒会将 redo log buffer 中的日志写入磁盘。</li><li>redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。</li><li>并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。</li></ul><p><strong>组提交（Group Commit）机制</strong>：</p><ul><li>通过延迟 fsync 操作，将多个事务的 redo log 或 binlog 合并写入磁盘，减少磁盘 I/O 操作。</li><li>组提交可以显著提升性能，尤其是在高并发场景下。</li></ul><p><strong>WAL 机制的优势</strong>：</p><ul><li>redo log 和 binlog 都是顺序写入，顺序写比随机写速度快。</li><li>组提交机制减少了磁盘 I/O 操作的次数。</li></ul><p><strong>性能优化建议</strong>：</p><ul><li>设置 <code>binlog_group_commit_sync_delay</code> 和 <code>binlog_group_commit_sync_no_delay_count</code> 参数，减少 binlog 的写盘次数。</li><li>将 <code>sync_binlog</code> 设置为大于 1 的值（如 100~1000），减少 fsync 次数，但主机掉电时会丢 binlog 日志。</li><li>将 <code>innodb_flush_log_at_trx_commit</code> 设置为 2，减少 redo log 的 fsync 次数，但主机掉电时会丢失数据。</li></ul><p><strong>数据一致性与可靠性</strong>：</p><ul><li>MySQL 通过 redo log 和 binlog 的持久化来保证 crash-safe。</li><li>即使事务未提交，redo log 和 binlog 的丢失也不会导致数据不一致，因为事务未提交的数据不会被应用到数据库中。</li></ul><p><strong>常见问题解答</strong>：</p><ul><li>解释了为什么 binlog cache 是线程独有，而 redo log buffer 是全局共享的。</li><li>讨论了事务执行期间发生 crash 时，redo log 和 binlog 的丢失不会导致主备不一致。</li><li>解释了 binlog 写入后发生 crash 的情况，客户端重连后事务已提交成功是正常现象。</li></ul><h2 id="_24-mysql-是怎么保证主备一致的" tabindex="-1"><a class="header-anchor" href="#_24-mysql-是怎么保证主备一致的" aria-hidden="true">#</a> 24 Mysql 是怎么保证主备一致的</h2><p><strong>MySQL 主备同步的基本原理</strong>：</p><ul><li>主库（节点 A）负责处理客户端的读写请求，备库（节点 B）通过同步主库的 binlog 来保持数据一致。</li><li>主备切换时，客户端会从主库切换到备库，备库变为新的主库。</li><li>备库通常设置为只读模式，防止误操作和双写问题，但同步线程拥有超级权限，可以绕过只读限制。</li></ul><p><strong>主备同步的流程</strong>：</p><ul><li>备库通过 <code>change master</code> 命令设置主库的连接信息，并通过 <code>start slave</code> 命令启动两个线程：<code>io_thread</code> 和 <code>sql_thread</code>。</li><li><code>io_thread</code> 负责从主库读取 binlog 并写入备库的中转日志（relay log）。</li><li><code>sql_thread</code> 负责解析并执行中转日志中的命令，保持备库与主库的数据一致。</li></ul><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/202503200809203.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>binlog 的三种格式</strong>：</p><ul><li><strong>statement</strong>：记录 SQL 语句的原文。优点是日志量小，缺点是某些情况下可能导致主备数据不一致（如使用了 <code>LIMIT</code> 或 <code>NOW()</code> 函数）。</li><li><strong>row</strong>：记录每一行数据的变更。优点是保证主备数据一致，缺点是日志量大，尤其是批量操作时。</li><li><strong>mixed</strong>：MySQL 自动选择 statement 或 row 格式，结合两者的优点，避免数据不一致问题。</li></ul><p><strong>binlog 格式的选择</strong>：</p><ul><li><strong>statement</strong> 格式可能导致主备数据不一致，尤其是在使用不确定函数（如 <code>NOW()</code>）或 <code>LIMIT</code> 时。</li><li><strong>row</strong> 格式记录了每一行数据的变更，确保主备数据一致，但日志量较大。</li><li><strong>mixed</strong> 格式是 MySQL 的折中方案，自动选择 statement 或 row 格式，避免数据不一致问题。</li></ul><p><strong>binlog 的数据恢复</strong>：</p><ul><li><strong>row</strong> 格式的 binlog 记录了每一行数据的变更，可以用于数据恢复。例如，误删数据后可以通过 binlog 恢复删除的行。</li><li><strong>insert</strong>、<strong>update</strong> 和 <strong>delete</strong> 操作都可以通过 binlog 进行恢复，尤其是 row 格式的 binlog 记录了完整的行数据。</li></ul><p><strong>循环复制问题</strong>：</p><ul><li>在双 M 结构（主备互为主备）中，可能会出现循环复制问题，即主库和备库互相同步 binlog，导致无限循环。</li><li>MySQL 通过 server id 解决循环复制问题：每个库在收到 binlog 时，会检查 server id，如果与自己的相同，则丢弃该日志，避免循环复制。</li></ul><p><strong>binlog 的其他用途</strong>：</p><ul><li>binlog 不仅可以用于主备同步，还可以用于数据恢复、审计、数据同步等场景。</li><li>通过 <code>mysqlbinlog</code> 工具可以解析 binlog，并将其用于数据恢复或重放。</li></ul><p><strong>总结</strong>：</p><ul><li>binlog 是 MySQL 主备同步的核心机制，通过不同的格式（statement、row、mixed）来平衡日志大小和数据一致性。</li><li>主备同步通过 <code>io_thread</code> 和 <code>sql_thread</code> 实现，确保备库与主库的数据一致。</li><li>双 M 结构中的循环复制问题通过 server id 机制解决，避免无限循环。</li></ul><h2 id="_25-mysql-是怎么保证高可用的" tabindex="-1"><a class="header-anchor" href="#_25-mysql-是怎么保证高可用的" aria-hidden="true">#</a> 25 Mysql 是怎么保证高可用的</h2><p><strong>主备同步与最终一致性</strong>：</p><ul><li>MySQL 通过 binlog 实现主备同步，备库接收并执行主库的 binlog，最终达到与主库一致的状态。</li><li>最终一致性是主备同步的基础，但要实现高可用性，还需要解决主备延迟等问题。</li></ul><p><strong>主备延迟的来源</strong>：</p><ul><li><strong>备库性能不足</strong>：备库所在机器的性能较差，导致同步速度慢。</li><li><strong>备库压力大</strong>：备库承担了过多的读请求，消耗了大量 CPU 资源，影响了同步速度。</li><li><strong>大事务</strong>：主库上的大事务（如大量数据删除或大表 DDL）会导致备库延迟，因为备库需要等待主库的事务完成后才能同步。</li><li><strong>备库的并行复制能力</strong>：备库的并行复制能力不足也会导致延迟。</li></ul><p><strong>主备切换策略</strong>：</p><ul><li><strong>可靠性优先策略</strong>： <ul><li>在主备切换时，确保备库的数据与主库完全一致后再切换。</li><li>切换过程中会有短暂的不可用时间，但能保证数据的一致性。</li></ul></li><li><strong>可用性优先策略</strong>： <ul><li>在主备切换时，优先保证系统的可用性，允许短暂的数据不一致。</li><li>这种策略可能会导致数据不一致，尤其是在使用 statement 或 mixed 格式的 binlog 时。</li></ul></li></ul><p><strong>binlog 格式对数据一致性的影响</strong>：</p><ul><li><strong>statement 格式</strong>：记录 SQL 语句的原文，可能导致主备数据不一致（如使用 <code>LIMIT</code> 或 <code>NOW()</code> 函数时）。</li><li><strong>row 格式</strong>：记录每一行数据的变更，确保主备数据一致，但日志量较大。</li><li><strong>mixed 格式</strong>：MySQL 自动选择 statement 或 row 格式，结合两者的优点，避免数据不一致问题。</li></ul><p><strong>高可用性与数据一致性的权衡</strong>：</p><ul><li>大多数情况下，建议使用<strong>可靠性优先策略</strong>，确保数据的准确性。</li><li>在某些特殊场景下（如操作日志记录），<strong>可用性优先策略</strong>可能更为合适，因为短暂的数据不一致可以通过 binlog 修复，且不会对业务造成严重影响。</li></ul><p><strong>异常切换与主备延迟</strong>：</p><ul><li>在主库故障时，主备延迟会影响系统的可用性。延迟越小，系统恢复的时间越短，可用性越高。</li><li>如果主备延迟较大，切换时可能会导致系统不可用或数据不一致。</li></ul><p><strong>总结</strong>：</p><ul><li>MySQL 的高可用性依赖于主备同步机制，主备延迟是影响高可用性的关键因素。</li><li>通过优化备库性能、减少大事务、提升并行复制能力等手段，可以减少主备延迟。</li><li>在主备切换时，应根据业务需求选择<strong>可靠性优先</strong>或<strong>可用性优先</strong>策略，确保在数据一致性和系统可用性之间找到平衡。</li></ul><h2 id="_26-备库为什么会延迟好几个小时" tabindex="-1"><a class="header-anchor" href="#_26-备库为什么会延迟好几个小时" aria-hidden="true">#</a> 26 备库为什么会延迟好几个小时</h2><p><strong>备库延迟的原因</strong>：</p><ul><li>备库执行日志的速度持续低于主库生成日志的速度，导致延迟可能达到小时级别。</li><li>单线程复制是备库延迟的主要原因之一，尤其是在主库并发高、TPS 高的情况下。</li></ul><p><strong>并行复制的核心原则</strong>：</p><ul><li><strong>不能造成更新覆盖</strong>：更新同一行的两个事务必须被分发到同一个 worker 中。</li><li><strong>同一个事务不能被拆开</strong>：同一个事务的多个更新语句必须放到同一个 worker 中执行。</li></ul><p><strong>多线程复制的演进</strong>：</p><ul><li><strong>MySQL 5.5 及之前版本</strong>：只支持单线程复制，导致备库延迟问题严重。</li><li><strong>MySQL 5.6 版本</strong>：支持了并行复制，允许不同数据库的事务在备库上并行执行。</li><li><strong>MariaDB 的并行复制策略</strong>：基于组提交（group commit）特性，相同 commit_id 的事务可以在备库上并行执行，但存在大事务拖后腿的问题。</li><li><strong>MySQL 5.7 版本</strong>：引入了基于 LOGICAL_CLOCK 的并行复制策略，允许处于 prepare 状态的事务在备库上并行执行，提升了并行度。</li><li><strong>MySQL 5.7.22 版本</strong>：引入了基于 WRITESET 的并行复制策略。算出这一行的 hash 值，组成集合 writeset。如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行。</li></ul><p><strong>不同并行复制策略的优缺点</strong>：</p><ul><li><strong>按库并行策略</strong>：适用于多数据库场景，但在单数据库或热点表场景下效果不佳。</li><li><strong>按表并行策略</strong>：适用于多表场景，但在热点场景下会退化为单线程复制。</li><li><strong>按行并行策略</strong>：并行度最高，但消耗更多的内存和 CPU 资源，适用于大事务较少的场景。</li><li><strong>MariaDB 的组提交策略</strong>：基于 commit_id 的并行复制，简单易实现，但容易受大事务影响。</li><li><strong>MySQL 5.7 的 LOGICAL_CLOCK 策略</strong>：基于 prepare 状态的并行复制，提升了并行度，但依赖于主库的 binlog 组提交机制。</li><li><strong>MySQL 5.7.22 的 WRITESET 策略</strong>：基于行 hash 值的并行复制，减少了计算量和内存消耗，支持 statement 格式的 binlog。</li></ul><p><strong>大事务对备库延迟的影响</strong>：</p><ul><li>大事务（如大表 DDL 或大量数据删除）会导致备库延迟增加，因为备库需要等待大事务完成后才能继续执行其他事务。</li><li>建议将大事务拆分为小事务，以减少对备库同步的影响。</li></ul><p><strong>总结</strong>：</p><ul><li>多线程复制是解决备库延迟问题的关键，MySQL 通过不同版本的演进逐步提升了并行复制的效率和灵活性。</li><li>不同的并行复制策略适用于不同的业务场景，DBA 需要根据实际情况选择合适的策略。</li><li>大事务是造成备库延迟的主要原因之一，开发人员应尽量避免大事务操作，将其拆分为小事务。</li></ul><h2 id="_27-主库出问题了-从库怎么办" tabindex="-1"><a class="header-anchor" href="#_27-主库出问题了-从库怎么办" aria-hidden="true">#</a> 27 主库出问题了，从库怎么办？</h2><p><strong>一主多从架构</strong></p><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/snap/20220803070027.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>一主多从架构通常用于读写分离，主库负责写操作和部分读操作，从库分担读请求。</li><li>当主库发生故障时，需要进行主备切换，从库需要重新指向新的主库，增加了切换的复杂性。</li></ul><p><strong>基于位点的主备切换</strong>：</p><ul><li>在切换过程中，从库需要找到与新主库同步的位点（binlog 文件名和偏移量），以确保数据一致性。</li><li>位点的获取通常是通过解析新主库的 binlog 文件，找到故障时刻的大致位置。</li><li>由于位点不精确，可能会导致从库重复执行某些事务，出现主键冲突等问题。</li><li>解决方法包括： <ul><li>使用 <code>sql_slave_skip_counter</code> 跳过重复事务。</li><li>设置 <code>slave_skip_errors</code> 参数，跳过常见的错误（如 1062 主键冲突和 1032 删除数据找不到行）。</li></ul></li></ul><p><strong>GTID（全局事务标识符）</strong></p><ul><li>GTID 是 MySQL 5.6 引入的机制，用于唯一标识每个事务，格式为 <code>server_uuid:gno</code>。</li><li>GTID 模式简化了主备切换过程，不再需要手动指定位点，系统会自动处理同步问题。</li><li>GTID 的生成方式有两种： <ul><li>自动生成：事务提交时分配 GTID。</li><li>手动指定：通过 <code>set gtid_next</code> 指定 GTID，适用于跳过某些事务的场景。</li></ul></li></ul><p><strong>基于 GTID 的主备切换</strong>：</p><ul><li>在 GTID 模式下，从库只需要执行 <code>CHANGE MASTER TO</code> 命令，并设置 <code>master_auto_position=1</code>，系统会自动计算需要同步的事务。</li><li>新主库会计算自己与从库的 GTID 集合差集，确保从库获取到所有缺失的事务。</li><li>如果新主库缺少从库所需的事务，会直接报错，确保数据完整性。</li></ul><p><strong>GTID 与在线 DDL</strong>：</p><ul><li>在双 M 结构下，备库执行的 DDL 语句可以通过 GTID 机制确保不会在主库上重复执行。</li><li>通过手动设置 GTID，可以确保 DDL 操作的 binlog 记录不会影响主库。</li></ul><p><strong>总结</strong>：</p><ul><li>基于位点的主备切换复杂且容易出错，而 GTID 模式简化了这一过程，提升了主备切换的效率和可靠性。</li><li>如果 MySQL 版本支持 GTID，建议使用 GTID 模式进行主备切换。</li><li>GTID 模式不仅适用于主备切换，还可以用于在线 DDL 操作，确保数据一致性。</li></ul><h2 id="_28-读写分离有哪些坑" tabindex="-1"><a class="header-anchor" href="#_28-读写分离有哪些坑" aria-hidden="true">#</a> 28 读写分离有哪些坑</h2><p><strong>读写分离的基本架构</strong>：</p><p>一主多从架构通常用于读写分离，主库负责写操作，从库分担读请求。</p><p>读写分离的两种常见架构：</p><ul><li><strong>客户端直连</strong>：客户端直接连接数据库，性能较好，但主备切换时客户端需要调整连接信息。</li><li><strong>带 Proxy 的架构</strong>：客户端连接 Proxy，由 Proxy 负责路由请求，对客户端友好，但架构复杂。</li></ul><p><strong>过期读问题</strong>：</p><p>由于<strong>主从延迟</strong>，客户端<strong>在从库上可能会读到过期的数据</strong>，这种现象称为“<strong>过期读</strong>”。过期读的常见场景是主库更新后，从库还未同步完成，客户端查询从库时读到旧数据。</p><p><strong>解决过期读的几种方案</strong>：</p><ul><li><strong>强制走主库</strong>：对于必须读到最新数据的请求，强制查询主库。适用于对数据实时性要求高的场景，但会增加主库的压力。</li><li><strong>Sleep 方案</strong>：在查询从库前先 sleep 一段时间，假设主从延迟在 1 秒内。虽然简单，但不精确，可能导致等待时间过长或仍然读到过期数据。</li><li><strong>判断主备无延迟</strong>：通过 <code>show slave status</code> 判断主从延迟，确保从库同步完成后再查询。可以通过 <code>seconds_behind_master</code>、位点对比或 GTID 集合对比来判断。</li><li><strong>配合 semi-sync</strong>：使用半同步复制（semi-sync），确保主库在事务提交后，至少有一个从库收到 binlog 后才返回确认。可以减少过期读的概率，但在多从库场景下仍可能有问题。semi-sync 流程： <ul><li>事务提交的时候，主库把 binlog 发给从库；</li><li>从库收到 binlog 以后，发回给主库一个 ack，表示收到了；</li><li>主库收到这个 ack 以后，才能给客户端返回“事务完成”的确认。</li></ul></li><li><strong>等主库位点方案</strong>：使用 <code>select master_pos_wait(file, pos, timeout)</code> 命令，等待从库同步到指定位点后再查询。可以精确控制查询时机，避免过期读。</li><li><strong>等 GTID 方案</strong>：使用 <code>select wait_for_executed_gtid_set(gtid_set, timeout)</code> 命令，等待从库执行到指定 GTID 后再查询。适用于 GTID 模式，减少了查询主库位点的开销。</li></ul><p>不同的方案适用于不同的业务场景，需要根据业务需求选择。在实际应用中，可以混合使用多种方案，根据请求的类型选择不同的处理方式。</p><p><strong>总结</strong>：</p><ul><li>过期读是读写分离架构中常见的问题，主从延迟是主要原因。</li><li>通过合理的方案选择，可以在保证读写分离的同时，尽量减少过期读的发生。</li><li>对于高一致性要求的场景，建议使用等主库位点或等 GTID 方案，确保查询结果的准确性。</li></ul><h2 id="_29-如何判断一个数据库是不是出问题了" tabindex="-1"><a class="header-anchor" href="#_29-如何判断一个数据库是不是出问题了" aria-hidden="true">#</a> 29 如何判断一个数据库是不是出问题了</h2><p><strong>select 1 判断</strong>：</p><ul><li><code>select 1</code> 只能检测数据库进程是否存活，无法检测数据库内部的并发线程数是否过高或是否存在其他问题。</li><li>当并发线程数达到 <code>innodb_thread_concurrency</code> 设置的上限时，数据库可能无法处理新请求，但 <code>select 1</code> 仍然可以成功返回，导致误判。</li></ul><p><strong>查表判断</strong>：</p><ul><li>在系统库（如 <code>mysql</code> 库）中创建一个健康检查表（如 <code>health_check</code>），里面只放一行数据，然后定期执行 <code>select * from mysql.health_check</code>。</li><li>这种方法<strong>可以检测到由于并发线程过多导致的数据库不可用</strong>情况，但<strong>无法检测磁盘空间满等问题</strong>。</li></ul><p><strong>更新判断</strong>：</p><ul><li>在健康检查表中加入一个 <code>timestamp</code> 字段，定期执行更新操作（如 <code>update mysql.health_check set t_modified=now()</code>）。</li><li>这种方法可以检测到磁盘空间满等问题，因为更新操作需要写 binlog，如果磁盘空间满，更新操作会失败。为了避免主备冲突，可以在健康检查表中使用 <code>server_id</code> 作为主键，确保主库和备库的更新操作不会冲突。</li><li>更新判断的局限性： <ul><li>更新判断存在“判定慢”的问题，即在系统 IO 资源紧张时，更新操作可能仍然成功返回，导致误判。</li><li>外部检测的随机性可能导致问题无法及时被发现，尤其是在定时轮询的间隔期间。</li></ul></li></ul><p><strong>内部统计</strong>：</p><ul><li>MySQL 5.6 版本以后提供了 <code>performance_schema</code> 库，可以统计每次 IO 请求的时间。</li><li>通过监控 <code>performance_schema.file_summary_by_event_name</code> 表中的 IO 请求时间，可以更准确地判断数据库是否出现性能问题。</li><li>可以设置阈值，当单次 IO 请求时间超过一定值（如 200 毫秒）时，认为数据库出现异常。</li></ul><p><strong>总结</strong>：</p><ul><li>不同的检测方法各有优缺点，<code>select 1</code> 简单但不精确，查表和更新判断可以检测更多问题，但仍存在局限性。</li><li>内部统计方法（如 <code>performance_schema</code>）可以提供更精确的数据库状态信息，但会带来一定的性能损耗。</li><li>在实际应用中，可以根据业务需求选择合适的检测方法，通常建议结合更新判断和内部统计方法，以提高检测的准确性。</li></ul><h2 id="_30-答疑文章-二-用动态的观点看加锁" tabindex="-1"><a class="header-anchor" href="#_30-答疑文章-二-用动态的观点看加锁" aria-hidden="true">#</a> 30 答疑文章（二）：用动态的观点看加锁</h2><p><strong>加锁规则回顾</strong>：</p><ul><li><strong>原则 1</strong>：加锁的基本单位是 next-key lock，即前开后闭区间。</li><li><strong>原则 2</strong>：查找过程中访问到的对象才会加锁。</li><li><strong>优化 1</strong>：唯一索引上的等值查询，next-key lock 退化为行锁。</li><li><strong>优化 2</strong>：索引上的等值查询，向右遍历时且最后一个值不满足等值条件时，next-key lock 退化为间隙锁。</li><li><strong>一个 bug</strong>：唯一索引上的范围查询会访问到不满足条件的第一个值为止。</li></ul><p><strong>不等号条件里的等值查询</strong>：</p><ul><li>在不等号查询中，虽然条件是不等号，但在索引树的搜索过程中，引擎内部会使用等值查询来定位记录。</li><li>例如，<code>select * from t where id&gt;9 and id&lt;12 order by id desc for update;</code> 的加锁范围是 (0,5]、(5,10] 和 (10,15)，其中 id=15 不满足条件，next-key lock 退化为间隙锁 (10,15)。</li></ul><p><strong>等值查询的过程</strong>：</p><ul><li>对于 <code>select id from t where c in(5,20,10) lock in share mode;</code>，加锁过程是逐个进行的，先加 c=5 的记录锁，再加 c=10 的记录锁，最后加 c=20 的记录锁。</li><li>如果并发执行 <code>select id from t where c in(5,20,10) order by c desc for update;</code>，加锁顺序相反，可能导致死锁。</li></ul><p><strong>死锁分析</strong>：</p><ul><li>死锁发生时，InnoDB 会选择回滚成本较小的事务来解除死锁。</li><li>通过 <code>show engine innodb status</code> 可以查看死锁信息，了解哪些事务持有锁、等待锁，以及最终回滚了哪个事务。</li></ul><p><strong>锁等待分析</strong>：</p><ul><li>通过 <code>show engine innodb status</code> 可以查看锁等待信息，了解哪些事务在等待锁，以及等待的锁类型（如间隙锁、插入意向锁等）。</li><li>间隙锁的范围是由间隙右边的记录定义的，删除记录后，间隙锁的范围可能会发生变化。</li></ul><p><strong>update 语句的加锁行为</strong>：</p><ul><li>update 语句的加锁范围可以通过语句的执行逻辑来分析。例如，<code>update t set c=1 where c=5</code> 会先插入新记录，再删除旧记录，可能会被间隙锁阻塞。</li></ul><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料" aria-hidden="true">#</a> 参考资料</h2>`,182),p={href:"https://time.geekbang.org/column/intro/139",target:"_blank",rel:"noopener noreferrer"};function h(_,b){const i=r("ExternalLinkIcon");return t(),s("div",null,[c,o("p",null,[o("a",a,[l("极客时间教程 - MySQL 实战 45 讲"),e(i)]),l(" 学习笔记")]),u,o("ul",null,[o("li",null,[o("a",p,[l("极客时间教程 - MySQL 实战 45 讲"),e(i)])])])])}const y=n(d,[["render",h],["__file","index.html.vue"]]);export{y as default};
