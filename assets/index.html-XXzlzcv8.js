import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as t,o as e}from"./app-W6ac-OwH.js";const n={};function l(r,i){return e(),a("div",null,[...i[0]||(i[0]=[t(`<h1 id="hive-简介" tabindex="-1"><a class="header-anchor" href="#hive-简介"><span>Hive 简介</span></a></h1><h2 id="简介" tabindex="-1"><a class="header-anchor" href="#简介"><span>简介</span></a></h2><p><strong>Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能</strong>，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。</p><p><strong>特点</strong>：</p><ol><li>简单、容易上手 （提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；</li><li>灵活性高，可以自定义用户函数 (UDF) 和存储格式；</li><li>为超大的数据集设计的计算和存储能力，集群扩展容易；</li><li>统一的元数据管理，可与 presto／impala／sparksql 等共享数据；</li><li>执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。</li></ol><h2 id="hive-的体系架构" tabindex="-1"><a class="header-anchor" href="#hive-的体系架构"><span>Hive 的体系架构</span></a></h2><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/archive/2020/02/f40f7bdbe3a24bfa9c4968c92ad5b7ef.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="command-line-shell-thrift-jdbc" tabindex="-1"><a class="header-anchor" href="#command-line-shell-thrift-jdbc"><span>command-line shell &amp; thrift/jdbc</span></a></h3><p>可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据：</p><ul><li><strong>command-line shell</strong>：通过 hive 命令行的的方式来操作数据；</li><li><strong>thrift／jdbc</strong>：通过 thrift 协议按照标准的 JDBC 的方式操作数据。</li></ul><h3 id="metastore" tabindex="-1"><a class="header-anchor" href="#metastore"><span>Metastore</span></a></h3><p>在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替 derby。</p><p>Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql 中创建一张表，在 Hive 中也可以直接使用。</p><h3 id="hql-的执行流程" tabindex="-1"><a class="header-anchor" href="#hql-的执行流程"><span>HQL 的执行流程</span></a></h3><p>Hive 在执行一条 HQL 的时候，会经过以下步骤：</p><ol><li>语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree；</li><li>语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock；</li><li>生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree；</li><li>优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量；</li><li>生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务；</li><li>优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。</li></ol><blockquote><p>关于 Hive SQL 的详细执行流程可以参考美团技术团队的文章：<a href="https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html" target="_blank" rel="noopener noreferrer">Hive SQL 的编译过程</a></p></blockquote><h2 id="数据类型" tabindex="-1"><a class="header-anchor" href="#数据类型"><span>数据类型</span></a></h2><h3 id="基本数据类型" tabindex="-1"><a class="header-anchor" href="#基本数据类型"><span>基本数据类型</span></a></h3><p>Hive 表中的列支持以下基本数据类型：</p><table><thead><tr><th>大类</th><th>类型</th></tr></thead><tbody><tr><td><strong>Integers（整型）</strong></td><td>TINYINT—1 字节的有符号整数 <br>SMALLINT—2 字节的有符号整数<br> INT—4 字节的有符号整数<br> BIGINT—8 字节的有符号整数</td></tr><tr><td><strong>Boolean（布尔型）</strong></td><td>BOOLEAN—TRUE/FALSE</td></tr><tr><td><strong>Floating point numbers（浮点型）</strong></td><td>FLOAT— 单精度浮点型 <br>DOUBLE—双精度浮点型</td></tr><tr><td><strong>Fixed point numbers（定点数）</strong></td><td>DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2)</td></tr><tr><td><strong>String types（字符串）</strong></td><td>STRING—指定字符集的字符序列<br> VARCHAR—具有最大长度限制的字符序列 <br>CHAR—固定长度的字符序列</td></tr><tr><td><strong>Date and time types（日期时间类型）</strong></td><td>TIMESTAMP — 时间戳 <br>TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度<br> DATE—日期类型</td></tr><tr><td><strong>Binary types（二进制类型）</strong></td><td>BINARY—字节序列</td></tr></tbody></table><blockquote><p>TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下：</p><ul><li><strong>TIMESTAMP WITH LOCAL TIME ZONE</strong>：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。</li><li><strong>TIMESTAMP</strong> ：提交什么时间就保存什么时间，查询时也不做任何转换。</li></ul></blockquote><h3 id="隐式转换" tabindex="-1"><a class="header-anchor" href="#隐式转换"><span>隐式转换</span></a></h3><p>Hive 中基本数据类型遵循以下的层次结构，按照这个层次结构，子类型到祖先类型允许隐式转换。例如 INT 类型的数据允许隐式转换为 BIGINT 类型。额外注意的是：按照类型层次结构允许将 STRING 类型隐式转换为 DOUBLE 类型。</p><figure><img src="https://raw.githubusercontent.com/dunwu/images/master/archive/2020/02/f7ffc4a5940e47c8b1ce0d0fbcb35404.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="复杂类型" tabindex="-1"><a class="header-anchor" href="#复杂类型"><span>复杂类型</span></a></h3><table><thead><tr><th>类型</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td><strong>STRUCT</strong></td><td>类似于对象，是字段的集合，字段的类型可以不同，可以使用 <code>名称。字段名</code> 方式进行访问</td><td>STRUCT (&#39;xiaoming&#39;, 12 , &#39;2018-12-12&#39;)</td></tr><tr><td><strong>MAP</strong></td><td>键值对的集合，可以使用 <code>名称 [key]</code> 的方式访问对应的值</td><td>map(&#39;a&#39;, 1, &#39;b&#39;, 2)</td></tr><tr><td><strong>ARRAY</strong></td><td>数组是一组具有相同类型和名称的变量的集合，可以使用 <code>名称 [index]</code> 访问对应的值</td><td>ARRAY(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;)</td></tr></tbody></table><h3 id="示例" tabindex="-1"><a class="header-anchor" href="#示例"><span>示例</span></a></h3><p>如下给出一个基本数据类型和复杂数据类型的使用示例：</p><div class="language-sql line-numbers-mode" data-highlighter="shiki" data-ext="sql" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-sql"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">CREATE</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> TABLE</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> students</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">  name</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      STRING,   </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 姓名</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  age       </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">INT</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,      </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 年龄</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">  subject</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">   ARRAY</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">STRING</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,   </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--学科</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  score     MAP</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">STRING,</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">FLOAT</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&gt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--各个学科考试成绩</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">  address</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">   STRUCT</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&lt;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">houseNumber:</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">int</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, street:STRING, city:STRING, province：STRING</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">&gt;</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  --家庭居住地址</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">ROW</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FORMAT DELIMITED FIELDS TERMINATED </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">BY</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;\\t&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="内容格式" tabindex="-1"><a class="header-anchor" href="#内容格式"><span>内容格式</span></a></h2><p>当数据存储在文本文件中，必须按照一定格式区别行和列，如使用逗号作为分隔符的 CSV 文件 (Comma-Separated Values) 或者使用制表符作为分隔值的 TSV 文件 (Tab-Separated Values)。但此时也存在一个缺点，就是正常的文件内容中也可能出现逗号或者制表符。</p><p>所以 Hive 默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在文件中。Hive 默认的行和列分隔符如下表所示。</p><table><thead><tr><th>分隔符</th><th>描述</th></tr></thead><tbody><tr><td><strong>\\n</strong></td><td>对于文本文件来说，每行是一条记录，所以可以使用换行符来分割记录</td></tr><tr><td><strong>^A (Ctrl+A)</strong></td><td>分割字段 （列），在 CREATE TABLE 语句中也可以使用八进制编码 <code>\\001</code> 来表示</td></tr><tr><td><strong>^B</strong></td><td>用于分割 ARRAY 或者 STRUCT 中的元素，或者用于 MAP 中键值对之间的分割，<br>在 CREATE TABLE 语句中也可以使用八进制编码 <code>\\002</code> 表示</td></tr><tr><td><strong>^C</strong></td><td>用于 MAP 中键和值之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 <code>\\003</code> 表示</td></tr></tbody></table><p>使用示例如下：</p><div class="language-sql line-numbers-mode" data-highlighter="shiki" data-ext="sql" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-sql"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">CREATE</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> TABLE</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> page_view</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(viewTime </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">INT</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, userid </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">BIGINT</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> ROW</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FORMAT DELIMITED</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">   FIELDS TERMINATED </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">BY</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;\\001&#39;</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">   COLLECTION</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ITEMS TERMINATED </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">BY</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;\\002&#39;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">   MAP KEYS TERMINATED </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">BY</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;\\003&#39;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> STORED </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">AS</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> SEQUENCEFILE;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="存储格式" tabindex="-1"><a class="header-anchor" href="#存储格式"><span>存储格式</span></a></h2><h3 id="支持的存储格式" tabindex="-1"><a class="header-anchor" href="#支持的存储格式"><span>支持的存储格式</span></a></h3><p>Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式：</p><table><thead><tr><th>格式</th><th>说明</th></tr></thead><tbody><tr><td><strong>TextFile</strong></td><td>存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。</td></tr><tr><td><strong>SequenceFile</strong></td><td>SequenceFile 是 Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。</td></tr><tr><td><strong>RCFile</strong></td><td>RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。</td></tr><tr><td><strong>ORC Files</strong></td><td>ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。</td></tr><tr><td><strong>Avro Files</strong></td><td>Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。</td></tr><tr><td><strong>Parquet</strong></td><td>Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。</td></tr></tbody></table><blockquote><p>以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。</p></blockquote><h3 id="指定存储格式" tabindex="-1"><a class="header-anchor" href="#指定存储格式"><span>指定存储格式</span></a></h3><p>通常在创建表的时候使用 <code>STORED AS</code> 参数指定：</p><div class="language-sql line-numbers-mode" data-highlighter="shiki" data-ext="sql" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-sql"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">CREATE</span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> TABLE</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> page_view</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(viewTime </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">INT</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, userid </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">BIGINT</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;"> ROW</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> FORMAT DELIMITED</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">   FIELDS TERMINATED </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">BY</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;\\001&#39;</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">   COLLECTION</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> ITEMS TERMINATED </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">BY</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;\\002&#39;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">   MAP KEYS TERMINATED </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">BY</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;\\003&#39;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> STORED </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">AS</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> SEQUENCEFILE;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>各个存储文件类型指定方式如下：</p><ul><li>STORED AS TEXTFILE</li><li>STORED AS SEQUENCEFILE</li><li>STORED AS ORC</li><li>STORED AS PARQUET</li><li>STORED AS AVRO</li><li>STORED AS RCFILE</li></ul><h2 id="内部表和外部表" tabindex="-1"><a class="header-anchor" href="#内部表和外部表"><span>内部表和外部表</span></a></h2><p>内部表又叫做管理表 (Managed/Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下：</p><table><thead><tr><th></th><th>内部表</th><th>外部表</th></tr></thead><tbody><tr><td>数据存储位置</td><td>内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 <code>/user/hive/warehouse/数据库名。db/表名/</code> 目录下</td><td>外部表数据的存储位置创建表时由 <code>Location</code> 参数指定；</td></tr><tr><td>导入数据</td><td>在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理</td><td>外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置</td></tr><tr><td>删除表</td><td>删除元数据（metadata）和文件</td><td>只删除元数据（metadata）</td></tr></tbody></table><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h2><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener noreferrer">Hive Getting Started</a></li><li><a href="https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html" target="_blank" rel="noopener noreferrer">Hive SQL 的编译过程</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener noreferrer">LanguageManual DDL</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types" target="_blank" rel="noopener noreferrer">LanguageManual Types</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/Managed+vs.+External+Tables" target="_blank" rel="noopener noreferrer">Managed vs. External Tables</a></li></ul>`,51)])])}const p=s(n,[["render",l]]),o=JSON.parse('{"path":"/pages/01602c0e/","title":"Hive 简介","lang":"zh-CN","frontmatter":{"icon":"simple-icons:apachehive","title":"Hive 简介","date":"2020-02-24T21:14:47.000Z","categories":["大数据","Hive"],"tags":["大数据","Hive"],"permalink":"/pages/01602c0e/","description":"Hive 简介 简介 Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。 特点： 简单、容易上手 （提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Hive 简介\\",\\"image\\":[\\"https://raw.githubusercontent.com/dunwu/images/master/archive/2020/02/f40f7bdbe3a24bfa9c4968c92ad5b7ef.png\\",\\"https://raw.githubusercontent.com/dunwu/images/master/archive/2020/02/f7ffc4a5940e47c8b1ce0d0fbcb35404.png\\"],\\"datePublished\\":\\"2020-02-24T21:14:47.000Z\\",\\"dateModified\\":\\"2026-02-11T15:41:54.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"钝悟\\",\\"url\\":\\"https://dunwu.github.io/waterdrop\\"}]}"],["meta",{"property":"og:url","content":"https://dunwu.github.io/waterdrop/waterdrop/pages/01602c0e/"}],["meta",{"property":"og:site_name","content":"钝悟"}],["meta",{"property":"og:title","content":"Hive 简介"}],["meta",{"property":"og:description","content":"Hive 简介 简介 Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。 特点： 简单、容易上手 （提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://raw.githubusercontent.com/dunwu/images/master/archive/2020/02/f40f7bdbe3a24bfa9c4968c92ad5b7ef.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-02-11T15:41:54.000Z"}],["meta",{"property":"article:tag","content":"Hive"}],["meta",{"property":"article:tag","content":"大数据"}],["meta",{"property":"article:published_time","content":"2020-02-24T21:14:47.000Z"}],["meta",{"property":"article:modified_time","content":"2026-02-11T15:41:54.000Z"}]]},"git":{"createdTime":1697125600000,"updatedTime":1770824514000,"contributors":[{"name":"dunwu","username":"dunwu","email":"forbreak@163.com","commits":8,"url":"https://github.com/dunwu"}]},"readingTime":{"minutes":8.35,"words":2506},"filePathRelative":"16.大数据/Hive/Hive_简介.md","excerpt":"\\n<h2>简介</h2>\\n<p><strong>Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能</strong>，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。</p>\\n<p><strong>特点</strong>：</p>\\n<ol>\\n<li>简单、容易上手 （提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；</li>\\n<li>灵活性高，可以自定义用户函数 (UDF) 和存储格式；</li>\\n<li>为超大的数据集设计的计算和存储能力，集群扩展容易；</li>\\n<li>统一的元数据管理，可与 presto／impala／sparksql 等共享数据；</li>\\n<li>执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。</li>\\n</ol>","autoDesc":true}');export{p as comp,o as data};
